---
layout: post
title:  "cs230阅读"
date:   2019-04-14 10:38:27 +0800
categories: DeepLearning
---


 <script type="text/x-mathjax-config">
  MathJax.Hub.Config({tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]}});
</script>
<script type="text/javascript" async
  src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML">
</script> 

滤波器的zero-padding。

tensorflow有两种形式：一种same，另一种valid

valid是不进行zero padding。输出的图像会减小。

same，如果stride为1的话，意味着输出和输入保持一致。

如果stride > 1的话，

设滤波器的大小为`F*F*C`，输入为`I*I*C`，输出为`O*O*K`

<!-- more -->
stride为`S`。

Valid的话，padding为0.

Same的话，

$$
P_{\text { start }}=\left\lfloor\frac{S\left\lceil\frac{I}{S}\right\rceil- I+F-S}{2}\right\rfloor
$$

$$
P_{\mathrm{end}}=\left\lceil\frac{S\left\lceil\frac{I}{S}\right\rceil- I+F-S}{2}\right\rceil
$$

需要得到`I/S`得到的上界。

最后的输出层数为：

$$
O=\frac{I-F+P_{\text { start }}+P_{\text { end }}}{S}+1
$$

这个看起来很可怕，但是实际上输出是输入的图像大小除以步长的大小，如果是小数的话，去上界。

比如227*227的输入，stride为4，卷积核大小为11，padding为same，输出其实就是227/4=56.75，然后向上取整为57。

padding的计算是： 占用的格子数为57\*4+11-1=237, 减去原来的格子是227，总的padding为10，所以前面padding是5，后面padding是5。

一般情况下，前面的padding和后面的padding是相同的，也就写成2P这种。

模型复杂度的话。

|什么层？|卷积层|池化层|全连接层|
|-------|-----|-----|------|
|权重描述|`F*F*C*K`|`F*F`|`输入*输出`|
|输入大小|`I*I*C`|`I*I*C`|`输入`|
|输出大小|`O*O*K`|`O*O*C`|`输出`|
|参数数量|`(F*F*C+1)*K`|0|`(输入+1)*输出`|

加1是因为每一滤波器都有一个偏差
一般情况下，stride小于滤波器大小
一般选择滤波器的数量为通道数量的2倍

池化操作时以通道为单位进行的
一般来说，stride和滤波器大小相同

全连接层的输入被拍扁了
每一个神经元都有一个偏差
FC神经元的数量没有结构上的限制

感受野是什么？
看公式：

$$
R_{k}=1+\sum_{j=1}^{k}\left(F_{j}-1\right) \prod_{i=0}^{j-1} S_{i}
$$

经典的激活函数：校正线性单元，leaky-ReLU， ELU（处处可导）
softmax可以输出一个概率值，
$$
p=\left( \begin{array}{c}{p_{1}} \\ {\vdots} \\ {p_{n}}\end{array}\right)
$$
$$
p_{i}=\frac{e^{x_{i}}}{\sum_{j=1}^{n} e^{x_{j}}}
$$

物体检测：
3类不同的模型
1. 对图片进行分类，预测可能性
2. 分类并定位，预测在什么地方
3. 检测一副图当中的不同物体；预测物体的可能性，并且他们在什么地方。

检测方法：
1. bounding box检测法，检测物体在什么地方，参数为中心和宽高。
2. 检测标志点，比如眼睛啥的，有一系列的参考点。

IOU检测法：Intersection over Union。两个区域交集和并集的比值。一般情况下，大于0.5才说这个有点好

$$
\operatorname{IoU}\left(B_{p}, B_{a}\right)=\frac{B_{p} \cap B_{a}}{B_{p} \cup B_{a}}
$$

Anchor boxing，预测重叠的bounding box。也就是说，网络可以预测多于一个的框，每个框都有各自不同的属性。比如，第一个是固定形状，第二个是不同形状。

非极大值抑制：去掉表示同一物体的，重复的框，只剩下最具有代表性的一个框。移除所有低于0.6的框，执行剩下的步骤：
1. 选择最大概率的框
2. 扔掉和这个框的IoU>=0.5的框

### YOLO： you only look once.
1. 把输入图像分成G*G个格子
2. 对每个格子，执行cnn预测y：
    $$
    y=\left[\underbrace{p_{c}, b_{x}, b_{y}, b_{h}, b_{w}, c_{1}, c_{2}, \ldots, c_{p}}_{\text { repeated } k \text { times }}, \dots\right]^{T} \in \mathbb{R}^{G \times G \times k \times(5+p)}
    $$

    $$p_c$$是检测物体的概率，$$b_x,b_y,b_h,b_w$$是bounding box的属性，$$c_1, c_2, ..., c_p$$是每个分类的one-hot 表示，k是anchor box数量。

3. 使用非极大值抑制，移除重叠的框。

### R-CNN： Region With Convolutional Neural Networks
物体检测算法，首先把图像分割成了可能相关的bounding box，然后在每一个bounding box里面去运行检测算法。

但是太慢了。

### 熊脸识别？？？

验证问题：
- 是不是同一个人？
- 一个一个地去找

识别问题：
- 这个是不是在数据库当中？
- 一对多查找

One shot learing：脸验证算法，使用有限的训练集，来学习一个相似函数，量化两幅图像之间有多大的差别。相似函数被称为d(img1,img2)

Siamese网络：致力于学习如何编码图像，然后量化两幅图像之间的差异。比如输入图像是x(i), 编码的输出是f(x(i))

三元损失：三元损失l的计算是三个损失的表示：A（anchor），P（positive）和N（negative）。anchor和positive例子属于相同的分类，negative样本属于另一个。通过调用$$\alpha \in \mathbb{R}^+$$这个边际参数，这个三元损失可以这么定义：

$$
\ell(A, P, N)=\max (d(A, P)-d(A, N)+\alpha, 0)
$$

（有点像hinge loss，multi class svm这种）

### 风格转移

在一个给定的层上，激活被称为：$$a^l$$，shape是$$n_H*n_w*n*c$$

内容损失：content图和生成的图的，每一层的二范数

$$
J_{\text { content }}(C, G)=\frac{1}{2}\left\|a^{[l](C)}-a^{[l](G)}\right\|^{2}
$$

首先定义克矩阵，是协方差的一种近似：
$$
G_{k k^{\prime}}^{[l]}=\sum_{i=1}^{n_{H}^{[l]}} \sum_{j=1}^{\boldsymbol{n}_{w}^{[l]}} a_{i j k}^{[l]} a_{i j k^{\prime}}^{[l]}
$$

风格损失：

$$
J_{\text { style }}^{[l]}(S, G)=\frac{1}{\left(2 n_{H} n_{w} n_{c}\right)^{2}}\left\|G^{[l](S)}-G^{[l](G)}\right\|_{F}^{2}=\frac{1}{\left(2 n_{H} n_{w} n_{c}\right)^{2}} \sum_{k, k^{\prime}=1}^{n_{c}}\left(G_{k k^{\prime}}^{[l](S)}-G_{k k^{\prime}}^{[l](G)}\right)^{2}
$$

总的损失函数：
$$
J(G)=\alpha J_{\text { content }}(C, G)+\beta J_{\text { style }}(S, G)
$$

### 计算trick

gans：可以使用来生成新的样本。
resnet： resblock：

$$
a^{[l+2]}=g\left(a^{[l]}+z^{[l+2]}\right)
$$

inception network：1x1卷积来降低计算复杂度。

### RNN
优点：
- 处理任意长度输入
- 模型大小不随着输入的大小而变化
- 计算考虑了历史信息
- 权重共享

缺点：
- 计算很慢
- 很难利用很久以前的数据
- 当前状态对未来的输入没有预测

应用RNN：NLP，语音识别

1生成1：传统神经网络
1生成多：音乐生成
多生成1：情绪分类
多生成多（时间是一致的）：名字实体识别
多生成多（时间不一致）：机器翻译

损失函数：

$$
\mathcal{L}(\widehat{y}, y)=\sum_{t=1}^{T_{y}} \mathcal{L}\left(\widehat{y}^{<t>}, y^{<t>}\right)
$$
就是每一次的计算，把损失函数加起来嘛。

向后投影：
$$
\frac{\partial \mathcal{L}^{(T)}}{\partial W}=\sum_{t=1}^{T}\left.\frac{\partial \mathcal{L}^{(T)}}{\partial W}\right \vert _{(t)}
$$

引入长期依赖：

激活函数：sigmoid，tanh，relu
在RNN当中，梯度消失和梯度爆炸一直存在，这是因为RNN不能很好的捕捉长期依赖引起的。多个梯度相乘，最大特征值大于1会爆炸，最大特征值小于1会消失。

对于爆炸的梯度来说，可以进行梯度裁剪，也就是对梯度进行限幅。

对于消失的梯度来说，需要引入长期记忆。
$$
\Gamma=\sigma\left(W x^{<t>}+U a^{<t-1>}+b\right)
$$

门的类型| 功能| 在什么地方使用
--|--|--
门门g|过去的数据对现在的影响|GRU，LSTM
关联门i|丢掉过去的信息|GRU，LSTM
遗忘门f|删除不删除这个cell|LSTM
输出门o|对cell的影响|LSTM

Gated Recurrent Unit(GRU)
Long Short-Term Memory units(LSTM)

*****************

还是不懂上面的qwq

### 单词的表示

V表示词汇表，\|V\|是大小

表示方法：
- one-hot表示，比如(泰迪熊，软，书)，记做$$o_w$$，是一个naive的表示，没有揭示信息的关联性
- word embedding：一个单词用几个浮点数来表示，考虑了单词之间的关联，用$$e_w$$表示。

用embedding matrix来把word embedding的单词映射到one-hot空间上面去，$$e_w = Eo_w$$。

word2vec：学习单词嵌入，通过估计给出单词被其他单词包围的可能性。流行的模型有：
- skip-gram
- nagative sampling
- CBOW

skip-gram：监督学习任务，通过评估任意给出的目标单词t在给定上下文c当中的顺序来学习单词嵌入，$$\theta_t$$和t有关，概率

$$
P(t | c)=\frac{\exp \left(\theta_{t}^{T} e_{c}\right)}{\frac{|V|}{\sum} \exp \left(\theta_{j}^{T} e_{c}\right)}
$$

但是，把整个词典都加起来，计算的代价太昂贵了。CBOW是另一个word2vec的模型，使用周围的单词来预测给定的单词出现的概率。

负采样：这是一个二分类器的集合，使用logistic回归，评估一个给定的上下文和一个给定的目标单词有多大的可能同时出现。模型训练的时候，给出k个负样本和一个正样本，给一个上下文c和目标单词t，预测可以表达为：

$$
P(y=1 | c, t)=\sigma\left(\theta_{t}^{T} e_{c}\right)
$$

这种计算量比上面提到的skip-gram方法要小

GloVe： global vectors单词表示，也是一个单词嵌入技术，使用同时出现矩阵X来表示，$$X_{ij}$$表示目标单词i在上下文j当中出现的次数。损失函数这么来定义：

$$
J(\theta)=\frac{1}{2} \sum_{i, j=1}^{|V|} f\left(X_{i j}\right)\left(\theta_{i}^{T} e_{j}+b_{i}+b_{j}^{\prime}-\log \left(X_{i j}\right)\right)^{2}
$$

f是权重函数，也就是说如果$$X_{ij}=0$$的话，$$f(X_{ij})=0$$。
对称的e和θ在这个模型当中表示的话，最后的单词嵌入的表示为：

$$
e_{w}^{(\text { final })}=\frac{e_{w}+\theta_{w}}{2}
$$

学习单词嵌入的单个组件不一定需要能够让人理解qwq

### 比较单词

余弦相似性：
$$
\text { similarity }=\frac{w_{1} \cdot w_{2}}{\left\|w_{1}\right\|\left\|w_{2}\right\|}=\cos (\theta)
$$

t-SNE：t-distributed Stochastic Neighbor Embedding t分布随机邻居嵌入，为了减少高维嵌入到低维。实际上，通常在2维平面使用可视化单词向量。

### 语言模型
估计一个句子的可能性
n-grams模型：naive方法，量化在训练尸体（文集）当中的可能性，通过数在训练集当中出现的次数。

perplexity（困惑）：语言模型使用很复杂的尺度去评估，有一种叫做PP，可以被解释为被单词书T归一化后的，数据集概率的倒数，如果这个PP很小的话，就说明这个模型越好：
$$
\mathrm{PP}=\prod_{t=1}^{T}\left(\frac{1}{\sum_{j=1}^{|V|} y_{j}^{(t)} \cdot \widehat{y}_{j}^{(t)}}\right)^{\frac{1}{T}}
$$
在t-SNE当中经常被使用

### 机器翻译

和语言模型相似，除了他有一个编码器网络，代替了之前的。所以有的时候被解释为条件语言模型。这个目标是找到这么一个句子，使得：

$$
y=\underset{y<1>, \ldots, y}{\arg \max } P\left(y^{<1>}, \ldots, y^{<T_{y}>} | x\right)
$$

beam搜索: 启发式搜索算法，找到最可能的句子y，给定一个输入x：

1. 首先找到得分最高的前B个单词：$$y^{<1>}$$
2. 计算条件概率$$y<k>| x, y<1>, \ldots, y<k-1>$$
3. 保持最好的B个组合$$x, y^{<1>}, \ldots, y^{<k>}$$

beam宽度：更大的B会导致更好的结果，但是会变慢，并且内存也会增加。小的B会差点。标准的B是10

长度正则化：为了改进数值稳定性，通常使用这么一个正则化目标，叫做对数似然目标：

$$
\text { Objective }=\frac{1}{T_{y}^{\alpha}} \sum_{t=1}^{T_{y}} \log \left[p\left(y^{<t>} | x, y^{<1>}, \ldots, y^{<t-1>}\right)\right]
$$

参数$$\alpha$$可以看做是一个软化剂，值通常在0.5到1之间。

误差分析：

当得到的预测值$$\hat{y}$$太差了，有可能是这么几个原因：

如果$$P\left(y^{*} | x\right)>P(\widehat{y} | x)$$的话，可能是因为Beam寻找太差了，需要增加beam宽度。

如果$$P\left(y^{*} | x\right) \leqslant P(\widehat{y} | x)$$的话，可能是RNN太差了，可能的解决方案有：
- 使用更复杂的网络结构
- 正则化
- 得到更多的数据

bleu分数：bilingual evaluation understudy双向评估替补分数，量化了机器转化有多好，通过计算相似性分数，基于几个n克精度的分数。定义：
$$
\text { bleu score }=\exp \left(\frac{1}{n} \sum_{k=1}^{n} p_{k}\right)
$$
这里$$p_n$$是bleu分数，在n克上，这么来定义：
********************

## 调参玄学：

### 数据增强：

- 原图
- 翻转
- 旋转
- 随机剪裁
- 颜色转移
- 添加噪点
- 信息损失
- 对比度增强

### batch norm

$$
x_{i} \longleftarrow \gamma \frac{x_{i}-\mu_{B}}{\sqrt{\sigma_{B}^{2}+\epsilon}}+\beta
$$

通常是在全连接层或者卷积层后加，在非线性层之前加。目的是使得数据充分激活。得到更高的学习率，减少对初始化过程的依赖。

### 训练的时候

定义： 
- epoch（刷完一遍整个训练集）
- mini-batch gradient descent（不是针对整个训练集，而是分成小批次这样子，batchnum也是一个超参数可以调整）
- loss函数（估计实际的输出有多少被模型正确估计程度）
- 交叉熵：二分类问题，定义：
$$
L(z, y)=-[y \log (z)+(1-y) \log (1-z)]
$$

找到最好的权重：

向后传播，链式法则，更新权重
$$
w \longleftarrow w-\alpha \frac{\partial L(z, y)}{\partial w}
$$

- 首先拿一批数据，执行前向
- 向后传播，计算每个权重关于loss的梯度，
- 使用梯度对网络权重进行更新。

### 参数初始化

权重：Xavier初始化：不是纯用随机数，Xavier初始化考虑了网络的结构对权重进行初始化

迁移学习：训练一个网络很费劲，比较好的方法是把我们的数据迁移学习过去。但是要考虑到自己的数据集和得到这个网络训练时候的数据集之间的差异：

- 训练数量少：冻结所有层，训练softmax的输出
- 中等：冻结大部分的层，只训练在最后几层和softmax层的权重
- 很大：用预训练网络权重的当做初始值，训练所有层。

优化收敛：
学习率：可以自适应的调整。现在最常用的方法是Adam
自适应学习率：让学习率随着时间变化减小。

Momentum：2参数要调整
RMSprop
Adam

其他还有Adadelta，Adagrad和SGD

### 正则化

dropout，避免网络找到很多不必要的特征。
大多数的深度学习框架保持概率为1-p

权重正则化：
1范数，2范数，结合

1范数对变化选择很有效。
2范数使得每个参数变小。

过早停止也是一种正则化。

### 实践

1. 对于小数据集的话过拟合。当我们调试一个模型的时候，一个快速的测试是看看网络结构本身有没有什么问题。特别的，为了保证模型能够很好的被训练，选择一个mini-batch进入这个网络，来看看能不能过拟合。如果不能的话，意味着这个模型要不然就是太复杂了，或者是不够复杂，甚至不能使得小batch过拟合，更不用说正常规模的数据集了。

2. 梯度检查。也就是cs231作业里面的梯度检查工具。检查反向传播的时候对不对。（不过现在的框架都自动求导了，应该不会有bug把qwq）。还有sanity-check，比如一开始因为是对权重进行小范围的初始化，所以loss算出来应该是-log(0.1)这种。