---
layout: post
title:  "写一个爬ppt的爬虫"
date:   2019-04-14 16:55:03 +0800
categories: common
---

<script type="text/javascript" async
  src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/MathJax.js?config=TeX-MML-AM_CHTML">
</script>

今天开始想学cv，在[awesome_computer_vision](https://github.com/jbhuang0604/awesome-computer-vision)里面找的话，找到一门课程，想要好好学习一下。

这个ppt的网址在[这里](https://courses.engr.illinois.edu/cs543/sp2015/)，虽然是很陈旧的知识了吧，但是我感觉也不是没有用处，我觉得需要了解一些图像处理方面，频域的知识。

另外[这个](https://cs.nyu.edu/~fergus/teaching/vision/index.html)好像也不错

首先的一个问题就是ppt下载不下来，一个一个点太费劲了qwq。

所以就写个爬虫吧。

## 肯定要用beautifulsoup了

参考我之前的东西，写一个模板。

首先要读文件，打开网页。
打开网页的方式：

```py
net_url = 'https://courses.engr.illinois.edu/cs543/sp2015/'
html = requests.get(net_url)
print(html.text)
soup = BeautifulSoup(html.text)
```

然后找到所有的`a`标签。访问他的`href`属性。
`pdf_urls = soup.find_all('a')`
`href = item.get('href')`

追加到列表当中，然后下载。当然需要对里面的扩展名进行分类，如果是需要的扩展名的话就下载。传入一个`filter`字典来实现。

下载的话，遍历整个列表。try except里面的东西，也就是使用`request`来得到相应的文件。

```py
r=requests.get(url,stream=True)
new_name = name.replace('%20','_')
with open(new_name, 'wb') as f:
    print('正在下载%s这个文件...' % ( new_name ,))
    for chunk in r.iter_content(chunk_size=4096):
        if chunk:
            f.write(chunk)
            f.flush()
```

注意名字是怎么样的。

这是完整代码：

```py
#  coding: utf-8

##################################################################
#  这个文件是   某项目的一部分
#  作者： zbf
#  email: hgkdzbf6@163.com
#  创建时间：2019-03-17 17:34:04
#  最近修改时间：2019-03-17 17:39:22
##################################################################

import sys
import os
import cv2

import requests
from bs4 import BeautifulSoup

def get_name(filter):
    net_url = 'https://courses.engr.illinois.edu/cs543/sp2015/'
    html = requests.get(net_url)
    soup = BeautifulSoup(html.text)
    pdf_urls = soup.find_all('a')
    pdf_list = []
    for i, item in enumerate(pdf_urls):
        href = item.get('href')
        suffix = href.split('.')[-1]
        if suffix in filter:
            if href[0:4]=='http':
                pdf_list.append(href)
            else:
                pdf_list.append(net_url + href)
    print(pdf_list)
    return pdf_list

def download(pdf_list):
    for url in pdf_list:
        name = url.split('/')[-1]
        try:
            r=requests.get(url,stream=True)
            new_name = name.replace('%20','_')
            with open(new_name, 'wb') as f:
                print('正在下载%s这个文件...' % ( new_name ,))
                for chunk in r.iter_content(chunk_size=4096):
                    if chunk:
                        f.write(chunk)
                        f.flush()
            print('文件%s已经下载完成' % (new_name,))
            r.close()
        except Exception:
            pass

if __name__=='__main__':
    download(get_name({'zip'}))
```