---
layout: post
title:  "cs231n 第三次作业"
date:   2019-04-07 19:01:34 +0800
categories: AI
---

<script type="text/javascript" async
  src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/MathJax.js?config=TeX-MML-AM_CHTML">
</script>

## 做作业的顺序

1. 图像标注，使用Vanilla RNN来进行标注，文件是`RNN_Captioning.ipynb`，操作的数据集是MS-COCO
2. 图像标注，使用LSTM，文件是`LSTM_Captioning.ipynb`，操作的数据集是MS-COCO
3. 网络可视化：特征(saliency)图，分类可视化，Fooling图？，文件是`NetworkVisualization-Tensorflow.ipynb`或者是`NetworkVisualization-PyTorch.ipynb`
4. 风格迁移：`StyleTransfer-Tensorflow.ipynb`和`StyleTransfer-PyTorch.ipynb`
5. GAN：`GANS-Tensorflow.ipynb`和`GANS-PyTorch.ipynb`

## 开始做第一个，RNN

总结来说，作业的难度是在逐渐下降的，之前的assignment1和assignment2的话，由于全部是数学推导，求起来确实很费劲qwq。

RNN也是挺费劲的qwq。

第二个block让装一个叫做`h5py`的东西，这是因为COCO数据集的格式就是h5py格式的。

第四个block，当中的图像显示也值得说一下吧。

下面这个代码`非常重要`，这是因为从COCO数据集上下载图片的话，要用到一个`decode_captions`函数，需要ssl验证。下面这段代码可以使得以后的代码不需要ssl验证。

```python
import ssl
ssl._create_default_https_context = ssl._create_unverified_context
```

讲讲这个画图的函数：

```python
captions, features, urls = sample_coco_minibatch(data, batch_size=batch_size)
for i, (caption, url) in enumerate(zip(captions, urls)):
    plt.imshow(image_from_url(url))
    plt.axis('off')
    caption_str = decode_captions(caption, data['idx_to_word'])
    plt.title(caption_str)
    plt.show()
```

其实就是先画一张图，然后加个caption。图像标注。

这一段文字一开始的话是以一个`<start>`为开始，以一个`<end>`为结束，中间有可能出现一个叫做`<unk>`的东西，应该是从图片当中判断不出是什么东西，所以给一个位置占住这样`unknown`。

参考[这里](https://github.com/FortiLeiZhang/cs231n/blob/master/document/Assignment%203%20--%20RNN_Captioning.md)，有：

> train dataset 中有 82783 张图片，每一张图片对应多个 caption，共有 400135 个caption，每一个 caption 最多包含 17 个整形数字，每一个数字通过 idx_to_word 对应到一个单词。idx_to_word 是一个 list，每一个位置上对应一个单词，其中位置0-3分别是特殊字符 <NULL>, <START>，<END>，<UNK>。所有的 caption 都是以 <START> 起，以 <END> 止，如果不足17个单词，那么在 <END> 以后补 <NULL>，不在 idx_to_word 中的记为 <UNK>。

> train feature 是直接取自 VGG16 的第 fc7 层，也就是从 4096 映射到 1000 个 class 的前一层，所以是 4096 维的。这里为了减少计算量，使用 PCA 将维度减小到 512 维。



### Vanilla RNN

这个应该是一开始的RNN才对，需要在`rnn_layers.py`文件当中写一个rnn吧。

首先是`rnn_step_forward`：

也是比较简单qwq，记住公式：

$$
h_{t}=\tanh \left(W_{h h} h_{t-1}+W_{x h} x_{t}\right)
$$

所以代码就是这样：

{% codeblock lang:python %}
h = x.dot(Wx) + prev_h.dot(Wh) + b
next_h = np.tanh(h)
cache = (x, prev_h, Wx, Wh, b, next_h)
{% endcodeblock %}

最好是每一个中间结果都保存到cache当中，目前除了需要求导的一系列变量之外，还有`next_h`。

然后就是`rnn_step_backward`：

需要求导的有这么多：`dx, dprev_h, dWx, dWh, db`

注意：$$y = tanh(x)$$的导数为：$$1-y**2$$
复习：$$y = sigmoid(x)$$的导数为：$$(1-y)*y$$

这对求导有帮助：

然后加法，乘法原则，注意shape，也就算出来了。

{% codeblock lang:python %}
# 首先把cache展开
(x, prev_h, Wx, Wh, b, next_h) = cache
# 首先计算tanh的导数
# h = x.dot(Wx) + prev_h.dot(Wh) + b
dscore = (1 - next_h**2)*dnext_h
db = np.sum(dscore, axis=0)
dxWx = dscore
dhWh = dscore
dx = dxWx.dot(Wx.T)
dWx = x.T.dot(dxWx)
dprev_h =dhWh.dot(Wh.T)
dWh = prev_h.T.dot(dhWh)
{% endcodeblock %}

下面是整个forward：

这个输入x变成了`N, T, D`，每一步都需要进入RNN当中进行计算，上一步的输出是下一步的输入。

```py
N, T, D = x.shape
H = b.shape[0]
h = np.zeros((N, T+1, H))
h[:,0,:] = h0
for t in range(T):
  h[:, t+1, :], _ = rnn_step_forward(x[:, t, :], h[:,t,:], Wx, Wh, b)
h = h[:,1:,:]
cache = (x, h0, Wx, Wh, b, h)
```

下面是整个的backward（其实是抄网上的qwq），每一次反向传播都会更新Wx和Wh以及b。step_backward还有两个输出，dx用来更新每一个时间点的值，dprev_h用来更新dnext_h，

{% codeblock lang:python %}
(x, h0, Wx, Wh, b, h) = cache
N, T, H = dh.shape
D = x.shape[2]

dprev_h = np.zeros((N,H))
dx = np.zeros_like(x)
dh0 = np.zeros_like(h0)
dWx = np.zeros_like(Wx)
dWh = np.zeros_like(Wh)
db = np.zeros_like(b)
next_h = h[:,T-1,:]
for i in range(T):
  t = T-1-i
  if t==0:
    prev_h = h0
  else:
    prev_h = h[:,t-1,:]
  step_cache = (x[:, t, :], prev_h, Wx, Wh, b, next_h)
  next_h = prev_h
  dnext_h = dh[:,t,:] + dprev_h
  dx[:,t,:], dprev_h, dWxt, dWht, dbt = rnn_step_backward(dnext_h, step_cache)
  dWx, dWh, db = dWx + dWxt, dWh+dWht, db+dbt

dh0 = dprev_h
{% endcodeblock %}

### Word embedding

> Word Embedding

  word embedding 的作用实际上是一个空间的映射，它将用 int 型数字编码的单词，映射到一个 D 维度的 float 型数字编码的空间。具体来说，将一个单词用一个整型数字来表示，所有的单词组成了一个词汇表，这个词汇表的词汇量大小是 V，词汇表中每一个单词的编码值应该在 [0, V) 范围内。然后定义一个形如 (V, D) 的映射关系 W。W 中的每一行是一个 D 维的 float 向量，对应一个单词在词汇表中的 offset。这样，就将一个单词由一个整型数字表示映射到一个 D 维的 float 向量。

  举个例子，单词 cat 在词汇表中的 offset 为 10，在 W 中，第10行的向量是 $[0.21428571, 0.28571429, 0.35714286]$，那么单词 cat 就映射成了一个三维的向量 $[0.21428571, 0.28571429, 0.35714286]$。

  在这里，单词在词汇表中的 offset 是固定的，而映射关系 W 是通过学习而来的参数。

forward

一般用向量来表示一个单词，词库的每一个单词都用一个向量来表示，这些向量会用下面得到系统计算出来。`word_embedding_forward`这个函数。

N: mini-batch数
T: 序列的长度为T
V: 单词数
D: 每个单词的维数

核心代码就一行：

{% codeblock lang:python %}
out[:,:] = W[x[:,:]]
{% endcodeblock %}

backward

使用np.add.at，在某一个位置加上前面传过来的值。

{% codeblock lang:python %}
(x, W_shape) = cache
dW = np.zeros(W_shape)
np.add.at(dW,x,dout)
{% endcodeblock %}

数值稳定版本的sigmoid函数：

{% codeblock lang:python %}
def sigmoid(x):
    """
    A numerically stable version of the logistic sigmoid function.
    """
    pos_mask = (x >= 0)
    neg_mask = (x < 0)
    z = np.zeros_like(x)
    z[pos_mask] = np.exp(-x[pos_mask])
    z[neg_mask] = np.exp(x[neg_mask])
    top = np.ones_like(x)
    top[neg_mask] = z[neg_mask]
    return top / (1 + z)
{% endcodeblock %}

### Temporal Affine layer

forward:

x: N, T, D
w: D, M
b: M,

out: N, T, M

做的事情是：首先把x的前两个向量合并，然后再全连接层的操作。

{% codeblock lang:python %}
N, T, D = x.shape
M = b.shape[0]
out = x.reshape(N * T, D).dot(w).reshape(N, T, M) + b
cache = x, w, b, out
{% endcodeblock %}

backward:

也是注意shape，dout的shape是N, T, M
dx: N, T, D
dw: D, M
db: M

乘法原则，加法原则传过去，大方向是乘法。和forward相结合来看应该就好理解了。

{% codeblock lang:python %}
x, w, b, out = cache
N, T, D = x.shape
M = b.shape[0]

dx = dout.reshape(N * T, M).dot(w.T).reshape(N, T, D)
dw = dout.reshape(N * T, M).T.dot(x.reshape(N * T, D)).T
db = dout.sum(axis=(0, 1))
{% endcodeblock %}

softmax_loss:

传入一个mask，来表示一个单词可能为空白，所以空白的就不显示了，也不会进入损失和梯度计算。

### 用RNN来实现图像标注 

这次需要修改`rnn.py`这个文件。

首先初始化：

输入：
- word_to_idx, 词典，给出词汇表。包含V个条目，把每个字符串映射成唯一的，在[0,V)区间的整数
- idx_to_word: `{i: w for w, i in word_to_idx.items()}`
self._null = word_to_idx['<NULL>']
self._start = word_to_idx.get('<START>',None)
self._end = word_to_idx('<END>',None)

单词向量的权重都是被初始化成randn
bias被初始化成0

lstm的权重数量是rnn的4倍，因为有fiog层。

W_embed: 单词向量的权重
W_vocab: 词典当中的权重

然后是loss：

输入是captions_in，从0开始到倒数第一个
输出时captions_out, 从1开始到最后一个

CaptionCNN的算法：

1. 使用affine变换，从图像特征当中，计算初始隐含层的状态，这一层的输出应该是(N,H)
2. 使用word_embedding层，来把在captions_in中的单词从序号转化成向量，输出形状应该是(N,T,W)
3. 使用Vanilla RNN或者lstm，处理输入单词向量的序列，对所有的隐含层状态计算(N,T,H)
4. 使用temporal全连接层，计算在每个时刻，词汇表当中的得分，形状为(N,T,V)
5. 使用temporal softmax，计算loss，使用captions_out，忽略输出的字符为`<NULL>`的字符

{% codeblock lang:python %}
ao, ac = affine_forward(features, W_proj, b_proj)
weo, wec = word_embedding_forward(captions_in, W_embed)
if self.cell_type == 'rnn':
  out, cache = rnn_forward(weo,ao,Wx,Wh,b)
elif self.cell_type == 'lstm':
  out, cache = lstm_forward(weo,ao,Wx,Wh,b)
tao,tac = temporal_affine_forward(out,W_vocab,b_vocab)
loss, dtao = temporal_softmax_loss(tao,captions_out,mask)
dout, grads['W_vocab'], grads['b_vocab'] = temporal_affine_backward(dtao, tac)
if self.cell_type == 'rnn':
  dweo, dao,grads['Wx'], grads['Wh'], grads['b'] = rnn_backward(dout,cache)
elif self.cell_type == 'lstm':
  dweo, dao,grads['Wx'], grads['Wh'], grads['b'] = lstm_backward(dout,cache)
grads['W_embed']= word_embedding_backward(dweo,wec)
dfeatures, grads['W_proj'], grads['b_proj'] = affine_backward(dao,ac)
{% endcodeblock %}

RNN的缺点：

每一次反向传播都要乘一次权重矩阵，如果权重矩阵的最大特征值大于1的话，梯度爆炸；如果小于1，梯度消失。

RNN关于梯度爆炸的解决方法有：

- 当梯度的某个维度的绝对值大于某个上限时，裁剪为这个上限。
- 梯度的L2范数大于上限后，让梯度除以范数，避免过大。

LSTM里面，反向传播c的时候，只需要进行元素级别的乘除法就可以了，不需要权重矩阵的参与。和ResNet很相似。

Highway Networks。

以后学习一下GRU：

### 还有一个问题，图片的特征是怎么得到的？

> RNN for image captioning

  训练一个 RNN 来做 image caption 需要输入图片的 feature 和 caption，并用同样的 caption 做 label。整个过程包含三个学习过程，需要训练三组参数。分别为：1.图片的 feature 向 h(0) 的 projection；2. RNN；3. 单词的 projection。

  图片 feature 向 h(0) 的 projection

  输入的图片 feature 是从 VGG 的 FC7 层截取的，原始值是 4096 维的，为了减小计算量，通过 PCA 降维到 512 维，而 RNN 的 h(0) 是 H 维的。从形如 (N, D) 的 feature 映射到 (N, H) 的隐状态，需要一组形如 (D, H) 的参数.

## LSTM

### step forward

照着写就行了，cache当中这么写：

{% codeblock lang:python %}
cache =  (i, f, o, g, t, prev_c, prev_h, Wx, Wh, x)
{% endcodeblock %}

### step backward

对照图和公式一起来就行了。

{% codeblock lang:python %}
(i, f, o, g, t, prev_c, prev_h, Wx, Wh, x) = cache
do = dnext_h * t
dt = dnext_h * o * ( 1- t**2 ) + dnext_c
df = prev_c * dt
di = g * dt
dg = i * dt
dprev_c = f * dt
dai = i*(1-i)*di
daf = f*(1-f)*df
dao = o*(1-o)*do
dag = (1-g**2)*dg
da = np.hstack((dai,daf,dao,dag))
dWx = x.T.dot(da)
dWh = prev_h.T.dot(da)
db = np.sum(da,axis=0)
dx = da.dot(Wx.T)
dprev_h = da.dot(Wh.T)
{% endcodeblock %}

### forward

关键就是更新h和c，也没啥（我当时抄答案也做了好几天qwq）

{% codeblock lang:python %}
N, T, D = x.shape
H = b.shape[0]//4
h = np.zeros((N, T, H))
cache = {}
prev_h = h0
prev_c = np.zeros((N, H))

for t in range(T):
  next_h, next_c, cache[t] = lstm_step_forward(x[:,t,:], prev_h, prev_c, Wx, Wh, b)
  prev_h = next_h
  prev_c = next_c
  h[:,t,:] = prev_h
{% endcodeblock %}

### backward

关键是一样的，更新也和rnn比较相似。注意最后给dh0梯度

{% codeblock lang:python %}
N,T,H = dh.shape
_, _, _, _, _, _, _, _, _, x = cache[T-1]
D = x.shape[1]
dprev_h = np.zeros((N, H))
dprev_c = np.zeros((N, H))
dx = np.zeros((N, T, D))
dh0 = np.zeros((N, H))
dWx = np.zeros((D, 4*H))
dWh = np.zeros((H, 4*H))
db = np.zeros(4*H)

for i in range(T):
  t = T-1-i
  dnext_h = dh[:,t,:] + dprev_h
  dnext_c = dprev_c
  dx[:,t,:],dprev_h,dprev_c,dWxt,dWht,dbt = lstm_step_backward(dnext_h,dnext_c,cache[t])
  dWx, dWh, db = dWx+dWxt, dWh+dWht, db+dbt

dh0 = dprev_h
{% endcodeblock %}

## Network Visualization

### 特征图PyTorch版本

先写一下torch的用法，这里牵涉到一个gather的函数，我参考[这里](https://blog.csdn.net/edogawachia/article/details/80515038)

这里解释是这样的
`torch.gather(input, dim, index, out=None) → Tensor`

dim就是tensorflow， numpy里面的axis，index就是索引。
这个index是input沿着dim这个轴的索引，index的元素不能超过这个轴的shape

squeeze: 就是把向量当中的1维向量去掉，和numpy里面差不多。

总之这个是能够实现像`s[np.arange(N),y]`这样的功能。

计算特征地图的时候，是这么来运行的：

`model.eval()` 保证这个模型在测试模式，也就是不对其中的参数进行更新了。

`X.requires_grad_()`, X需要求导。

这个模型最后计算出来，就是每个项目的得分，所以就有：`scores = model(X)`，scores的shape是[5,1000]，也就是5张图片（batch_size），然后1000个输出的全连接层把。这1000个输出就是分类特征。

正确的得分，应该是1000分类特征当中最明显的一个，y的shape是(5,)。correct_scores的shape是(5,)

然后计算loss，这个loss是通过把所有正确的得分加起来计算出来的。

然后`loss.backward()`,会给所有的项目backward，直到最后的X。

取的X梯度的方法是：`X.grad.data`

最后得到的输出是梯度的三个通道的值，这次应该在三个通道上进行

最后画图的时候使用了一个`plt.cm.hot`，colormap一样的东西，有点像opencv里面的`applycolormap`，其实就是涂色。

为了找到使得分数最大的图像，可以执行梯度上升（这句没错），但也可以使用特征图来更新图片（这句就错了，因为特征图求的时候，最后有一个求极大值，然后分散到各个通道的时候就不能恢复原来的梯度了所以不行qwq）

### 特征图 Tensorflow版本

### Fooling图 Pytorch

Fooling的意思是，使用图像梯度来生成fooling图，给一张图片和一个随便的分类，在整张图片上执行梯度上升，使得目标分类的分数最大，当这个网络把这张图片识别成了目标分类的时候，停止。这样的话，这个网络就被愚弄了qwq。

完成这个`make_fooling_image`函数：

- 输入X: 1,3,224,224
- 目标标签: target_y，一个整数，在[0,1000)范围内
- model

首先复制一份原图：
`X_fooling=X.clone()`

然后需要求导：
`X_fooling = X_fooling.requires_grad_()`

学习率为1
`learning_rate=1`

进入一个循环，首先得到模型输出的分数：
`score=model(X_fooling)`

然后预测，预测的方法就是在1000个分类当中选择最大的分数，预测结果是最大分数对应的标签，scores的shape是(1,1000)，所以在dim或者axis=1上面求极大值。

pytorch当中，执行`score.max(dim=1)`的时候，返回的是一个元组，第一个元素是最大值的值，第二个元素是最大值的位置，我们正好要的就是最大值的位置。

最后需要对tensor解引用，所以还需要在加一个[0]

结果是：
`pred = score.max(dim=1)[1][0]`

然后判断，预测的标签和目标标签是否相同，相同的话，算法结束break，否则继续

```python
if pred == target_y:
    break
```

然后向后投影：
`score[0,target_y].backward()`

然后更新X_fooling：
`X_fooling.data += learning_rate * dx / dx.norm()`

最后把导数变成0：
`X_fooling.grad.zero_()`

测试的时候：使用`torch.cat`，就是把两个向量连接起来。

然后再复习一下torch和numpy之间的转化：

这里有一些`import torchvision.transforms as T`，图像的导入是这么做的：
{% codeblock lang:python %}
def preprocess(img, size=224):
    transform = T.Compose([
        T.Resize(size),
        T.ToTensor(),
        T.Normalize(mean=SQUEEZENET_MEAN.tolist(),
                    std=SQUEEZENET_STD.tolist()),
        T.Lambda(lambda x: x[None]),
    ])
    return transform(img)
{% endcodeblock %}

Resize就是resize

np转torch：img.ToTensor()
torch转np: img.numpy()
Normalize就是Normalize
`SQUEEZENET_MEAN`和`SQUEEZENET_STD`分别是每个颜色通道的方差和均值。
`[:,None]`的效果是将二维数组按每行分割，最后形成一个三维数组。

图像的输出是这么做的：
{% codeblock lang:python %}
def deprocess(img, should_rescale=True):
    transform = T.Compose([
        T.Lambda(lambda x: x[0]),
        T.Normalize(mean=[0, 0, 0], std=(1.0 / SQUEEZENET_STD).tolist()),
        T.Normalize(mean=(-SQUEEZENET_MEAN).tolist(), std=[1, 1, 1]),
        T.Lambda(rescale) if should_rescale else T.Lambda(lambda x: x),
        T.ToPILImage(),
    ])
    return transform(img)
{% endcodeblock %}

反正就和输入是逆操作qwq。

### tensorflow版本

### 分类可视化，Pytorch版本

从随机噪声开始，对图像执行梯度上升，可以生成一个图像，这个图像最终会变成目标分类最大的图像。另外有论文能够显著改进生成图像的质量。

具体来说，$$I$$是图像，$$s_y(I)$$是一个卷积网络配置的图像的针对类别y的分数。

注意到这些分数并没有归一化，所以不能够作为分类的预测。这里想要生成这么一张图，使得：
$$I^* = \arg \max_I (s_y(I) - R(I))$$

R（有可能是隐含的）是正则项：(我们想要使得正则化系数最小)。然后解这个优化问题，使用梯度上升，计算梯度来生成相应的图像。使用明确的L2范数来使得这个形式：

$$R(I) = \lambda\|\| I \|\|^2_2$$

文献3中还提到一个不明确的正则化建议，周期性的模糊这个生成的图像。在这个图像上使用梯度上升，可以解决这个问题。

定义了一个jitter函数，抖动这个图像，这个图像的上面和下面，或者左边和右边进行互换。

然后自己写了些，发现基本都对，有这么几个地方不对：

1. 更新图片的时候，需要把上一个时刻的梯度除以他的范数。（当然我觉得，对梯度进行截断也是可以的）
2. 最后要注意的一点，就是最后的梯度要置为0的。

后面的就直接都能够运行了。

## Style Transfer PyTorch 版本

反正就是两幅图像的风格转换吧qwq

特征提取使用SqueezeNet，因为这个网络很小，并且能达到AlexNet的精度。

写了一个提取特征的函数`extract_features`

- x: NCHW
- cnn: 模型
- features: 是一个列表，features[i]的shape是(N, C_i, H_i, W_i)，这是因为不同的features包含不同的channel数量，空间维度也不一样。

看这个写的，有点像循环网络的意思，有一个prev_feat是x，然后有一个next_feat，并且一直保持更新。

### 计算loss

这个loss包含三部分

惩罚原图的内容变化，和风格图片的风格变化，使用混合loss函数，不再对参数（也就是W，b这种）而是对原图的像素值进行梯度下降处理。

- 上下文loss
- 风格loss
- 总的变化loss
原图：content img
风格图片：

先写一个content loss：content loss表示，生成的特征图和原图的特征图相差多少。我们只关心一层的内容的表示l，特征图：$$A^l$$的形状为1，C，H，W。C是在l层的滤波器数量或者通道的数量，H和W是宽度和高度，我们把所有的空间位置都变成一个1维向量，设当前的特征地图为$$F^l$$，形状为C，M，作为内容的来源，$$P^l$$表示内容的特征图。$$M=H*W$$是每个特征图的元素数量。Fl和Pl的每一列都表示，一个特定滤波器的向量化的激活，卷积到图像的整个位置。最后wc表示内容损失占总的loss的比重。

公式如下：

$$L_c = w_c \times \sum _{i,j}(F^l_{ij}-P^l_{ij})^2$$

表示成代码就是很简单了
`Lc = content_weight * torch.sum((content_current - content_original) ** 2)`

### 风格损失

克矩阵，表每个滤波器的响应的关系。克矩阵其实是的一种协方差矩阵的一种近似。我们想要我们生成图像的激活的统计值，来匹配我们风格图像的激活的统计值，怎么匹配呢？也就是通过和协方差矩阵近似来实现匹配。有很多方法可以做到这一点，但是克矩阵的话易于计算，并且实际效果和很好。

特征图Fl，形状为C，M，克矩阵的形状为C，C，元素之间的表示是：

$$G_{ij}^l = \sum_k F_{ik}^l F^{l}_{jk}$$

假设$$G^l$$是克矩阵，$$A_l$$是原始图像的克矩阵，w_l是权重，那么这个loss可以表示成：两个克矩阵的欧氏距离

$$L_s^l = w_l \sum_{i,j}(G_{i,j}^l - A_{i,j}^l)^2$$

然后最终的风格损失就是把每一层的克矩阵加起来

$$L_s = \sum _{l \in L}L^l_s$$

这里就要用到`torch.bmm`了，叫做`batch matrix multiply`。也就是只有后面的一些维度上面进行矩阵乘法，而别的维度不变。

[参考这里](https://daiwk.github.io/posts/pytorch-usage.html)

{% codeblock lang:python %}
(N, C, H, W) = features.shape
f_reshape = features.view(N, C, H*W)
gram = torch.bmm(f_reshape, f_reshape.transpose(1, 2))
if normalize:
    gram = gram / (H * W * C)
{% endcodeblock %}

最后计算总的style loss：

{% codeblock lang:python %}
loss = torch.tensor([0])
    for i, index in enumerate(style_layers):
        loss+= style_weights[i] * torch.sum((gram_matrix(feats[index]) - style_targets[i])**2, dim=(0,1,2))
    return loss
{% endcodeblock %}

### 总的变化正则项损失。

代码可以写成一行qwq：

`return tv_weight * ( torch.sum(torch.sum((img[:,:,:,1:] - img[:,:,:,:-1])**2, dim=(2,3) ),dim=(0,1)) + torch.sum(torch.sum((img[:,:,1:,:] - img[:,:,:-1,:])**2, dim=(2,3) ),dim=(0,1)))`

然后开始分析这个风格转化函数：

输入：

- content_image, 内容图片
- style_image, 风格图片
- image_size, 最小图像维度的大小，用来计算内容损失
- style_size, 风格的图片的大小
- content_layer, 内容层
- content_weight
- style_layers, 用来进行style loss计算的层
- style_weights
- tv_weight
- init_random, 从一个随机的起始图像开始生成噪声

风格层和内容层选择哪些，是输入已经确定好的qwq

首先从content img当中提取特征，选择内容层
{% codeblock lang:python %}
content_img = preprocess(PIL.Image.open(content_image), size=image_size)
feats = extract_features(content_img, cnn)
content_target = feats[content_layer].clone()
{% endcodeblock %}

从style img当中提取特征，选择风格层。
{% codeblock lang:python %}
style_img = preprocess(PIL.Image.open(style_image), size=style_size)
feats = extract_features(style_img, cnn)
style_targets = []
for idx in style_layers:
    style_targets.append(gram_matrix(feats[idx].clone()))
{% endcodeblock %}

初始化输出图像，用随机生成的点作为初始图像
`img = torch.Tensor(content_img.size()).uniform_(0, 1).type(dtype)`
或者用content img作为初始图像
`img = content_img.clone().type(dtype)`

需要对图像求梯度`img.requires_grad_()`

设定优化参数：
{% codeblock lang:python %}
initial_lr = 3.0
decayed_lr = 0.1
decay_lr_at = 180
{% endcodeblock %}

选择优化器：
`optimizer = torch.optim.Adam([img], lr=initial_lr)`

然后画图，先画两张图像，就是内容图和风格图。

{% codeblock lang:python %}
f, axarr = plt.subplots(1,2)
axarr[0].axis('off')
axarr[1].axis('off')
axarr[0].set_title('Content Source Img.')
axarr[1].set_title('Style Source Img.')
axarr[0].imshow(deprocess(content_img.cpu()))
axarr[1].imshow(deprocess(style_img.cpu()))
plt.show()
plt.figure()
{% endcodeblock %}

然后开始200次迭代了。

torch的tensor里面的data里面的clamp_属性的话，是夹紧的意思，也就是使得输入和输出的范围变到一个范围里面。

```py
if t < 190:
    img.data.clamp_(-1.5, 1.5)
```

设置优化器的梯度为0。
`optimizer.zero_grad()`

提取特征
`feats = extract_features(img, cnn)`

计算loss
{% codeblock lang:python %}
c_loss = content_loss(content_weight, feats[content_layer], content_target)
s_loss = style_loss(feats, style_layers, style_targets, style_weights)
t_loss = tv_loss(img, tv_weight) 
loss = c_loss + s_loss + t_loss
{% endcodeblock %}

然后这个loss.backward()
`loss.backward()`

执行优化：

```py
if t == decay_lr_at:
    optimizer = torch.optim.Adam([img], lr=decayed_lr)
optimizer.step()
```

然后画图：
{% codeblock lang:python %}
if t % 100 == 0:
    print('Iteration {}'.format(t))
    plt.axis('off')
    plt.imshow(deprocess(img.data.cpu()))
    plt.show()
{% endcodeblock %}

最后发现，从随机噪声当中初始化图像好像和从原图上初始化图像，最后都能够完成相应的操作吧。