---
layout: post
title:  "cs231n_3"
date:   2019-03-22 14:38:13 +0800
categories: AI
---

<script type="text/javascript" async
  src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/MathJax.js?config=TeX-MML-AM_CHTML">
</script>

## 一上来就是写一个网络是什么样的

### 全连接层

前向传播，后项传播

首先有这个：
{% codeblock lang:python %}
x:(N, d_1, d_2, ..., d_k)
w:(D, M)
b:(M)
s:(N, M)
{% endcodeblock %}

首先有：

$$s_{ij} = x_{ik}W_{kj}+b_j$$
这里
$$i=1,...,N; j=1,...,M; k=1,...,D$$

从上一步得到：$$ds$$

$$\frac{\partial s}{\partial b_j} = \sum_i ds$$

$$\frac{\partial s}{\partial x_ik} = ds*W^T$$

$$\frac{\partial s}{\partial W_kj} = x^T*ds$$

### ReLU层

前向：就是$$ s = \max (0,x)$$

后项：就是当x>0的时候梯度传播，为dout，当x<0的时候梯度不传播，为0

用`dx = np.where(x > 0, dout, 0).reshape(x.shape)`

### 夹心层：全连接之后ReLU

### 这是svm作为一个单元的网络

{% codeblock lang:python %}

def svm_loss(x, y):
    """
    Computes the loss and gradient using for multiclass SVM classification.

    Inputs:
    - x: Input data, of shape (N, C) where x[i, j] is the score for the jth
      class for the ith input.
    - y: Vector of labels, of shape (N,) where y[i] is the label for x[i] and
      0 <= y[i] < C

    Returns a tuple of:
    - loss: Scalar giving the loss
    - dx: Gradient of the loss with respect to x
    """
    N = x.shape[0]
    correct_class_scores = x[np.arange(N), y]
    margins = np.maximum(0, x - correct_class_scores[:, np.newaxis] + 1.0)
    margins[np.arange(N), y] = 0
    loss = np.sum(margins) / N
    num_pos = np.sum(margins > 0, axis=1)
    dx = np.zeros_like(x)
    dx[margins > 0] = 1
    dx[np.arange(N), y] -= num_pos
    dx /= N
    return loss, dx
{% endcodeblock %}

### softmax作为一个网络单元

{% codeblock lang:python %}
def softmax_loss(x, y):
    """
    Computes the loss and gradient for softmax classification.

    Inputs:
    - x: Input data, of shape (N, C) where x[i, j] is the score for the jth
      class for the ith input.
    - y: Vector of labels, of shape (N,) where y[i] is the label for x[i] and
      0 <= y[i] < C

    Returns a tuple of:
    - loss: Scalar giving the loss
    - dx: Gradient of the loss with respect to x
    """
    # 减去最大值
    shifted_logits = x - np.max(xn)
    # 所有得分的总和
    Z = np.sum(np.exp(shifted_logits), axis=1, keepdims=True)
    # 输出
    log_probs = shifted_logits - np.log(Z)
    probs = np.exp(log_probs)
    N = x.shape[0]
    loss = -np.sum(log_probs[np.arange(N), y]) / N
    dx = probs.copy()
    dx[np.arange(N), y] -= 1
    dx /= N
    return loss, dx

{% endcodeblock %}

### 完成一个二层网络，TwoLayerNet

这个比较难写的是softmax的写法，其他都很简单，最后不要把正则项忘掉。还有正则项不只是乘以0.5就可以了，还要乘以reg，我个傻蛋居然忘掉这个了qwq

## 看懂一个解算器，并使用这个解算器能够找到最好的超参数这样

这个解算器需要这么一些东西：
数据： `X_train, y_train, X_val, y_val`
模型：
    隐层数量，正则强度。

解算器需要：
    模型，
    数据，
    更新规则：sgd，
    optim_config：
        learning_rate：1e-3
    学习率下降率：0.95
    epoch数，
    batch_num，
    每隔多少打印一次

然后train就好了。

这里使用了**kwarg，多学学这个用法：
{% codeblock lang:python %}
    self.update_rule = kwargs.pop('update_rule', 'sgd')
{% endcodeblock %}

kwarg其实是个字典，然后一步步的pop这个字典。直到把这个词典的正确的语法都pop掉，留下不对的然后raise一个错误。

然后建立了一个step方法和一个reset方法，前面都有一个下划线，意思是不希望外界直接调用吧。

然后居然还有保存当前的配置的方法_save_checkpoint。注意保存成python object的方法：pickle：
{% codeblock lang:python %}
with open(filename, 'wb') as f:
    pickle.dump(checkpoint, f)
{% endcodeblock %}

然后还有检验准确性的方法check_accuracy。输入是X，y，采样数和batch大小。输出是准确率。

最后就是train方法了。如何输出呢？