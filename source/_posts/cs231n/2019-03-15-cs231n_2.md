---
layout: post
title:  "cs231n_2"
date:   2019-03-15 14:38:13 +0800
categories: AI
---


 <script type="text/x-mathjax-config">
  MathJax.Hub.Config({tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]}});
</script>
<script type="text/javascript" async
  src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML">
</script> 

## 继续学习

这次讲课换了个好漂亮的小姐姐，好开心😺

解析算梯度，算梯度参考cs230，吴恩达讲的。

计算图。

## 反向传播怎么work？

比如$$f(x,y,z) = (x+y)z$$

$$x = -2, y=5, z=-4$$

中间变量$$q= x+y$$

链式法则求导数。

加门：梯度分配器。梯度会均等的分配给两个支路

最大门：两者之间的最大值，较小的变化的话梯度不变，较大的变化的话梯度会变化。梯度路由器。只进其中一路而不进另一路

乘法门：梯度交换机？另一路的值和交汇值相乘……不太好形容，还是画图吧

问题：雅克比矩阵有怎样的形式？大小?比如输入时4096维的输入，4096维的输出?

$$\frac{\partial x}{\partial L}= \frac{\partial f}{\partial x}\frac{\partial L}{\partial f}$$

<!-- more -->
大小就是(4096,4096)

在实际当中，可能选择比如选择整个minibatch，也就是一次训练100个。

问题2： $$\frac{\partial f}{\partial x}$$是什么？注意这里得到函数是max函数

是个对角矩阵。

向量化的例子：

$$f(x,W) = \vertW\dot x\vert^2 = \sum_{i=1}^n (W \cdot x)^2_i$$

W*x+L2正则化项，设$$q = W \cdot x$$

$$f(q) = \vertq\vert^2 = q_1^2 + ... + q_n^2$$

$$\frac {\partial q_k}{\partial W _ {i,j}} = \mathbf{1}_{k=i}x_j$$

$$\frac{\partial f}{\partial W_{i,j}} = \sum_k \frac{\partial f}{\partial q_k} = \sum_k(2q_k)(\mathbf{1}_{k=i}x_j) = 2q_i x_j$$

永远检查：梯度和他们对应的向量应该有相同的shape 

W1可能是可以解释的，包含了很多特征，W2可能是一个分数，对W1的特征进行提取再计算。

h=W_1x是每个模板的分数，是分数函数，W2决定了得到这些分数的权重。

非线性体现在W2?

## NN

一开始，线性分数函数f=Wx
现在，两层网络： $$f = W_2 \max(0, W_1x)$$
三层网络： $$f = W_3 \max (0,W_2 \max(0,W_1x))$$

作业要写个2层神经网络。

实际上和生物当中的联系非常少。

激活函数：sigmoid函数，ReLU，tanh， Leaky ReLU, Maxout, ELU

firing rate
spike？
dendrites
synapses
analogies

全连接层。

## 开始写作业qwq

### 先要计算分数，实现loss函数

就是一个前向误差和一个反向的梯度计算。

其实理解起来不难，关键是矩阵的计算一定要搞清楚。什么矩阵乘以什么矩阵，然后转置还是不转置这种。

比如这个网络是这么分配的：

输入层 --> 全连接层 ---> ReLU ---> 全连接 ---> softmax层

里面节点的链接顺序如下所示：

![节点连接顺序](/assets/2019-03-15-cs231n_2/1.png)

要时刻注意矩阵的维数匹配，注意`b1`和`b2`的情况，这个矩阵维数虽然不匹配，但是能够广播。

现在进行如下假定：

{% codeblock lang:python %}
X: (N, D1)
W1: (D1, D2)
b1: (D2,)
h: (N, D2)
W2: (D2, D3)
b2: (D3)
s: (N, D3)
{% endcodeblock %}

也就是说，`N`是训练样本数，`D1`为输入层的维数，`D2`为中间层的维数，`D3`是输出层的维数。

还有一项很重要，就是每个元素的梯度和原来的梯度的维数是相同的，这样才能保证能够以某种学习率更新。

然后对课上讲的加法是分配门，乘法是交换机，max是路由器更有了一些理解吧。

我们按照我画的那个图来讲。

每个元素有几个分支，最后都要合并在一起，最明显的体现在正则项上。

首先前项推：

`X.dot(W1) + b1` 这个是的第一步计算出的分数，叫做`h`。
`h`经过`ReLU`之后，得到了`h_ReLU`。
`h_ReLU.(W2) + b2`，得到`s`，这就是计算出来的分数了。

这个时候我们要计算loss，loss的计算方法和前面讲的是一样的，也就是从`s`到`L`的过程：

首先要计算softmax，首先要找到所有正确的标签，
{% codeblock lang:python %}
# 因为训练样本有N个，同样正确标签y也有N个
s_y = s[range(N), list(y)]
{% endcodeblock %}

计算规则是：
{% codeblock lang:python %}
l=0.0
l+=np.mean(- np.log(np.exp(s_y).reshape(-1,1)/np.sum(np.exp(s), axis=1).reshape(-1,1)))
{% endcodeblock %}
注意上面遇到的沿着哪个轴加的问题，因为是个样本当中所有项的和，所以是把行加起来的，所以`axis=1`。然后每一个正确的值。然后为了保险起见，在每一项上面都把维数reshape成(-1,1)。

这只是第一项，还需要加上另外的两个正则项：

{% codeblock lang:python %}
l += reg * np.sum(W1**2)
l += reg * np.sum(W2**2)
{% endcodeblock %}

然后就把误差算出来了。

### 算反向梯度

梯度也是遇到一个符号就解决一个符号，从`l`开始，然后这个梯度的计算值就是1。

首先是加号，这个加号其实就是把梯度分配给了三个量，也就是两个正则项和一个softmax项。

这两个正则项的梯度就是在原来的基础上增加`2*reg*W1`和`2*reg*W2`

而softmax项的梯度有点复杂，需要求分数每一项的softmax函数，然后针对每一个正确的项-1，也就是：

{% codeblock lang:python %}
ds = np.exp(s)/np.sum(np.exp(s), axis=1)
ds[range(N), list(y)] += -1
{% endcodeblock %}

然后我逐渐递推吧。遇到➕的话就把相同的梯度分成两份，一份给`b2`，一份给`*`。但是`b2`的维数和`ds`不相等，又因为`b2`的维数为`D3`，`ds`的维数和`s`相等，为`(N,D3)`，所以需要把`b2`按照第一个轴加起来，也就是：
{% codeblock lang:python %}
grads['b2'] = np.sum(dS, axis=0)
{% endcodeblock %}

下面遇到了`*`，连接的是`W2`和`h`，也是根据维数来判断，`W2`是`(D2,D3)`，`h`是`(N,D2)`，在原有梯度的`ds`基础上，进行梯度的分配，根据元素shape来计算是一种验算方法，我们还是看看有没有理论指导吧：
比如这个函数：

$$ s = W_2h + b_2$$

如果对W求导，得到的结果是：

$$\frac{\partial s}{\partial W_2} = h^T$$

如果对X求导，得到的结果是：

$$\frac{\partial s}{\partial h} = W$$

如果对b求导，得到的结果是：

$$\frac{\partial s}{\partial b_2} = 1$$

再考虑上一层的结果，我们有：

$$L = softmax(s) + R(W_1) + R(W_2)$$

所以有：

$$ \frac{\partial L}{\partial W_2} = \frac{\partial L}{\partial s} \frac{\partial s}{\partial W_2} + 2 * reg * W_2 = \frac{\partial L}{\partial s} * h^T + 2 * reg * W_2 $$

最后还是遇到了矩阵维数的问题，看来有个地方我没有注意啊，待会儿看看。

{% codeblock lang:python %}
grads['W2'] = h.T.dot(dS) + 2 * reg * W2
dh = dS.dot(W2.T)
{% endcodeblock %}

然后后面的问题都比较简单，

1. 学习率的话，就直接在self.params['XX']里面更新就好。
2. 比如预测的话，就是把分数计算出来之后，挑出最大的那一个。
3. 计算的话，就是把分数计算出来之后，挑最大的哪一个。
4. 最后有个选择合适的超参数，这个选择的话，能够选择的有：学习率，正则项（这个数字要给的非常小才行，不然loss一致都是2.3），隐含层的数量，学习率递减率等等。
最后做到了52.4%，勉强及格吧。