---
layout: post
title:  "cs231n 作业2 Review"
date:   2019-04-06 18:25:06 +0800
categories: AI
---


 <script type="text/x-mathjax-config">
  MathJax.Hub.Config({tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]}});
</script>
<script type="text/javascript" async
  src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML">
</script> 

## 又到了review的时候了qwq

本次作业一共分为6个部分:

- 全连接层
- batch normalization
- dropout
- CNN
- Tensorflow
- PyTorch

虽然最后两个是选做，但是现在代码有用tf写的，也有用pytorch写的，感觉也是有学习的必要的。

中文文档先摆在[这里](https://www.numpy.org.cn)

先看第一个，FC

<!-- more -->
## FC

这一部分一般都是把之前作业1的内容变成模块化的操作。也就是完成一个`forward`和一个`backward`操作。

`forward`里面存放的是所需要的输入变量，out里面存放最后的计算结果，然后cache里面存放的是暂存的值，这些值给`backward`来使用。

`backward`接受上一步的dout，以及forward当中存放的cache。然后对forward里面的变量每一个都分别求导，返回的也是各个变量的导数。

> 小提醒
这里有个函数而非常有意思rel_error，简单的计算：
`np.max(np.abs(x-y))/ np.maximum(1e-8, np.abs(x)+np.abs(y))`。这个的意思就是判断两个矩阵当中，最大差与最大和的比值。

然后读数据，这些数据存放的格式是一个字典，分别被分成：

    ('X_train: ', (49000, 3, 32, 32))
    ('y_train: ', (49000,))
    ('X_val: ', (1000, 3, 32, 32))
    ('y_val: ', (1000,))
    ('X_test: ', (1000, 3, 32, 32))
    ('y_test: ', (1000,))

### forward

测试affine_forward函数。这里的affine应该就是全连接层，也就是tensorflow对应的tf.matmul(x,W)+b，以及pytorch对应的nn.Linear([1000,10])

现在反正是瞎定义的东西。
`np.prod(input_shape)`
这个的意思应该是计算函数的连乘操作。然后就能够得到每个变量的输入了。
`np.linspace(-0.1,0.5, num=input_size)`，功能是在给定的间隔里面按返回均匀间隔的数字。
> 扩展
`np.arange([start,] stop[, step,][, dtype])` 在给定的间隔内返回均匀间隔的值。
`np.logspace(start, stop[, num, endpoint, base, …])` 返回数在对数刻度上均匀分布。
`np.geomspace(start, stop[, num, endpoint, dtype])` 返回数在对数尺度上均匀分布(几何级数)。

然后就到了第一个要实现的函数：`affine_forward`.
功能是计算fc的前向传递。

mini-batch的大小为N
总的维数为：D = d_1 * ... * d_k
输出的维数为：M

输入的形状是:
- x: N, d_1, ..., d_k
- w: D, M
- b: M,

输出为：
- out: N, M
- cache: (x, w, b)

所以这个作业应该为：

```python
out = x.reshape(x.shape[0],-1).dot(w) + b
```

最后的输出就是`out`, `cache`

验证之后的difference差不多是9.7e-10

### backward

这个也要写了，然后他给了一个验证的好方法，也就是`eval_numerical_gradient_array`,这个的写法参见`cs231n/gradient_check.py`

输入为
- f: 一个函数，用来计算前向传递的结果。
- x: 输入的自变量
- df: 真实的导数

> 小提醒
`np.nditer`:迭代器，访问数组当中的每个元素，使用标准Python迭代器接口逐个提供每个元素。
`flag=['multi_index']`表示多索引，也就是如果是二维数组的话，这个的multi_index属性就是二维数组。
`op_flags=['readwrite']`表示操作数既能够读you能够写。
然后就naive求导的方法，取中心极限而不是其中的某一边。
然后就是`grad[ix] = np.sum((pos - neg) * df) / (2 * h)`
最后返回的`grad`，大小和`x`是一样的。
调用方法是这么调用：`dx_num = eval_numerical_gradient_array(lambda x: affine_forward(x, w, b)[0], x, dout)`

然后开始写`affine_backward`

输入时dout和cache
首先把cache展开成为x, w, b

`db = np.sum(dout, axis=0)`。这个是用到了加法法则，加法是路由器，直接传过去。但是要符合这个矩阵的大小。由于前面forward的时候，out是(N，M)，而b是(M,)加的时候是broadcast过去的，所以梯度还要加回来，也就是把axis = 0的N干掉。在后面的reshape是双保险，其实不加也没关系qwq。

后面的`dwx`的导数就是`dout`了。

然后是乘法法则，是交换机，dw = x * dwx。但是要满足乘法规则。x是(N, D), dwx是(N, M)，那么最终的dw和w的形状一样，是(D, M)，所以就得到了 `D, M = D, N * N, M`，也就是`dw = x.T * dwx`，但是之前x是N, d_1, ..., d_n的，所以要展平，成为N，D。于是就有了这样的结果：

```python
x_temp = x.reshape(x.shape[0],-1)
dw = x_temp.T.dot(dout).reshape(w.shape)
```

后面的dx也是同样的分析方法，`N,D = N, M * M, D`,也就是`dx = dwx * w.T`，所以也就有这样的结果：`dx = dout.dot(w.T).reshape(x.shape)`

### relu forward

很简单，就一行：
`out = np.maximum(0,x)`
就是取0和x的最大值

### relu backward

挑出所有取正值的元素，只有这些元素才传递东西。

我的写法是：

```python
    dx = np.where(x > 0, dout, 0).reshape(x.shape)
```

就是选择x大于0的标签，如果是的话就是dout，否则就是0。

看网上有种写法：

```python
dx = dout
dout[x<=0]=0
```

好像确实是更简洁一点哦。

### 三明治层，也就是fc+relu，forward

有了前面的算法的话，就能够写这个了。看`layer_utils.py`这个文件。

前向传播的话，每一个cache都追加在cache后面，组成一个新元组。然后就是按照顺序，feed输入和输出这种。

```python
def affine_relu_forward(x, w, b):
    """
    Convenience layer that perorms an affine transform followed by a ReLU

    Inputs:
    - x: Input to the affine layer
    - w, b: Weights for the affine layer

    Returns a tuple of:
    - out: Output from the ReLU
    - cache: Object to give to the backward pass
    """
    a, fc_cache = affine_forward(x, w, b)
    out, relu_cache = relu_forward(a)
    cache = (fc_cache, relu_cache)
    return out, cache
```

### 三明治层的backward

这个也是一层一层向后面传播，要注意这个方向和前向传播的方向正好相反。

```python
def affine_relu_backward(dout, cache):
    """
    Backward pass for the affine-relu convenience layer
    """
    fc_cache, relu_cache = cache
    da = relu_backward(dout, relu_cache)
    dx, dw, db = affine_backward(da, fc_cache)
    return dx, dw, db
```

说明：第一步要把cache当中的东西展开。上一层的dx（输出）是下一层的dout（输入）

## 二层的神经网络

这个类介绍一下，在的位置是`fc_net.py`

模块化设计。一个类里面完成数据的输入输出，前向传递，反向求导，以及训练。
参数存放在字典self.params里面。
使用额外的优化器来对梯度进行计算。

结构是：affine -> relu -> affine -> softmax

首先是构造函数，包括输入维数，隐含层，类的数量，权重的比例（一开始初始化小值乘以一个系数的这个系数值），正则化强度。
构造函数当中需要对输入和输出进行初始化，这个原则是：

1. 所有的权重，包括卷积层的权重都给一个随机小的数字。
2. 其他给0，尤其是bias

然后是定义loss的计算。
有两个参数，一个是图片X，一个是真实标签y。如果真实标签不存在的话，就只计算前向；如果都有的话，也把梯度计算出来。

计算的话，就是这段代码了：

```python
h = X.dot(self.params['W1']) + self.params['b1']
h_ReLU = np.maximum(0,h)
scores = h_ReLU.dot(self.params['W2'])+self.params['b2']
```

前向一时爽，求导火葬场，这是后向的代码：

```python
## 减去最大值
scores = scores - np.max(scores, axis=1, keepdims=True)
## 求每一个score的exp值
Z = np.sum(np.exp(scores),axis=1,keepdims=True)
## 求Li
log_probs = scores - np.log(Z)
N = X.shape[0]
probs = np.exp(log_probs)
## 求loss
loss = -np.sum(log_probs[np.arange(N),y])/N + 0.5*self.reg* np.sum(self.params['W2']**2) +0.5* self.reg*np.sum(self.params['W1']**2)
## 求导结果参见assignment1的推导吧qwq准确来说我现在忘得差不多了qwq
ds = probs.copy()
ds[np.arange(N), y] -= 1
ds /= N
# ds += 0.5 * self.params['W2'] + 0.5 * self.params['W1']
grads['W2'] = h_ReLU.T.dot(ds).reshape(self.params['W2'].shape) + self.reg * self.params['W2']
grads['b2'] = np.sum(ds,axis=0).reshape(self.params['b2'].shape)
dh_Relu = ds.dot(self.params['W2'].T).reshape(h_ReLU.shape)
dh = np.where(h>0, dh_Relu, 0)
grads['b1'] = np.sum(dh,axis=0).reshape(self.params['b1'].shape)
grads['W1'] = X.T.dot(dh).reshape(self.params['W1'].shape) + self.reg*self.params['W1']
```

### 下面看一下解算器Solver

有很多的Solver。最简单的有随机梯度下降方法，但是对于鞍点的数据不咋好用，所以加了一个动量。

现在看一下这个Solver。封装了所有训练分类模型的必要的逻辑。使用optim.py当中的随机梯度下降stochastic gradient descent来进行优化。

接受训练数据和验证数据，所以这能够周期性检测分类精度，来看一下这个模型是不是过拟合了。

训练这个模型的时候，需要构建一个solver实例，传入模型，数据集，以及其他超参数（学习率，batchnum）。然后调用给一个train方法，就能够欧开始执行优化过程以及训练这个模型了。

等这个方法返回的时候，model.params里面会存放这些参数，在验证集和训练集当中效果最好的会保留下来。

另外，变量solver.loss_history这个变量会包含一个列表，这个列表里面会包含所有训练集计算的loss记录，同样的solver.train_acc_history里面会存放每个epoch里面模型的精度。

数据可能张这个样子

{% codeblock lang:python %}
data = {
  'X_train': # training data
  'y_train': # training labels
  'X_val': # validation data
  'y_val': # validation labels
}
model = MyAwesomeModel(hidden_size=100, reg=10)
solver = Solver(model, data,
                update_rule='sgd',
                optim_config={
                  'learning_rate': 1e-3,
                },
                lr_decay=0.95,
                num_epochs=10, batch_size=100,
                print_every=100)
solver.train()
{% endcodeblock %}

符合模型的api必须保证有如下形式（定义接口吧）：

- model.params是一个字典，映射所有需要使用到的参数值
- model.loss(X,y)包含一个函数，用来计算训练时候的loss，梯度和测试时候的分类分数。输入输出如下：
    - X: minibatch的输入，形状为N, d_1, ..., d_k
    - y: 标签的数组，形状为(N,)

    - 如果y是None的话，只运行前向的时候的分数
    - scores：数组形状为：(N,C)，C是分类数量，也就是给出N个数据关于每一类的得分。

    - 如果y不是None的话，前向和后项都要进行。
    - loss: 是一个标量
    - grad: 是一个字典，包含了当中每一层的权重值。

然后是构造函数：

前面的model，data都已经说过了。
下面说一下可选参数：

1. 更新规则：就是随机梯度下降等方法，`sgd`,`adam`啥的。
2. optim_config:
    - learning_rate:
    - lr_decay: 每个epoch之后，学习率下降为原来的多少。
    - batch_size: 
    - num_epochs:
    - verbose: 打印输出这种
    - num_train_samples: 选择多少训练集当中的样本作为验证集，默认值是1000。None表示在整个训练集当中进行验证。
    - num_val_samples: 选多少样本来进行验证，默认是None，也就是整个验证集。
    - checkpoint_name: 如果不是None的话，就会在每一个epoch当中保存一个checkpoint
3. 有一个很有用的东西叫做kwargs，前面如果是一个\*，也就是\*kwargs这种，就会把kwargs当成一个元组。如果前面是有两个\*，也就是**kwargs，这就会把后面的内容，变成字典，前面是key后面是value。
4. 学习这个写法，判断输入是不是合法：

  ```python
  if len(kwargs) > 0:
    extra = ', '.join('"%s"' % k for k in list(kwargs.keys()))
    raise ValueError('Unrecognized arguments %s' % extra)
  ```

  以及学习如何只在每一项后面添加逗号，最后一项后面不加逗号这种。

  判断某种梯度更新算法是不是存在。

  ```python
  if not hasattr(optim, self.update_rule):
      raise ValueError('Invalid update_rule "%s"' % self.update_rule)
  self.update_rule = getattr(optim, self.update_rule)
  ```
  使用这个`hasattr`，字符串就转化成了实际的方法了，和后面的getattr是一样的，有点像java的反射。

  ps: c++里面实现反射，真是太难了qwq，还是要boost库的帮助qwq

5. _reset函数，前面有一个下划线表示是protected的方法，虽然python里面没这个方法，但是这么写了应该就行了把。

ps: python里面并没有强制这一点，所以一个约定就是protected的方法前面加一个下划线，private前面加两个下划线。但是python对两个下划线的方法还是有点保护的，就是`__原方法`这个方法被换成了`_类名__原方法`，子类就没办法覆盖这个父类当中的方法了。当然这是一种假的覆盖，子类当中直接调用这个`_类名__原方法`也能访问得到，但是就比较混乱了。

完成的任务主要是进行变量的初始化，字典和列表都初始化好这种，epoch数也弄对。

6. _step方法。这个方法应该是train方法自动调用的，可以看一下到底做了什么。

  - 首先就是简单的计算了一下loss和梯度，把loss放到history当中去。
  - 执行参数更新，对model.params当中的每一个参数，都执行相应的优化方法：self.update_rule。

7. _save_checkpoint。

  - 如果保存的文件名为空的话，返回
  - 否则记录以下变量：
  ```python
  checkpoint = {
    'model': self.model,
    'update_rule': self.update_rule,
    'lr_decay': self.lr_decay,
    'optim_config': self.optim_config,
    'batch_size': self.batch_size,
    'num_train_samples': self.num_train_samples,
    'num_val_samples': self.num_val_samples,
    'epoch': self.epoch,
    'loss_history': self.loss_history,
    'train_acc_history': self.train_acc_history,
    'val_acc_history': self.val_acc_history,
  }
  ```

  然后保存在一个文件当中。使用`pickle.dump`方法。

8. 检验精度

  - num_samples: 如果是None的话，下采样数据，并且只测试num_samples个数据的模型。
  - batch_size: 把X和y分成小batches防止内存爆炸
  - acc：分类正确占分类综述的比例。
  
  - 计算步骤是先计算出开始和结束，这个开始和结束是batch_size决定的，然后计算在这个区间之内的分数，y_pred也添加这个分数，精度的计算就是这样的：

  ```python
    y_pred = np.hstack(y_pred)
    acc = np.mean(y_pred == y)
  ```

9. train:

  - 计算了每个epoch迭代的次数iterations_per_epoch,以及一共进行的迭代数量：num_iterations。
  - 对于每次迭代：_step()
  - 然后根据verbose打印相应的消息
  - 在每一个epoch结束之后，增加epoch计数器，并根据lr_decay来使得学习率下降
  - 检验训练集和测试集的精度。测试的时机是：
    - 第一次迭代
    - 最后一次迭代
    - 以及每个epoch结束之后迭代
  - 使用check_accuracy检验测试机和训练集精度，测试数量根据`num_train_samples`和`num_val_samples`来决定。
  - 保存这个checkpoint，并打印相应的测试精度消息。
  - 保持这个模型是在验证集上最好的模型。

10. 缺陷：
  
  - 这个东西虽然保存了权重，但是没有恢复啊，这算个啥子嘛qwq

### 看一下数据的表示

和matlab语法差不多。

线型当中有一个'-o'，这个是既可以是点又可以是折线。每次plot之后都可以规定label是什么。

调整图例位置：
```python
plt.legend(loc='lower right')
```

调整画布大小使用这一句话：
```python
plt.gcf().set_size_inches(15, 12)
```

### 多层网络

这里面的输入变成了一个向量，也就是说根据`hidden_dims`这个列表来确定简历网络的层数这样子。需要修改的类是`FullyConnectedNet`，在`fc_net.py`这个文件当中。

- 网络结构：
    `{affine -> [batch norm] -> relu -> [dropout]} x (L - 1) -> affine -> softmax`
    中括号的是可选参数。
- 初始化过程
  {% codeblock lang:python %}
    # input_dim: D
    # Hidden1: H1,H2
    # class: C
    last_item = input_dim
    for i,item in enumerate (hidden_dims):
      key_W = 'W' + str(i+1)
      key_b = 'b' + str(i+1)
      key_gamma = 'gamma'+ str(i+1)
      key_beta = 'beta'+str(i+1)
      self.params[key_W] = np.random.randn(last_item, item) * weight_scale
      self.params[key_b] = np.zeros(item)
      if self.use_batchnorm:
        self.params[key_gamma] = np.ones(item)
        self.params[key_beta] = np.zeros(item)
      last_item = item
    key_W = 'W' + str(self.num_layers)
    key_b = 'b' + str(self.num_layers)
    self.params[key_W] = np.random.randn(last_item, num_classes) * weight_scale
    self.params[key_b] = np.zeros(num_classes) 
  {% endcodeblock %}
  遵循的原则：
  - 对于fc来说，W给随机数，b给0；对batchnorm来说，gamma给全1，beta给全0。
  - 相邻两层的权重层的维度要匹配，下一层的axis0和上一层的axis1是相同的
  - 对于dropout层来说，需要定义mode和概率，mode就是究竟是训练还是测试。对于每层的bn来说，要给这个mode为train
  - 最后把所有的字典当中的数据变成正确数据类型，比如np.float64,np.float32等等

- 计算loss

  如何判断当前是训练状态还是测试状态：看y是不是None就可以了。
  然后计算loss：
  按照3层网络来计算的话，需要有两个W在循环当中，因为第三个W是输出。

  ```python
  input_layer = X
  layer_cache = {}
  drop_cache = {}
  # 0 1
  for i in range(self.num_layers-1):
    Wi = 'W' + str(i+1)
    bi = 'b' + str(i+1)
    layer_index = i+1
    # 然后batchnorm
    if self.use_batchnorm:
      gammai = 'gamma'+ str(i+1)
      betai = 'beta'+str(i+1)
      input_layer, layer_cache[layer_index] = affine_bn_relu_forward(input_layer,self.params[Wi],self.params[bi],self.params[gammai],self.params[betai],self.bn_params[i])
    else:
      input_layer, layer_cache[layer_index] = affine_relu_forward(input_layer,self.params[Wi],self.params[bi])
    # 最后激活函数
    if self.use_dropout:
      input_layer, drop_cache[layer_index] = dropout_forward(input_layer,self.dropout_param)
  # 3 
  Wi = 'W' + str(self.num_layers)
  bi = 'b' + str(self.num_layers)
  output_layer, layer_cache[self.num_layers] = affine_forward(input_layer, self.params[Wi], self.params[bi])
  scores = output_layer
  ```

  判断用不用batchnorm，用不用dropout，还是有很大的区别的qwq。

  注意点：
  - 前向只需要执行到计算出score就可以了。

- 后项传播

直接上代码吧qwq：

  {% codeblock lang:python %}
  loss, dscores = softmax_loss(scores, y)
  dhout = dscores
  Wi = 'W' + str(self.num_layers)
  bi = 'b' + str(self.num_layers)#3
  loss = loss + 0.5 * self.reg * np.sum(self.params[Wi]**2)
  dx, dw, db = affine_backward(dhout, layer_cache[self.num_layers])
  grads[Wi] = dw + self.reg*self.params[Wi]
  grads[bi] = db
  dhout = dx
  for i in range(self.num_layers-1):
    layer_index = self.num_layers - i - 1
    Wi = 'W' + str(layer_index)
    bi = 'b' + str(layer_index)
    loss = loss + 0.5 * self.reg * np.sum(self.params[Wi]**2)
    if self.dropout_param:
      dhout = dropout_backward(dhout, drop_cache[layer_index])

    if self.use_batchnorm:
      dx, dw, db, dgamma, dbeta = affine_bn_relu_backward(dhout,layer_cache[layer_index])
    else:
      dx, dw, db = affine_relu_backward(dhout, layer_cache[layer_index])
    grads[Wi] = dw + self.reg * self.params[Wi]
    grads[bi] = db
    if self.use_batchnorm:
      gammai = 'gamma' + str(layer_index)
      betai = 'beta' + str(layer_index)
      grads[gammai] = dgamma
      grads[betai] = dbeta
    dhout = dx
  {% endcodeblock %}