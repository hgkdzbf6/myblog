---
layout: post
title:  "cs231n 后半段"
date:   2019-04-05 18:24:07 +0800
categories: AI
---


 <script type="text/x-mathjax-config">
  MathJax.Hub.Config({tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]}});
</script>
<script type="text/javascript" async
  src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML">
</script> 

## 无监督学习

自动编码器
变分自动编码器
gan

生成模型

PixelRNN PixelCNN
VAE
GAN

监督学习的数据是(x,y)，x是图片，y是标签。
目标是学习一个函数，得到从x到y的映射
例子有：分类，回归，目标检测，语义分割，图像标注等

无监督学习就是只有数据，没有标签。

目标是学习到这些数据隐藏的结构

<!-- more -->
例子有：聚类（K-means），降维(PCA)，特征学习(自动编码器)，密度估计等。

无监督学习的优点：
1. 只有数据没有标签，所以训练数据很廉价。
2. 解决无监督学习问题：其实就是在理解视觉世界的结构。

生成模型，想要学习到一个模型，和数据产生的模型是相似的。

密度估计，核心问题就是无监督学习。
一些方法：
- 明确的密度估计：明确定义，解出p_model(x)
- 含蓄的密度估计：学习模型，可以从p_model(x)采样得到，而不需要明确的定义它。

从艺术作品当中实际采样，超分辨率，涂色等等。

生成模型的时间序列数据能顾被用来模拟和计划（增强学习应用）
训练生成模型也能够推理出潜在的表达，这能够生成更加一般的特征。

生成模型的分类：

- 明确的密度估计
  - 易解显性模型
    - Fully Visible Belief Nets
      - NADE
      - MADE
      - `PixelRNN`
      - `PixelCNN`
      - 非线性ICA
    - 近似显性模型
      - 可变的
        - `差分自动编码器`
      - 马尔可夫链
        - 玻尔兹曼机
- 隐含的密度估计
  - 含蓄定义模型密度分布
    - 直接法
      - `GAN`，生成对抗网络
    - 马尔可夫链
      - GSN，生成随机网络

明确的密度估计。使用链式法则去结算每张图片的n个分布。

$$p(x)=\prod_{i=1}^{n} p\left(x_{i} \vert  x_{1}, \ldots, x_{i-1}\right)$$
然后去最大化训练数据的可能性。

需要定义前面像素的顺序
复杂的像素层面的分布，使用一个神经网络来表达

PixelRNN：
生成像素从一个角上开始。
依赖前一个像素的建模，使用一个RNN（LSTM）。
一层一层的往外面扩散。
缺点：序列生成太慢了。

PixelCNN：
还是从一个角上开始生成像素
依赖前一个像素，现在是使用一个cnn在内容区域里面建模。
训练：最大化训练图像的可能性

$$p(x)=\prod_{i=1}^{n} p\left(x_{i} \vert  x_{1}, \ldots, x_{i-1}\right)$$

比PixelRNN快，可以并行的卷积，因为内容的值是从训练图像当中已知的。

生成必须仍然序列化的处理：还是慢

优点：
- 可以明确的计算可能性p(x)
- 明确训练数据的可能性，这给了一个很好的评价指标
- 好的样本

缺点：
- 序列生成样本：太慢了

改进PixelCNN的性能：
- 给卷积层加个门
- 短路一些连接
- 离散化逻辑损失
- 多尺度
- 训练时候加trick

也有PixelCNN++

### 变分自动编码器

PixelCNN定义了听话的密度函数，优化训练数据的可能性。

$$p_{\theta}(x)=\prod_{i=1}^{n} p_{\theta}\left(x_{i} \vert  x_{1}, \ldots, x_{i-1}\right)$$

VAE定义了一个棘手的(intractable)密度函数，使用一个潜在的z：

$$p_{\theta}(x)=\int p_{\theta}(z) p_{\theta}(x \vert  z) d z$$

这个函数不能够直接优化，通过推导和优化下界来计算。

首先：什么是自动编码器？

从一组无标签的训练数据的学习低维特征的无监督学习方法。

输入数据维数一般很大，特征的维数一般比较小。

为什么会降维？因为我们想要特征能够尽量描述数据的变化

原来的编码器：
- 线性+非线性：sigmoid
- 后来深一点的全连接层
- 再后来是ReLU CNN

学习特征表示：
通过一个解码器来重构输入数据，自动编码器，编码他自己。

原来的编码器：
- 线性+非线性：sigmoid
- 后来深一点的全连接层
- 再后来是转置卷积ReLU CNN

训练这样的特征，能够重构原始数据。

最后输入数据和重构的输入数据得到L2损失函数，不需要使用标签。

训练结束之后，把这个解码器扔掉。

编码器能够初始化一个监督模型。
编码器和分类器串级，得到一个预测标签，然后根据这两个标签和真实标签的偏差来定义损失函数。

和分类器一起fine-tune

对于小样本来说非常好用。

自动编码器能够重构数据，我们能够从这些特征来初始化一个监督模型。
特征捕捉因子在训练过程当中使用。我们能够使用这个自动编码器生成新图像吗？

变分自动编码器：
假定训练数据x从潜在的没有观测的表示z当中生成的话:

真实的先验估计：
$$p_{\theta^{*}}(z)$$

从真实的情况当中采样得到的概率：
$$p_{\theta^{*}}\left(x \vert  z^{(i)}\right)$$

动机：从自动编码器当中记住一些东西。
x是图像，z是隐含的函数，用来生成x的，包括x的特征，朝向等。

我们想要从真实的参数$$\theta ^*$$当中来估计这个生成模型。
怎么来表示这个模型呢？
选择一个先验p(z)，简单点也就是高斯，有表示意义的隐含的特征，比如姿态，笑的程度。

条件$$p(x\vert z)$$的话复杂一点了，用神经网络来表示。

怎么去训练模型呢？记住FVBN当中的生成模型策略，训练模型参数来最大化训练数据的概率：
$$p_{\theta}(x)=\int p_{\theta}(z) p_{\theta}(x \vert  z) d z$$
现在是隐含的z

但是有什么问题？难懂，棘手。
数据似然：
$$p_{\theta}(x)=\int p_{\theta}(z) p_{\theta}(x \vert  z) d z$$

$$p_{\theta}(z)$$是简单的高斯先验。
$$ p_{\theta}(x \vert  z) $$是解码器神经网络。
对每一个z来说，$$p(x \vert z)$$不好求。对应的积分不好求

后验其实也不好求：
$$p_{\theta}(z \vert  x)=p_{\theta}(x \vert  z) p_{\theta}(z) / p_{\theta}(x)$$
$$p_\theta(x)$$不好求。

解决办法：添加额外的解码器网络，去建模这个$$p_\theta(x\vert z)$$，定义额外的编码器网络$$q_\phi(z\vert x)$$来接近这个$$p_\theta(x\vert z)$$。

这让我们推导出数据可能性的下界，这个是能够处理的和优化的。

因为我们建模了数据的可能性，编码器和解码器都是基于概率的。

编码器网络：
$$q_{\phi}(z \vert  x)$$
通过$$z\vert x$$均值和方差建模的，
高斯分布

解码器网络：
$$p_{\theta}(x \vert  z)$$
通过$$x\vert z$$的均值和方差建模

编码器/解码器网络又被称为识别、推理网络和生成网络。

求对数似然：
$$\log p_{\theta}\left(x^{(i)}\right)=\mathbf{E}_{z \sim q_{\phi}\left(z \vert  x^{(i)}\right)}\left[\log p_{\theta}\left(x^{(i)}\right)\right]$$

$$p_\theta(x^{(i)})$$不依赖于z。
贝叶斯推理：

$$=\mathbf{E}_{z}\left[\log \frac{p_{\theta}\left(x^{(i)} \vert  z\right) p_{\theta}(z)}{p_{\theta}\left(z \vert  x^{(i)}\right)}\right]$$

乘以常数：
$$=\mathbf{E}_{z}\left[\log \frac{p_{\theta}\left(x^{(i)} \vert  z\right) p_{\theta}(z)}{p_{\theta}\left(z \vert  x^{(i)}\right)} \frac{q_{\phi}\left(z \vert  x^{(i)}\right)}{q_{\phi}\left(z \vert  x^{(i)}\right)}\right]$$

展开对数：
$$=\mathbf{E}_{z}\left[\log p_{\theta}\left(x^{(i)} \vert  z\right)\right]-\mathbf{E}_{z}\left[\log \frac{q_{\phi}\left(z \vert  x^{(i)}\right)}{p_{\theta}(z)}\right]+\mathbf{E}_{z}\left[\log \frac{q_{\phi}\left(z \vert  x^{(i)}\right)}{p_{\theta}\left(z \vert  x^{(i)}\right)}\right]$$

得到两个kl散度
$$=\mathbf{E}_{z}\left[\log p_{\theta}\left(x^{(i)} \vert  z\right)\right\rceil- D_{K L}\left(q_{\phi}\left(z \vert  x^{(i)}\right) \vert  p_{\theta}(z)\right)+D_{K L}\left(q_{\phi}\left(z \vert  x^{(i)}\right) \vert  p_{\theta}\left(z \vert  x^{(i)}\right)\right)$$

$$\mathbf{E}_{z}\left[\log p_{\theta}\left(x^{(i)} \vert  z\right)\right]$$这一项是由解码器网络给出的$$p_\theta(x\vert z)$$，能够计算出这个估计。看paper。

编码器和高斯分布的KL项有很好的闭环解。

$$p_\theta(z\vert x)$$是不知道的，也计算不出来，但是我们知道KL散度总是大于0的。

前面两项组合起来，是可以得到的下界，我们可以拿这个来求梯度和优化。这两项都是能够求导的。

所以有：$$\log p_\theta(x^{(i)}) \ge \mathcal{L}\left(x^{(i)}, \theta, \phi\right)$$
变分下界ELBO

$$\theta^{*}, \phi^{*}=\arg \max _{\theta, \phi} \sum_{i=1}^{N} \mathcal{L}\left(x^{(i)}, \theta, \phi\right)$$
训练：最大化下界。

需要对输入数据进行重构。
需要使得后验分布和先验分布尽可能接近。

把这一切都放在一起：
$$\mathcal{L}\left(x^{(i)}, \theta, \phi\right) = \mathbf{E}_{z}\left[\log p_{\theta}\left(x^{(i)} \vert  z\right)\right]-D_{K L}\left(q_{\phi}\left(z \vert  x^{(i)}\right) \vert  p_{\theta}(z)\right)$$

然后前向pass计算吧：

根据输入数据，编码器网络得到均值和方差，钥匙的后验分布和先验分布尽可能接近的话，从z采样，有$$z \vert  x \sim \mathcal{N}\left(\mu_{z\vert x}, \Sigma_{z \vert  x}\right)$$。然后构建解码器网络$$p_{\theta}(x \vert  z)$$，也得到相应的均值和方差，然后从这个分布$$x \vert  z \sim \mathcal{N}\left(\mu_{x\vert z}, \Sigma_{x \vert  z}\right)$$当中进行采样，得到对x的估计，使得这估计值和输入数据的概率最大。

对于输入的每个小batch来说，计算前向pass，然后backprop。

然后来使用这个解码器网络来生成数据，现在采样z是先验数据了。

数据在不同的维度上变化，可以产生不同的效果，比如一个轴上是头的姿态，另一个轴上是笑的强度。

优点：
1. 主要用来生成模型
2. 允许来推断$$q(z\vert x)$$，可以作为其他任务的特征表示

缺点：
1. 最大化可能性的下界：还行但是没有PixelRNN/PixelCNN好
2. 样本更糊一点，质量更差一点，相比GAN来说。

关于VAE的活跃的研究方向：
1. 更加灵活的近似，比如更加丰富的后验，而不是用对角高斯
2. 和潜在的变量的结构结合。

生成对抗网络：

1. 现在我们已经定义了可以的跟踪的密度函数，可以优化训练数据的可能性。$$p_{\theta}(x)=\prod_{i=1}^{n} p_{\theta}\left(x_{i} \vert  x_{1}, \ldots, x_{i-1}\right)$$
2. VAE关于隐含变量z，定义了一个不可跟踪的密度函数。$$p_{\theta}(x)=\int p_{\theta}(z) p_{\theta}(x \vert  z) d z$$，这个不能直接优化，但是可以推导并优化可能性的下界。

GAN： 不直接计算任何具体的密度函数，而是采用博弈论的方法：通过2玩家的竞赛，从训练数据的分布中生成。

问题：想要从复杂，高维训练数据中采样，没有直接的方法。
解决办法：从一个简单的分布入手，也就是随机噪声，学习如何变成训练数据。

问题：如何来表示复杂的变换？
答案：神经网络啊！

两个玩家play
生成器网络：试着迷惑判别器，通过生成和真的很像的图片。
判别器网络：通过分辨出哪些是真的图哪些是假的图。

随机噪声 -> 生成网络 -> 生成假图 -> 和真图一起放入判别器 -> 是真图还是假图

训练联合极大极小游戏：
目标函数：$$\min _{\theta_{g}} \max _{\theta_{d}}\left[\mathbb{E}_{x \sim p_{d a t a}} \log D_{\theta_{d}}(x)+\mathbb{E}_{z \sim p(z)} \log \left(1-D_{\theta_{d}}\left(G_{\theta_{g}}(z)\right)\right)\right]$$

$$D_{\theta_d}(x)$$是鉴别器对真实数据的输出
$$D_{\theta_d}(G_{\theta_g(z)})$$是鉴别器对假数据G(z)的输出。

鉴别器$$\theta_d$$想要最大化目标，使得D(x)接近于1，并且D(G(z))接近于0
生成器$$\theta_g$$想要最小化这个目标，使得D(G(z))接近于1（鉴别器被愚弄，认为生成的G(z)是真的）

两种选择：
1. 对鉴别器使用梯度上升法$$\max _{\theta_{d}}\left[\mathbb{E}_{x \sim p_{d a t a}} \log D_{\theta_{d}}(x)+\mathbb{E}_{z \sim p(z)} \log \left(1-D_{\theta_{d}}\left(G_{\theta_{g}}(z)\right)\right)\right]$$

2. 对生成器使用梯度下降法$$\min _{\theta_{g}} \mathbb{E}_{z \sim p(z)} \log \left(1-D_{\theta_{d}}\left(G_{\theta_{g}}(z)\right)\right)$$

实际上但对生成器的优化目标优化，效果不是很好。
因为当样本可能是假的的话，想要改进生成器。但是梯度在这个时候非常平坦。
当样本可能是真的的话，梯度虽然是很大，但事实上这个时候生成器已经很好了。

所以使用另外的方法，对生成器来说，也使用梯度上升法，优化目标变为：
$$\max _{\theta_{g}} \mathbb{E}_{z \sim p(z)} \log \left(D_{\theta_{d}}\left(G_{\theta_{g}}(z)\right)\right)$$

现在优化鉴别器判断错误的可能性，目标仍然还是骗过鉴别器，但是这个时候，坏的样本的梯度比上一种情况更好！在实际当中使用。

另外连接两个网络的话还是很难，有可能导致不稳定。选择更好的目标，对于训练来说会更好，现在是研究的热点问题。

GAN训练算法：

对于每一次训练迭代
- 在第k步
  - 从噪声先验当中，采样m个噪声样本p_g(z)
  - 从数据产生样本当中，采样m个样本p_{data}(x)
  - 更新鉴别器，通过随机梯度上升法
    - $$\nabla_{\theta_{d}} \frac{1}{m} \sum_{i=1}^{m}\left[\log D_{\theta_{d}}\left(x^{(i)}\right)+\log \left(1-D_{\theta_{d}}\left(G_{\theta_{g}}\left(z^{(i)}\right)\right)\right)\right]$$
  - 从噪声先验当中，采样m个噪声样本p_g(z)
  - 使用改进的目标，使用随机梯度上升法更新生成器。
    - $$\nabla_{\theta_{g}} \frac{1}{m} \sum_{i=1}^{m} \log \left(D_{\theta_{d}}\left(G_{\theta_{g}}\left(z^{(i)}\right)\right)\right)$$

有的时候k=1的时候更稳定，有其他人认为k>1的好，不是啥大问题。

现在的网络，比如Wasserstein GAN焊合了这个问题，有更好的稳定性。

训练过后，使用生成器网络就能够生成新的图像了。

GAN的卷积结构：
生成器是一个上采样网络，分数stride的卷积网络
判别器是一个卷积神经网络

稳定的深度卷积GAN指南：
- 把所有池化层替换为，对于判别器来说替换为有stride的卷积层，对于生成器来说替换为分数stride的卷积层。
- 在生成器和判别器当中都使用batchnorm
- 为了更深的结构，移除所有的全连接层
- 对生成器，除了输出层使用tanh外，其他都使用ReLU
- 对判别器来说，使用泄露的ReLU

可解释的向量数学：
从模型当中采样。平均Z向量，做算术。

戴眼镜的男人-没有戴眼镜的男人+没有戴眼镜的女人=戴眼镜的女人。

用处：
1. 生成更好的训练样本
2. 来源->目标的转换
3. 文字->图片
等等等等

[gan zoo](https://github.com/hindupuravinash/the-gan-zoo)

[gan 调参秘籍](https://github.com/soumith/ganhacks)

GAN特点：
1. 不跟随具体的密度函数
2. 采用博弈论的方法，通过2玩家游戏，学习去生成训练分布。

优点：
- 美丽！先进的结果

缺点：
- 更考验技巧，训练时候不稳定
- 不能推导出$$p(x)$$, $$p(z\vert x)$$

活跃的工作：
1. 更好的损失函数，更稳定的训练结果（Wasserstein GAN, LSGAN，还有其他gan）
2. 条件GAN，GAN的应用

生成模型：
1. PixelRNN和PixelCNN：需要明确的密度模型，优化概率的结果，但是序列生成的话太慢了。
2. 变分自动编码器，优化可能性的变分下界，对于潜在的变量表达来说很有用，有一定的推导，但是现在效果不咋好
3. 博弈论方法，效果最好，但是更考验trick，训练时候不咋稳定，没啥内部的建模方法。

现在也有把他们结合的工作，比如对抗自动编码器，PixelVAE等等

## 强化学习

现在都是监督学习，x是数据，y是标签
目标是学习一个从x到y的映射
无监督学习。

强化学习：引入智能体，和环境交互，并且能够得到回报信号。
目标：为了最大化这个目标，学习如何去执行动作。

- 什么是强化学习？
- 马尔科夫决策链
- Q学习
- 策略梯度

倒立摆问题：在一个可移动的小车上面有个倒立摆
状态：角度，角速度，位置，水平速度
动作：水平方向上的力
回报：直立的时候为1

mujocu仿真
目标：使得机器人前进
状态：关节的角度和位置
动作：施加在关节上面的转矩
回报：每次状态为直立，并且向前移动

啊他日游戏
目标：完成这个游戏，得到最高的分数
状态：每个像素的输入
动作：游戏控制：上下左右
回报：分数的增减

围棋：
目标：赢得游戏
状态：所有棋子的位置
动作：下一步应该下在什么地方
回报：1如果赢得了这个游戏，0的话没有赢。

### 如何数学建模这个增强学习问题？

状态$$s_t$$
回报$$r_t$$
下一个状态为$$s_{t+1}$$
动作$$a_t$$

马尔科夫决策过程：
马尔科夫性：当前状态完全决定了下一个时刻的状态：

这里有这么一些变量：

$$(\mathcal{S}, \mathcal{A}, \mathcal{R}, \mathbb{P}, \gamma)$$

$$\mathcal{S}$$：所有可能状态的集合
$$\mathcal{A}$$：所有可能动作的集合
$$\mathcal{R}$$：对于给出的(动作，状态)给出的回报分布
$$\mathbb{P}$$：转化概率，也就是分配下一个状态的概率
$$\gamma$$：打折因子

过程：
一开始，所有的环境状态变为$$s_0 ~ p(s_0)$$
然后，直到结束：
- agent选择动作$$a_t$$
- 环境给出回报$$r_t ~ R（。\vert s_t, a_t)$$
- 环境采集下一个状态$$s_{t+1} ~ P(. \vert  s_t, a_t)$$
- Agent收到了回报$$r_t$$，并且做下一个状态$$s_{t+1}$$

还有一个策略函数$$\pi$$，决定在当前状态下，应该采取什么样的动作。
目标：找到一个策略函数$$\pi^*$$，最大化这个累计打折回报$$\sum_{t>0} \gamma^{t} r_{t}$$。

一个简单的搜索策略：网格地图
可以随机搜索，也可以最优搜索。

如何找到最优的策略$$\pi^*$$
如何处理这个随机性？
最大化回报和的期望
$$\pi^{*}=\arg \max _{\pi} \mathbb{E}\left[\sum_{t \geq 0} \gamma^{t} r_{t} \vert  \pi\right]$$

$$s_{0} \sim p\left(s_{0}\right), a_{t} \sim \pi\left(\cdot \vert  s_{t}\right), s_{t+1} \sim p\left(\cdot \vert  s_{t}, a_{t}\right)$$

定义：值函数和q函数
跟随怎样的策略路径？

如何描述一个状态的好坏？值函数，值函数是使用策略pi，在状态s的时候的期望累计回报。

$$V^{\pi}(s)=\mathbb{E}\left[\sum_{t \geq 0} \gamma^{t} r_{t} \vert  s_{0}=s, \pi\right]$$

如何描述一个状态-动作好坏？
Q价值函数，如果执行这个动作的话，会有怎样的价值回报函数
$$Q^{\pi}(s, a)=\mathbb{E}\left[\sum_{t \geq 0} \gamma^{t} r_{t} \vert  s_{0}=s, a_{0}=a, \pi\right]$$

贝尔曼等式：
最好的Q函数$$Q^{*}(s, a)=\max _{\pi} \mathbb{E}\left[\sum_{t \geq 0} \gamma^{t} r_{t} \vert  s_{0}=s, a_{0}=a, \pi\right]$$，满足下面的贝尔曼方程。

$$Q^{*}(s, a)=\mathbb{E}_{s^{\prime} \sim \mathcal{E}}\left[r+\gamma \max _{a^{\prime}} Q^{*}\left(s^{\prime}, a^{\prime}\right) \vert  s, a\right]$$

其实就是动态规划，如果下一个时刻的最优(状态-动作)值是已知的话，最优的策略就是执行动作，最大化这个期望$$r+\gamma Q^{*}\left(s^{\prime}, a^{\prime}\right)$$

最优的策略pi可以根据最优的Q得到。

使用贝尔曼方程的迭代形式：$$Q_{i+1}(s, a)=\mathbb{E}\left[r+\gamma \max _{a^{\prime}} Q_{i}\left(s^{\prime}, a^{\prime}\right) \vert  s, a\right]$$
当i趋于无穷的时候，$$Q_i收敛于Q^*$$

但是这有什么问题？
没啥比例性，所有的计算的$$Q$$都是一个状态-动作对，如果状态是游戏像素的话，那么用整个状态空间就是不可能的了。
解决办法：使用一个函数去逼近$$Q(s,a)$$，也就是用一个神经网络！

优化问题的解法：Q学习
使用一个函数来近似器来估计动作值函数：
$$Q(s, a ; \theta) \approx Q^{*}(s, a)$$
如果是个深度神经网络的话就是深度q学习了。

\theta是函数参数，权重。
记住这个Q函数要满足贝尔曼方程$$Q^{*}(s, a)=\mathbb{E}_{s^{\prime} \sim \mathcal{E}}\left[r+\gamma \max _{a^{\prime}} Q^{*}\left(s^{\prime}, a^{\prime}\right) \vert  s, a\right]$$。

前向传播：
损失函数：
$$L_{i}\left(\theta_{i}\right)=\mathbb{E}_{s, a \sim \rho(\cdot)}\left[\left(y_{i}-Q\left(s, a ; \theta_{i}\right)\right)^{2}\right]$$
这里的$$y_{i}=\mathbb{E}_{s^{\prime} \sim \mathcal{E}}\left[r+\gamma \max _{a^{\prime}} Q\left(s^{\prime}, a^{\prime} ; \theta_{i-1}\right) \vert  s, a\right]$$

后项传播：
$$\nabla_{\theta_{i}} L_{i}\left(\theta_{i}\right)=\mathbb{E}_{s, a \sim \rho(\cdot) ; s^{\prime} \sim \mathcal{E}}\left[r+\gamma \max _{a^{\prime}} Q\left(s^{\prime}, a^{\prime} ; \theta_{i-1}\right)-Q\left(s, a ; \theta_{i}\right)\right) \nabla_{\theta_{i}} Q\left(s, a ; \theta_{i}\right) ]$$

迭代如果Q函数的最优是Q^*的话，使得Q值离真实值y_i越来越接近。

对于上面的阿塔日游戏来说，当前状态s_t是一个84x84x4堆叠在一起的最后4个帧，网络结构是：

输入Input 84x84x4 --> conv 16x8x8, stride4 --> 32x4x4 --> FC-256 --> FC-4(Q值)。

最后的输出有4个动作，对应4个状态。$$Q(s_t,a_1),Q(s_t,a_2),Q(s_t,a_3),Q(s_t,a_4)$$

依据atari游戏来决定动作的数量。

单次前向pass来计算所有动作的q值->效率高。

训练q学习的方法：经验重放
- 如果样本都是相关的话，学习是不咋好的
- 现在的学习参数决定了下一次训练的样本，也就是如果最大化行为移到左边的话，训练样本会被左边的样本所支配，导致很差的反馈环。

所以要使用经验重放的方法。
- 连续更新一个重放记忆表，作为游戏经验。
- 训练q学习的时候偶使用记忆表中的转化，而不是连续样本。
- 每次转化都能够对多个权重更新，数据效率高。

放在一起：有经验重放的深度q学习
初始化重放记忆D，容量N
初始化Q函数，给随机权重
迭代M次，对于每一次迭代：（玩M个关卡，完整的游戏）
- 初始化序列$$s_1 = {x_1}$$，预处理序列：$$\phi_1 = \phi(s_1)$$  （初始化状态，打开游戏屏幕）
- 循环T次，对每一个时间t  （每次游戏）
  - 以$$\epsilon$$的概率选择一个随机的动作a_t，否则的话选择$$a_{t}=\max _{a} Q^{*}\left(\phi\left(s_{t}\right), a ; \theta\right)$$  （以一个小概率随机动作，否则执行贪婪搜索）
  - 在模拟器中执行动作a_t，观测回报r_t，和下一张图像x_{t+1}
  - s_{t+1} = s_t, a_t, x_{t+1}，预处理$$\phi_{t+1} = \phi(s_{t+1})$$  （执行动作，观察回报，和下一个状态）
  - 在D中存储变换$$(\phi_t, a_t, r_t, \phi_{t+1})$$  （存储等待重放）
  - 在D中随机采样一个小batch$$(\phi_j, a_j, r_j, \phi_{j+1})$$
  - 设定y_j为：（经验重放，从转化D当中随机找一个batch，执行梯度下降）
    - 对于$$\phi_{j+1}$$终止了的话，r_j
    - 对于$$\phi_{j+1}$$没有终止，r_j + \gamma \max_{a'}Q(\phi_{j+1},a';\theta)
  - 对$$\left(y_{j}-Q\left(\phi_{j}, a_{j} ; \theta\right)\right)^{2}$$执行梯度下降。

策略梯度：

Q学习有什么问题？
Q函数太复杂了

比如一个机器人抓一个在非常高维空间的物体，很难学到每一个确定的值对。

但是策略可以更简单，只需要靠近就可以了。
我们能不能直接学习一个策略，也就是从一个策略集合当中找到最好的策略？

首先定义一个参数化的策略类：$$\Pi=\left\{\pi_{\theta}, \theta \in \mathbb{R}^{m}\right\}$$
对于每个策略，定义它的值：$$J(\theta)=\mathbb{E}\left[\sum_{t \geq 0} \gamma^{t} r_{t} \vert  \pi_{\theta}\right]$$

找到最好的策略$$\theta^{*}=\underset{\theta}{\arg \max } J(\theta)$$，怎么找？
对策略参数进行梯度上升找。

数学上的形式$$\begin{aligned} J(\theta) &=\mathbb{E}_{\tau \sim p(\tau ; \theta)}[r( \tau) ] \\ &=\int_{\tau} r(\tau) p(\tau ; \theta) \mathrm{d} \tau \end{aligned}$$

这个是回报轨迹$$\tau=\left(s_{0}, a_{0}, r_{0}, s_{1}, \ldots\right)$$

强化算法：

求梯度：$$\nabla_{\theta} J(\theta)=\int_{\tau} r(\tau) \nabla_{\theta} p(\tau ; \theta) \mathrm{d} \tau$$
不好求，当p依赖\theta的时候，一个期望的梯度是求不出来的。
但是可以trick：$$\nabla_{\theta} p(\tau ; \theta)=p(\tau ; \theta) \frac{\nabla_{\theta} p(\tau ; \theta)}{p(\tau ; \theta)}=p(\tau ; \theta) \nabla_{\theta} \log p(\tau ; \theta)$$

如果推回去的话，得到$$\begin{aligned} \nabla_{\theta} J(\theta) &=\int_{\tau}\left(r(\tau) \nabla_{\theta} \log p(\tau ; \theta)\right) p(\tau ; \theta) \mathrm{d} \tau \\ &=\mathbb{E}_{\tau \sim p(\tau ; \theta)}\left[r(\tau) \nabla_{\theta} \log p(\tau ; \theta)\right] \end{aligned}$$

可以用蒙特卡洛来求了。

如果不知道转化可能性的话，咋求这些量捏？
$$p(\tau ; \theta)=\prod_{t \geq 0} p\left(s_{t+1} \vert  s_{t}, a_{t}\right) \pi_{\theta}\left(a_{t} \vert  s_{t}\right)$$

因此，$$\log p(\tau ; \theta)=\sum_{t \geq 0} \log p\left(s_{t+1} \vert  s_{t}, a_{t}\right)+\log \pi_{\theta}\left(a_{t} \vert  s_{t}\right)$$

进行差分的时候，$$\nabla_{\theta} \log p(\tau ; \theta)=\sum_{t \geq 0} \nabla_{\theta} \log \pi_{\theta}\left(a_{t} \vert  s_{t}\right)$$
就把后面的一项消掉了，所以这是不依赖于转化可能性的

然后当对轨迹进行采样的时候，可以估计这个状态：
$$\nabla_{\theta} J(\theta) \approx \sum_{t \geq 0} r(\tau) \nabla_{\theta} \log \pi_{\theta}\left(a_{t} \vert  s_{t}\right)$$

如果$$r(\tau)$$很大得到话，增大这个动作被执行的可能性
否则就减少这个可能性。

好像如果一条轨迹是好的话，这个轨迹上面的所有动作都是好的，但是这只是平均结果。
但是这也有很高的变化，因为信用卡配置是很难的。我们能不能帮帮这个估计器？

第一种想法：提高所能看到的行为的可能性，只通过从状态当中累计外来的回报
$$\nabla_{\theta} J(\theta) \approx \sum_{t \geq 0}\left(\sum_{t^{\prime} \geq t} r_{t^{\prime}}\right) \nabla_{\theta} \log \pi_{\theta}\left(a_{t} \vert  s_{t}\right)$$

第二种想法：使用打折因子来忽视延迟效果：
$$\nabla_{\theta} J(\theta) \approx \sum_{t \geq 0}\left(\sum_{t^{\prime} \geq t} \gamma^{t^{\prime}-t} r_{t^{\prime}}\right) \nabla_{\theta} \log \pi_{\theta}\left(a_{t} \vert  s_{t}\right)$$

减少方差：
问题：轨迹的原始值不一定非常有意义，比如如果回报都是正的的话，就提升这个可能性就可以了。

但又有什么问题呢？如何判断一个回报是更好或者更坏的呢？

想法：引入一个底线函数，依赖于当前的状态，具体来说如下：
$$\nabla_{\theta} J(\theta) \approx \sum_{t \geq 0}\left(\sum_{t^{\prime} \geq t} \gamma^{t^{\prime}-t} r_{t^{\prime}}-b\left(s_{t}\right)\right) \nabla_{\theta} \log \pi_{\theta}\left(a_{t} \vert  s_{t}\right)$$。

怎么去找这个底线呢？
- 一个简单的底线是：从所有得到的轨迹当中，选择目前实验过程中的常数移动平均回报。
方差减少技术使用Vanilla reinforce
- 更好的底线：如果这个行为比我们应该从这个状态当中得到的期望值高的话，就从一个状态当中提升一个行为的概率。

这提示我们使用q学习和价值函数。

直观上来说，我们对于一个在状态s_t中的行为a_t，如果$$Q^{\pi}\left(s_{t}, a_{t}\right)-V^{\pi}\left(s_{t}\right)$$很大的话，我们应该会很高兴。相反如果这个行为很小的话我们应该不高兴。

使用这个，我们得到了这个估计器：
$$\nabla_{\theta} J(\theta) \approx \sum_{t \geq 0}\left(Q^{\pi_{\theta}}\left(s_{t}, a_{t}\right)-V^{\pi_{\theta}}\left(s_{t}\right)\right) \nabla_{\theta} \log \pi_{\theta}\left(a_{t} \vert  s_{t}\right)$$

演员-批评家算法
问题，我们不知道Q和V，我们怎么学习他们呢？

当然用Q学习啊！我们可以一起用策略梯度和Q学习，通过训练一个演员（策略）和一个批评家（Q函数）

演员决定了应该执行什么样的动作，批评家告诉演员怎么这个行为有多好，应该怎么去调整。
同时减轻批评家的任务，他只需要学习通过策略生成出来的状态，行为pair。
也可以使用Q学习trick，比如经验重放
另外：可以定义`优势函数`，来决定一个行为比期望的有多好：$$A^{\pi}(s, a)=Q^{\pi}(s, a)-V^{\pi}(s)$$

演员-批评家算法：
初始化策略参数\theta, 批评家参数\phi
对于每次迭代结果：
  从m个轨迹当中采样
  先把delta theta设为0
  遍历这m个轨迹，轨迹编号为i
    遍历时间1到T，时间为t
      $$A_{t}=\sum_{t^{\prime} \geq t} \gamma^{t^{\prime}-t} r_{t}^{i}-V_{\phi}\left(s_{t}^{i}\right)$$
      $$\Delta \theta \leftarrow \Delta \theta+A_{t} \nabla_{\theta} \log \left(a_{t}^{i} \vert  s_{t}^{i}\right)$$
  $$\Delta \phi \leftarrow \sum_{i} \sum_{t} \nabla_{\phi}\left\vert A_{t}^{i}\right\vert ^{2}$$
  $$\theta \leftarrow \alpha \Delta \theta$$
  $$\phi \leftarrow \beta \Delta \phi$$

在行为上的增强学习：递归注意力模型
目标：图像分类
选择一系列的glimpse，选择性地关注图像，去预测这个类
- 灵感来自人类的感知和眼睛的移动
- 保存计算的资源，有尺度信息
- 能够忽略图像的不相关的部分
状态：当前看到的glimpse
行为：（x，y）坐标（在这个glimpses的中心）以及下面应该看这幅图像的哪一个部分。
回报：在这个图像被正确分类的时候设为1，否则设为0

glimpsing是一个不可微分的操作，所以使用增强学习策略来学习如何得到glimpse
给了当前glimpse看到的状态，使用RNN来建模和输出下一个动作。
在很多范围内被使用，包括细粒度图像识别，图像标注和视觉问答。

更策略梯度：alpha go
混合了监督学习和强化学习
混合了旧方法（蒙特卡洛树搜索）和最近的（深度强化学习）

怎么打赢柯洁？
- 对棋盘取特征，（棋子的颜色，移动合法性，偏差等）
- 从专业的比赛当中，通过监督训练，初始化策略网络。
- 然后使用策略梯度继续训练（从前面的棋局当中自己和自己下棋，对于赢和输给+1/-1的回报
- 然后学习价值网络（批评家）
- 最后连接策略和价值网络，使用蒙特卡洛树，找到算法通过镶嵌搜索选择行为。

总结：
策略梯度法：非常广泛，但是方差非常大，所以需要很多样本。挑战：样本的效率
Q学习：不一定总是管用，但是当他管用的时候一般都是样本效率很高的，挑战是搜索太难了。

要使得这些能够使用的话，保证这些东西：
- 策略梯度：收敛$$J(\theta)$$到局部的最小一般已经够好了
- Q学习：没啥保证的，因为你通过贝尔曼方程得到了一个复杂的函数近似器。

深度学习：
自动驾驶，机器翻译，阿尔法狗，智能机器。模型变得越来越大。
模型太大带来了更大的空间，很难在线更新。
速度太慢了，resnet152要10天qwq
能耗太大了，alpha go需要1920块cpu，280块gpu，每局比赛耗电3000美元。

哪里耗电多呢？更大的模型，就要更大的内存，就更耗电。
一次内存的储存要比以及乘法者加法耗电1000倍。
如何能够使得深度学习更省电呢？

算法硬件同时设计！

应用是个黑盒子，在硬件设计的时候把这个盒子打开，打破了算法和硬件的壁垒。

硬件：
- 一般用途：
  - CPU： 延迟导向
  - GPU： 吞吐量导向
- 特殊硬件
  - FPGA： 可编程逻辑
  - ASIC： 固定逻辑

提升效率的方法：
1. 修剪
2. 权重共享
3. 量化
4. 低秩近似
5. 二元/三元网络
6. winograd变换

对神经网络进行修剪。可以修剪神经元，也可以修剪连接。
修减一些，然后恢复一些。

修剪一些的RNN和LSTM效率高一点。

新生儿有50万亿连接，1岁有1千万亿连接，成人有500万亿连接。

训练量化：2.09,2.12,1.92，1.87其实都是2.0。本来是32bit现在只需要占用4bit。

先训练权重，然后训练离散的权重。
然后找到最少的bit数。

剪枝+训练量化一起的话，压缩率就很高了。

哈弗曼编码：越经常出现的权重用最少的bit来表示。

压缩之后vggnet的权重能从550M到11.3M。
squeezeNet深度压缩的话，能到470kb。

结果加速，并且能耗降低。

训练的时候用浮点数，最后转化成固定小数点的格式

低rank近似，把矩阵变成可以分解才行。

用改进硬件的方法来提升效果：一个共同的目标：最小化内存存储。

google的TPU是用来计算8bit整数的，稠密矩阵的计算。对稀疏结构留有一些冗余，稀疏性会在以后的设计中有更高的优先级。

## 我的看法：

好像强化学习的网络规模上都比分割的时候要小啊qwq