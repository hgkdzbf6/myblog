---
layout: post
title:  "cs231n"
date:   2019-03-12 14:38:13 +0800
categories: AI 
---
<script type="text/javascript" async
  src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/MathJax.js?config=TeX-MML-AM_CHTML">
</script>

## 推荐看东西的顺序：

1. 先看视频
2. **再看官网的md文件**
3. 看numpy
4. 自己写作业
5. 一个小时之后没有头绪看别人的答案
6. 分析别人是怎么做的，然后推导
7. 运行到没有bug

## 一些变量的说明吧

$$x_i$$： 图像信息
$$y_i$$： 实际标签信息
$$s=f(x_i,W)$$： 评分向量
$$k$$： 总的类别数量
$$l$$： 一幅图像的信息，也就是32\*32\*3+1酱
之后正则化的时候要用到。

## 图像分类

### 可能面临的问题：

1. 旋转，位移，视角的变化
2. 光照
3. 异型（动作不一样，也还是一个猫）
4. occlusion（只看到猫的一部分）
5. 背景干扰(环境和猫的颜色很相近)clutter
6. 组内分别（一张图上有好多猫，如何分开）

### 识别：特征方法

猫有眼睛耳朵……但是不好使。

两个函数，一个是train一个是predict。
Nearest Neighbor

图像距离的度量：L1距离，两个图像像素级别的减法的绝对值。

训练o（1）的时间，预测o(N)时间  

选择K和选择L1还是L2距离是超参数。

### 如何选择超参数？

最差：所有数据都拿来训练，K=1总是在所有训练数据上面训练的最好。

较差：把所有数据为测试集和训练集，对这个算法适合不适合新的数据是不知道的。

好：分为训练集，测试集和验证集。

### 交叉验证

把数据集平均分成n部分，拿固定的一份做测试室，其中n-1作为训练集，1分作为验证集。

如何区分验证集和测试集？验证集当中有真实的标签，测试集当中没有直接的手段拿到真实标签。

数据集如何设计是有效的？这是数据集设计者所要考虑的事情。

knn在图像上面从来不会被使用，因为：

1. 测试的时候非常慢。
2. 像素之间的距离没有多少有用信息。
3. 维数灾难。图片越大，越难以cover掉这个图像空间。 

## 损失函数

复习：knn
线性分类器
f=Wx+b
如何选择W？

wavy？啥意思？

$$L = \frac{1}{N}\sum_i{L_i(f(x_i, W),y_i)}$$

Notation：
$$(x_i, y_i)$$

$$x_i$$是图像，$$y_i$$是整数标签。

分数向量：
$$s = f(x_i, W)$$。

多分类SVM损失有如下形式：

$$
L_i = \sum_{j \neq y_{i}} \left\{\begin{array}{l l}{0}{~~~~~~~~~~~~~\text{if}~s_{y_i} \ge s_j + 1} \\ {s_{j}-s_{y_{i}}+1}{~~~~~~~\text{otherwise}}\end{array}\right.
$$

$$j \neq y_i$$，说明是不是真值的元素。
如果$$s_{y_i} \ge s_j + 1$$，说明这个估计是正确的，损失为0。
否则说明这个估计错误，损失为$$s_j - s_{y_i} + 1$$。

也能写成：

$$
\sum_{j \neq y_{i}} \max \left(0, s_{j}-s_{y_{i}}+1\right)
$$

Hinge? 什么意思

为什么在后面+1？我们对最后的分数是多少并不在意，加不加是随意的，为了使得结果更加稳定。

1. 如果改变🚘分数一点点的话，会对最后的结果有影响吗？
不会的呢qwq，因为🚘的分数已经超过其他分数很多了，更高或者更低都不会对损失函数造成影响呢。

2. 最大最小的损失是什么？
最小是0最大是无穷。

3. 如果W很小，你期望S应该是什么样？  
应该是x是特征的数量。这可以作为调试的依据。

4. 如果把所有的损失加起来，是什么东西（包括真值）？  
是原来的数量+1

5. 如果我们用均值而不是和来求的话会怎么样？         
不怎么样和原来一样qwq

6. 如果把损失函数改成了：

$$L_i = \sum_{j\ne y_i}\max(0,s_j - s_{y_i} + 1)^2$$
会怎样？
不是相同的算法了，会适应具体问题去使用不同的损失函数。依赖不同的应用
用平方的话对更坏的损失更敏感。更小的损失更不敏感。

numpy代码：

{% codeblock lang:python %}
def L_i_vectorlized( x, y,W):
    scoress = W.dot(x)
    margins = np.maximiun(0, scores - scores[y] + 1)
    margins[y] = 0
    loss_i = np.sum(margins)
    return loss_i
{% endcodeblock %}

如果存在W使得损失函数为0，W不是唯一的，比如2W。

训练时候的时间长短我们不关心，我们只关心测试时候的时间

奥卡姆剃刀，选择最简单的选择。

$$L(W) = \frac{1}{N}\sum_{i=1}^NL_i(f(x_i, W), y_i)+\lambda R(W)$$

加上常数损失，lambda决定要不要加这个误差。如果要使用更复杂的模型的话，你就需要克服模型复杂带来的惩罚项（有点正则化的味道），可以对模型的复杂度有一个测量。
后面确实再讲正则化qwq

$$\lambda $$正则化强度

L2正则化：$$R(W) = \sum_k \sum_l W^2_{k,l}$$

L1正则化：$$R(W) = \sum_k\sum_l\vertW_{k,l}\vert$$

弹性网络（L1 + L2) $$R(W) = \sum_k \sum_l \beta W^2_{k_l} + \vertW_{k,l}\vert$$

此外还有最大norm正则化（无穷范数？）
dropout,
fancier，
batch normalization
stochastic depth

MAP inference？最大后验概率。

Softmax 分类器，Multinominal Logistic Regression.

$$P(Y=k\vertX=x_i) = \frac{e^{s_k}}{\sum_j e^{s_j}}$$

$$s = f(x_i; W)$$

$$L_i = -\log P(Y=y_i\vert X=x_i)$$

总结：

$$L_i = -\log (\frac{e_{s_{y_i}}}{\sum_j e^{s_j}})$$

softmax之前，需要先exp，然后在正则化，然后在计算概率。

调试的时候检查第一次迭代情况，如果值不是log(C)的话，那肯定就有什么地方有问题。

softmax和上面的svm不一样，就算结果非常正确，也只是正确值接近正无穷，错误值接近负无穷。


总结：

我们有数据集$$(x,y)$$
评分函数$$s=f(x;W)=Wx$$
损失函数$$L_i$$

$$L_i = -\log(\frac{e^{s_{y_i}}}{\sum_j e^{s_j}})$$  `Softmax`

$$L_i=\sum_{j \ne y_i} \max(0, s_j-s_{y_i}+1)$$     `SVM`

$$L = \frac{1}{N}\sum_{i=1}^N L_i + R(W)$$  `全损失`

如何找到这个参数？优化方法：

1. 随机搜索
{% codeblock lang:python %}
bestloss = float('inf')
for num in range(1000):
    W = np.random.randn(10,3073)*0.0001 # 产生随机参数
    loss = L(X_train, Y_train, W)
    if loss < bestloss:
        bestloss = loss
        bestW = W
    print('在第%d次试验当中，损失为%f，当前最好的损失为%f' % (num, loss, bestloss))
{% endcodeblock %}

{% codeblock lang:python %}
# 设数据集X_test为[3073*10000],Y_test为[10000*1]
scores = Wbest.dot(Xte_cols)
# 预测步骤，找到在每一列当中每个序号对应的最大分数
Yte_predict = np.argmax(scores, axis=0)
# 计算准确度
np.mean(Yte_predict == Yte)
{% endcodeblock %}

比较好的方法：利用当前点的周围信息。
一维情况，导数：

$$\frac{df(x)}{dx} = \lim_{h \to 0}\frac{f(x+h)-f(x)}{h}$$

多维情况，梯度
梯度方向是方向和梯度的点乘

$$L = \frac{1}{N}\sum_{i=1}^N L_i + \sum_k W^2_k$$

$$L_i=\sum_{j \ne y_i} \max(0, s_j-s_{y_i}+1)$$

`SVM`

想要求下面的结果：

$$\nabla_WL$$

可以得到解析的梯度。数值计算梯度也是一个调bug的好办法。单元测试，真正测试的时候要减小问题规模。

梯度下降：

{% codeblock lang:python %}
while True:
    weight_grad = evaluate_gradient(loss_fun, data,weights)
    weights += step_size * weights_grad
{% endcodeblock %}

minibatch gradient descent:

$$\nabla_WL(W) = \frac{1}{N} \sum _{i=1}^N \nabla_WL_i(x_i, y_i, W)+\lambda \nabla_WR(W)$$

一般N可能会很大，一起计算成本太高了。
梯度是线性操作，所以选择minibatch来计算，比如32/64/128。

图像特征。两步法：第一步计算特征，第二部、步进行图像分类。

特征变化，比如同心圆转化成极坐标，这样就能分离了。

特征比如是：
1. 颜色直方图。
2. 梯度直方图。320\*240的图片分成40\*30的桶，每个桶有9个元素，所以一共有30\*40\*9=10800个元素。
3. 词袋模型：首先计算codebook，然后随机从图像当中截取小块，然后对小块进行编码，单词计算，然后对图像进行编码。

当然有卷积神经网络来比较。
是事先定义好特征的算法，还是从数据当中学习特征？
特征方法多进行了一步。

## 关于softmax和svm的性能比较。

一般来说差不多，不同的人对哪个分类器更好，有不同的观点。svm是一个本地化的目标，可以起将其视为bug或者特点。svm已经能够看到正确的类比错误的类分数显著高了（通过加的那个1来体现）的话，就不再计算其损失。而softmax分类器永远不会对他的得分感到满意，正确的类总是有更高的概率，错误的类总是有更低的概率，并且损失总会变得更好，所以一旦margin得到满足，softmax会变动的更好。也就是说，汽车分类器可以将大部分的努力用于在🚗和卡车之间的区别，但是并不会收到🐸类别不同的影响，因为🐸已经有一个非常低的得分了，并且聚集在数据云的另一侧。

## 下面来做作业

哎这个作业要自己想，想破脑袋也想不出来，还是去网上查找了资料，参考了点星星数量最多的人的代码，也就是[这里](https://github.com/lightaime/cs231n/blob/master/assignment1/cs231n/classifiers/linear_svm.py)。

### 第一个计算叫做$$dW$$的东西

发现还是要推导一点东西才行的，ppt上面什么都没写，也只是给了个参考文档qwq。

再次感谢一个软件叫做MathPix，直接从ppt上面截图，省去了我不少打公式的时间呢qwq。

现在开始推导$$dW$$的解析解：

$$L=\frac{1}{N} \sum_{i=1}^{N} L_{i}+\sum_{k} W_{k}^{2}$$

$$L_{i}=\sum_{j \neq y_{i}} \max \left(0, s_{j}-s_{y_{i}}+1\right)$$

$$s=f(x ; W)=W x$$

要求：$$\nabla_{W} L$$

对$$L$$关于$$W$$求导，那么就有：

$$\frac{dL}{dW} = \sum_{i=1}^{N} \frac {dL_{i}}{dW}+ 2 \sum_{k} W_{k} $$

现在求$$\frac{dL_i}{dW}$$，有：

<!-- $$\frac{dL_i}{dW} = \sum_{ j \ne y_i} \left \{ \begin{array}{l l}{0}{0} \\ {0}{0}\end{array} \right$$ -->

$$
\frac{dL_i}{dW} =\sum_{ j \ne y_i} {\frac {ds_{j}-ds_{y_{i}}+1 }{dW}}
$$

注意有个0的情况，和svm的条件一致。

因为$$s=f(x ; W)=W x$$，所以$$\frac{ds_j}{dW}=x_j^T$$，$$\frac{ds_{y_i}}{dW}=x_{y_i}^T$$

所以，一步一步能算的qwq。

效果还是蛮好的，第10个框。

## 下面是softmax的作业

这个作业有个坑的地方，主要也是怪自己没有认真听讲，将达到一个数值稳定性的问题：

讲义当中提到一个**Practical issues: Numeric stability**问题（ppt当中米有吧），官网上的md里面有，参考[这里](https://github.com/cs231n/cs231n.github.io/blob/master/linear-classify.md)。大意是说因为指数的缘故，指数值可能会非常大，所以就在原来的分数上面减去所有分数的最大值，才能得到正确答案。

交叉熵和最大边际loss。hinge(转轴；合页；枢纽) loss是验证svm的效果的算法，交叉熵是验证softmax的。

hinge loss的算法：max(0,s_j - s_y_i + 1)
交叉熵的算法：softmax计算每个标签的概率，按照归一化后的负对数算法得到概率。

然后应该就好写聊。

## softmax函数的loss和dW怎么算的问题

要求这个$$ \nabla_{W_k} L $$
我们首先有这个$$ \nabla_{W_k} L_i = \frac{ \partial L_i }{\partial W_k}$$

然后由于:
$$L_i = - \log \frac {e^{s_{y_i}}}{\sum_ j e^{s_j}} $$

设$$s_k = W_kx_k$$，所以$$\frac{\partial s}{\partial W_k} = x_k^T$$这里这么规定的原因是有可能三项的下标不相等，如果出现不相等的话，那么这个求导就是0了。

设$$P_k = \frac{e^{s_{y_i}}}{\sum_j e^{s_j}}$$所以：

$$\frac{ \partial L_i}{\partial W} = - \frac {\sum_j e^{s_j}}{e^{s_{y_i}}} \frac{\partial P_k}{\partial W_k}$$

根据链式法则我们求$$\frac{\partial P_k}{\partial W_k}$$。这里分为两种情况：

`1. ` 一种情况是$$y_i$$就是$$j$$，这种情况下，分子分母当中都含有自变量，也就是这种情况：

$$\frac{\partial P_k}{\partial W_k} = \frac {e^{s_{y_i}}} {\sum_j e^{s_j}} \cdot x_{y_i}^T- \frac{e^{s_{y_i}} e^{s_{y_i} }}{ (\sum_j e^{s_j})^2} \cdot x^T_{y_i} $$

所以原结果就变成了：

$$\frac{ \partial L_i}{\partial W} = x^T_{y_i} [ \frac{e^{s_{y_i}}}{\sum_j e^{s_j}} -1]$$

`2. ` 另一种情况，$$y_i$$和$$j$$不相等，那么就只有一个自变量在分母上了，也就是这种情况：

$$\frac{\partial P_k}{\partial W_k} = - \frac{e^{s_{y_i}} e^{s_{y_i} }}{ (\sum_j e^{s_j})^2} \cdot x^T_{y_i} $$

所以原结果就变成了：

$$\frac{ \partial L_i}{\partial W} = x^T_{y_i} \frac{e^{s_{y_i}}}{\sum_j e^{s_j}} \tag{1}$$

这样的话就能够计算dW了，注意两种情况分别讨论。可能在后面的时候讨论向量化的时候又会遇到问题，不过到时候再说吧qwq。

### 现在是向量化的`dW`和`loss`

`loss`好做，我都会做qwq，关键是`dW`。

`loss`的求法就是两点，一个是这个trick，`  S_y = S[range(num_train), list(y)].reshape(-1,1)`，另一个是要记住`reshape`，照着公式打就对了。

`dW`的求法，从两个循环的`dW`求法可以看出，`dW`的每一行加的值和j没有关系，也就是说对于每一列（先不考虑那个j和y[i]相等的情况），$$dW$$其实就是`X.T`和`softmax`计算值的点乘。再考虑相等情况，就是在原来的基础上减去一个`X.T`。这些都好办，关键是这个softmax计算值怎么求？

我们上一步推导的过程中，我们已经知道了对于dW要分成两种情况来讨论，不仅仅要考虑的正确值的影响，还要考虑错误值的影响。正确值影响分子和分母两部分，错误值影响分母值。所以要求$$\frac {\partial P_k}{\partial W_k}$$，这个值能转化到$$\frac {\partial S_k}{\partial W_k}$$上面，中间差的那个部分$$\frac {\partial P_k}{\partial S_k}$$就是softmax计算值，因为求导正好是一个softmax计算值。以上~

### 找最合适的超参数

抄svm的那个，很快的……不过还是要了解更新步骤。

1. 首先训练，得到相应的损失函数值和dW值。训练过程当中计算出损失函数值和dW之后，使用学习率来更新W。

2. 预测，预测步骤就是计算Wx，然后把分数都计算出来然后排序，排在最前面的的就是预测结果。

3. 判断结果好坏，通过验证集的正确率来判断。

4. 等到训练次数达到指定数量。

最后绘制一张图，用来可视化这个权重。可视化的方法是把[最小值，最大值]映射到[0，255]这个区间当中，然后再转成32\*32\*10的形状，然后看吧。从softmax这个例子里面什么都看不出来qwq。但是多类svm里面能够看到一点点汽车的轮廓呢有木有！