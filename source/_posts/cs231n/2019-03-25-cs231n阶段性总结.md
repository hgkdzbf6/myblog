---
layout: post
title:  "cs231n阶段性总结"
date:   2019-03-25 19:19:18 +0800
categories: AI
---

<script type="text/javascript" async
  src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/MathJax.js?config=TeX-MML-AM_CHTML">
</script>

## 首先从图像分类问题入手

什么是图像分类问题？

1. 预测图片所对应的真实标签

### 挑战：

1. 视角变换
2. 尺度变换
3. 异构
4. 遮挡
5. 光照条件
6. 背景条件
7. 类内分离

数据驱动的方法。给定一组数据，里面有图片和图片对应的标签。

### 流程：

1. 输入： N个样本，K个不同的类别
2. 学习过程：训练一个分类器，或者学习一个模型
3. 评估模型：计算分类出的结果，和真值进行比较，来确定分类器的好坏。

### NN(Nearest Neighbor Classifier)分类器：

1. 实际当中很少用，就了解基本方法就可以了qwq。
2. 数据集： CIFAR-10，10个分类，60000张图片。50000作为训练集和验证集，10000作为测试集。

### 距离：

L1距离：
$$d_1(I_1,I_2) = \sum_p \vertI^ps_1 - I^p_2 \vert$$
，就是每个像素点的差的绝对值的和。

L2距离:$$d_1(I_1,I_2) = \sum_p (I^ps_1 - I^p_2)^2$$。

### 开始测试

{% codeblock lang:python %}
def train(self, X,y):
    self.Xtr = X
    self.Ytr = Y

def predict(self,X):
    num_test = X.shape[0]
    Ypred = np.zeros(num_test, dtype=self.ytr.dtype)
    for i in range(num_test):
        # L1距离
        distances = np.sum(np.abs(self.Xtr - X[i,:]), axis=1)
        # L2距离，注意平方了之后还要开方的qwq
        distances = np.sum((self.Xtr - X[i,:])**2, axis=1) **(0.5)
        min_index  = np.argmin(distances)
        Ypred[i] =self.ytr[min_index]
    return Ypred
{% endcodeblock %}

两个距离的比较：
L1距离对于向量内部的差异不敏感，L2对于向量内部的差异很敏感。最后，我们看到在原始像素值上使用L1或L2距离是不够的，因为距离与图像的背景和颜色分布的相关性比与其语义内容的相关性更强。

### kNN

找k个最近的图片然后投票，票数最多的就是输出。k=1的话就是NN了。

验证最合适的超参数，这里超参数就只有一个k了（不，还有距离的选择，选择L1还是L2距离）

{% codeblock lang:python %}
# assume we have Xtr_rows, Ytr, Xte_rows, Yte as before
# recall Xtr_rows is 50,000 x 3072 matrix
Xval_rows = Xtr_rows[:1000, :] # take first 1000 for validation
Yval = Ytr[:1000]
Xtr_rows = Xtr_rows[1000:, :] # keep last 49,000 for train
Ytr = Ytr[1000:]

# find hyperparameters that work best on the validation set
validation_accuracies = []
for k in [1, 3, 5, 10, 20, 50, 100]:
  
  # use a particular value of k and evaluation on validation data
  nn = NearestNeighbor()
  nn.train(Xtr_rows, Ytr)
  # here we assume a modified NearestNeighbor class that can take a k as input
  Yval_predict = nn.predict(Xval_rows, k = k)
  acc = np.mean(Yval_predict == Yval)
  print 'accuracy: %f' % (acc,)

  # keep track of what works on the validation set
  validation_accuracies.append((k, acc))
{% endcodeblock %}

### 交叉验证

如果训练样本比较小的话，把原来的数据集平均分成n份，其中n-1份作为训练集，1份作为验证集，然后去训练酱。

### 优缺点分析：

优点：

1. 简单，容易理解。
2. 不需要什么时间去训练。

缺点：

1. 测试的时候花了太多时间了，因为每个样本都需要和训练样本进行比对。这和我们的日常使用是相违背的。
2. 低维度数据还行，高维数据就会爆炸。
3. 非常相似的数据有可能会得到非常不同的分类结果。
4. 受到背景影响太大了，比如🐸非常容易识别成🐶
5. 分类器必须记住所有训练数据并将其存储以备将来与测试数据进行比较。 这是空间效率低下的，因为数据集的大小可能很容易为千兆字节。
6. 对测试图像进行分类是昂贵的，因为它需要与所有训练图像进行比较。

也有其他NN的算法，比如[FLANN（Fast Library for Approximate Nearest Neighbors），ANN（Approximate Nearest Neighbor）](http://www.cs.ubc.ca/research/flann/)等。这些算法允许人们在检索期间通过检索其空间/时间复杂度来权衡最近邻居的正确性，并且通常与涉及构建kdtree或运行k均值算法的预处理/索引阶段相关联。

高维数据（尤其是图像）上的基于像素的距离可能非常不直观。 原始图像（左）和其旁边的三个其他图像，基于L2像素距离，它们都离它很远。 显然，像素距离根本不对应于感知或语义相似性。

t-SNE（数据降维）可视化技术。

### 总结：不在图像当中使用kNN：

1. 预处理，使得每一个元素的均值为0，方差为1
2. 如果数据非常高维，使用PCA技术（cs229,博客，wiki）
3. 70%-90%的数据用来训练。交叉验证一般会更好，但是计算成本更高。
4. 如果kNN的运行时间太长，使用FLANN等算法来加快速度（以一定的精度为代价）
5. 记下最优的超参数的结果。在验证集上面进行超参数的寻找，在测试集上进行该分类器的性能的描述。

### 好东西

[这个是个好东西](https://homes.cs.washington.edu/~pedrod/papers/cacm12.pdf)
读之！

## 线性分类器

两个主要的工具：`score函数`和`loss函数`。

公式：$$f(x_i,W,b) = W x_i + b$$

时刻注意维度是怎么样的：$$x_i$$展开之后是(D,1)的向量，$$W$$是(K,D)的向量，$$b$$是(K,1)的向量。所以写的可能需要反一下才能乘，比如`X.dot(W)+b`酱。

$$W x_i + b$$计算了10个分数，维数为(K,1)

这种方法的一个优点是训练数据用于学习参数W，b，但是一旦学习完成，我们就可以丢弃整个训练集并且仅保留学习的参数。这是因为新的测试图像可以简单地通过该功能转发并基于计算的分数进行分类。
最后，请注意，对测试图像进​​行分类涉及单个矩阵乘法和加法，这比将测试图像与所有训练图像进行比较要快得多。

f将包含更多的参数

### 线性分类器的一个解释

请注意，线性分类器将类的分数计算为其所有3个颜色通道中所有像素值的加权和。 根据我们为这些权重设置的确切值，该功能具有在图像中的某些位置处喜欢或不喜欢（取决于每个重量的符号）某些颜色的能力。 例如，你可以想象如果图像两侧有很多蓝色（可能与水相对应），“船”类可能更有可能。 你可能会认为“船”分类器在其蓝色通道权重上会有很多正权重（蓝色的存在会增加船的得分），红色/绿色通道中的负权重（红色/绿色的存在会降低分数） 船）。

这里的三个类和rgb三个通道没有关系

将图像类比为高维点。W的每一行都代表对应类别的一个分数，点乘x。

另外，请注意马模板似乎包含一个双头马，这是由于数据集中左右两侧的马。线性分类器将数据中的这两种马模式合并为单个模板。同样，汽车分类器似乎已将几种模式合并为一个模板，该模板必须识别来自所有侧面和所有颜色的汽车。特别是，这个模板最终变成红色，这暗示CIFAR-10数据集中的红色汽车比任何其他颜色都多。线性分类器太弱而无法正确解释不同颜色的汽车，但正如我们稍后将看到的，神经网络将允许我们执行此任务。展望未来，神经网络将能够在其隐藏层中开发中间神经元，可以检测特定的汽车类型（例如，面向左侧的绿色汽车，面向前方的蓝色汽车等），下一层的神经元可以将这些组合在一起通过各个汽车探测器的加权总和得到更准确的汽车得分。

偏差的trick：在x当中多一维1，在W当中也多一维度x。

图像数据预处理。 快速说明，在上面的例子中，我们使用了原始像素值（范围从[0 ... 255]）。 在机器学习中，总是对输入要素进行规范化是非常常见的做法（在图像的情况下，每个像素都被视为一个特征）。 特别是，通过从每个要素中减去平均值来居中数据非常重要。 在图像的情况下，这对应于计算训练图像上的平均图像并从每个图像中减去它以获得像素的范围从大约[-127 ... 127]的图像。 进一步常见的预处理是缩放每个输入要素，使其值的范围为[-1,1]。 其中，零均值中心可能更重要，但我们将不得不等待其理由，直到我们了解梯度下降的动态。

### 损失函数：

直观地说，如果我们在对训练数据进行分类方面做得很差，那么损失就会很高，如果我们做得好的话，它就会很低。

### 多分类支持向量机：

公式：

$$L_i = \sum_{j \ne y_i} \max(0, s_j - s_{y_j}+\Delta)$$

这个叫做hinge loss。或者也有L2-SVM，squared hinge loss。但是更标准的是没有平方的那一个。代入得到等价形式：

$$L_i = \sum_{j \ne y_i} \max(0, w_j^Tx_i - w_{y_j}^Tx_i+\Delta)$$

$$\Delta$$表示正确分类比其他分类的正确的显著程度。

### 正则化：

L2范数：

$$R(W) = \sum_k \sum_i W^2$$

最后的损失函数：

$$L = \frac{1}{N} \sum_i L_i + \lambda R(W)$$

展开形式：

$$L = \frac{1}{N} \sum_i \sum_{j \ne y_i}[\max(0,f(x_i,W)_j - f(x_i,W)_{y_i} + \Delta)] + \lambda \sum_k \sum_l W^2_{k,l}$$

如果有两个权重，一个是$$w_1 = [1,0,0,0]$$，一个权重是$$w_2 = [0.25,0.25,0.25,0.25]$$，L1距离是一样的，L2距离的话会更偏好更均匀的权重，也就是$$w_2$$。加入正则项能够减轻过拟合。

一般情况下，正则项只针对W而不针对b。因为有正则项的存在，我们的loss永远不会等于0。

{% codeblock lang:python %}
def L_i(x, y, W):
  """
  unvectorized version. Compute the multiclass svm loss for a single example (x,y)
  - x is a column vector representing an image (e.g. 3073 x 1 in CIFAR-10)
    with an appended bias dimension in the 3073-rd position (i.e. bias trick)
  - y is an integer giving index of correct class (e.g. between 0 and 9 in CIFAR-10)
  - W is the weight matrix (e.g. 10 x 3073 in CIFAR-10)
  """
  delta = 1.0 # see notes about delta later in this section
  scores = W.dot(x) # scores becomes of size 10 x 1, the scores for each class
  correct_class_score = scores[y]
  D = W.shape[0] # number of classes, e.g. 10
  loss_i = 0.0
  for j in xrange(D): # iterate over all wrong classes
    if j == y:
      # skip for the true class to only loop over incorrect classes
      continue
    # accumulate loss for the i-th example
    loss_i += max(0, scores[j] - correct_class_score + delta)
  return loss_i

def L_i_vectorized(x, y, W):
  """
  A faster half-vectorized implementation. half-vectorized
  refers to the fact that for a single example the implementation contains
  no for loops, but there is still one loop over the examples (outside this function)
  """
  delta = 1.0
  scores = W.dot(x)
  # compute the margins for all classes in one vector operation
  margins = np.maximum(0, scores - scores[y] + delta)
  # on y-th position scores[y] - scores[y] canceled and gave delta. We want
  # to ignore the y-th position and only consider margin on max wrong class
  margins[y] = 0
  loss_i = np.sum(margins)
  return loss_i

def L(X, y, W):
  """
  fully-vectorized implementation :
  - X holds all the training examples as columns (e.g. 3073 x 50,000 in CIFAR-10)
  - y is array of integers specifying correct class (e.g. 50,000-D array)
  - W are weights (e.g. 10 x 3073)
  """
  # evaluate loss over all examples in X without using any for loops
  # left as exercise to reader in the assignment
{% endcodeblock %}

问题转化成找到一个W能够最小化loss。

实际考量：

$$\Delta$$一般设定为1就可以了。$$\Delta$$和$$\lambda$$虽然是两个不同的超参数，但是控制的是同一个东西：loss的下降和reg的权重。
理解这一点的关键是权重W的大小直接影响分数（因此也影响它们的差异）：当我们缩小W内的所有值时，分数差异将变得更低，并且当我们按比例增加分数时 差异将变得更高。 因此，得分之间的边界的精确值（例如Δ= 1
或者Δ= 100）在某种意义上是没有意义的，因为权重可以任意缩小或拉伸差异。 因此，唯一真正的权衡是我们允许权重增长的程度（通过正则化强度λ）。

和二进制支持向量机的关系：

$$L_i = C \max(0,1-y_iw^Tx_i) + R(W)$$

C是超参数，$$y_i \in \{-1,1\}$$，可以看成是多类svm的一个特殊情况

### softmax

$$L_i = - log \frac{e^{s_{y_i}}}{\sum_j e^{s_j}}$$

或者等价形式：

$$L_i = - s_{y_i} + log \sum_j e^{s_j}$$

又叫做交叉熵损失（cross entropy loss）

信息理论：
$$H(p, q)=-\sum_{x} p(x) \log q(x)$$

p是识别正确的情况$$p=[0, \ldots 1, \ldots, 0]$$，q是识别错误的情况$$q=e^{f_{y_{i}} /} \sum_{j} e^{f_{j}}$$

$$H(p, q)=H(p)+D_{K L}(p \vert q)$$

换句话说，交叉熵目标希望预测分布的所有质量都在正确的答案上。

$$P\left(y_{i} \vert x_{i} ; W\right)=\frac{e^{f_{y_{i}}}}{\sum_{j} e^{f_{j}}}$$

这个表达式可以表示为给了X和W之后，得到正确$$y_i$$标签的概率。在概率学上的解释可以解释为最大似然估计（Maximum Likelihood Estimation）。可以把正则项解释为一个先验知识，所以最后的损失时可以当做是一个（Maximum a posteriori）最大后验估计。

数值稳定性：我们对每一个值都减去最大的分数，为了保证数字稳定性，因为指数会爆炸。

$$\frac{e^{f_{y_{i}}}}{\sum_{j} e^{f_{j}}}=\frac{C e^{f_{y_{i}}}}{C \sum_{j} e_{j}^{f_{j}}}=\frac{e^{f_{i_{i}}+\log C}}{\sum_{j} e^{f_{j}+\log C}}$$

svm分类器使用hinge loss，或者叫做最大边际loss。softmax使用交叉熵。

softmax给出每个分类的概率。svm不好去定义这个分数的具体含义。

例如，假设某些三个类的非标准化对数概率为[1，-2,0]。 然后softmax函数将计算结果为[0.7,0.04,0.26]。采取的步骤是取幂和归一化为一。 现在，如果正则化强度λ更高，则权重W将受到更多惩罚，这将导致更小的权重。 例如，假设权重变小了一半（[0.5，-1,0]）。 softmax现在计算结果为： [ 0.55 , 0.12 , 0.33 ]。现在概率更加分散。 此外，在由于非常强的正则化强度λ而权重朝向微小数量的极限中，输出概率将接近均匀。 因此，Softmax分类器计算的概率更好地被认为是置信度，其中类似于SVM，分数的排序是可解释的，但`绝对数字（或它们的差异）在技术上不是`。

一般来说，softmax和svm差不多。SVM并不关心单个分数的细节：如果它们是[10，-100，-100]或[10,9,9]，则SVM将无关紧要，因为1的余量得到满足，因此损失是零。然而，这些场景并不等同于Softmax分类器，它会为分数[10,9,9]累积比[10，-100，-100]更高的损失。Softmax分类器永远不会对它产生的分数感到满意：正确的类总是具有更高的概率，不正确的类总是具有更低的概率，并且损失总是会变得更好。然而，一旦边际得到满足，SVM就会感到高兴，并且它不会对超出此约束的精确分数进行微观管理。这可以直观地被认为是一个特征：例如，汽车分类器可能将大部分“努力”用于将汽车从卡车上分离出来的困难问题，不应该受到蛙类实例的影响，因为它已经分配得非常低得分为，并且可能聚集在数据云的完全不同的一侧。


### 总结

与kNN分类器不同，这种参数化方法的优点是，一旦我们学习了参数，我们就可以丢弃训练数据。另外，对新测试图像的预测是快速的，因为它需要与W的单个矩阵乘法，而不是与每个训练示例的详尽比较。

我们引入了偏置技巧，它允许我们将偏置矢量折叠到权重矩阵中，以方便仅跟踪一个参数矩阵。（就是b写到W里面）

我们定义了一个损失函数（我们为线性分类器引入了两个常用的损失：SVM和Softmax），它们测量一组给定参数与训练数据集中的地面实况标签的兼容性。我们还发现损失函数的定义方式是对训练数据做出良好的预测相当于损失很小。

我们现在看到了一种方法来获取图像数据集，并根据一组参数将每个图像映射到类别得分，我们看到了两个可用于衡量预测质量的损失函数示例。但是，我们如何有效地确定产生最佳（最低）损失的参数？

## 三个组成部分

1. score function
2. loss function
3. optimization

随机梯度下降

$$L=\frac{1}{N} \sum_{i} \sum_{j \neq y_{i}}\left[\max \left(0, f\left(x_{i} ; W\right)_{j}-f\left(x_{i} ; W\right)_{y_{i}}+1\right)\right]+\alpha R(W)$$

看到loss函数：一维的折线图，热力图。

比如我们有:
$$L_{i}=\sum_{j \neq y_{i}}\left[\max \left(0, w_{j}^{T} x_{i}-w_{y_{i}}^{T} x_{i}+1\right)\right]$$

最后有：

$$\begin{aligned} L_{0} &=\max \left(0, w_{1}^{T} x_{0}-w_{0}^{T} x_{0}+1\right)+\max \left(0, w_{2}^{T} x_{0}-w_{0}^{T} x_{0}+1\right) \\ L_{1} &=\max \left(0, w_{0}^{T} x_{1}-w_{1}^{T} x_{1}+1\right)+\max \left(0, w_{2}^{T} x_{1}-w_{1}^{T} x_{1}+1\right) \\ L_{2} &=\max \left(0, w_{0}^{T} x_{2}-w_{2}^{T} x_{2}+1\right)+\max \left(0, w_{1}^{T} x_{2}-w_{2}^{T} x_{2}+1\right) \\ L &=\left(L_{0}+L_{1}+L_{2}\right) / 3 \end{aligned}$$

其实这就是三个函数的最大值。碗形状其实是凸函数的一种，可以使用凸优化方法。次梯度方法。

## 优化方法：

1. 纯随机……这

    核心思想：迭代最优化。
    盲人远足类比。

2. 本地随机搜索

    在原来的基础上走一个小增量，但是这个方向是随机的，也是垃圾。

3. 跟随梯度

    没有必要使用随机的方向，使用梯度方向就可以了。

$$\frac{d f(x)}{d x}=\lim _{h \rightarrow 0} \frac{f(x+h)-f(x)}{h}$$

### 数值法计算梯度

{% codeblock lang:python %}
def eval_numerical_gradient(f, x):
  """ 
  a naive implementation of numerical gradient of f at x 
  - f should be a function that takes a single argument
  - x is the point (numpy array) to evaluate the gradient at
  """ 

  fx = f(x) # evaluate function value at original point
  grad = np.zeros(x.shape)
  h = 0.00001

  # iterate over all indexes in x
  it = np.nditer(x, flags=['multi_index'], op_flags=['readwrite'])
  while not it.finished:

    # evaluate function at x+h
    ix = it.multi_index
    old_value = x[ix]
    x[ix] = old_value + h # increment by h
    fxh = f(x) # evalute f(x + h)
    x[ix] = old_value # restore to previous value (very important!)

    # compute the partial derivative
    grad[ix] = (fxh - fx) / h # the slope
    it.iternext() # step to next dimension

  return grad
{% endcodeblock %}

实际考虑：考虑到数值稳定的问题，使用centered difference formula:

$$[f(x+h)-f(x-h)] / 2 h$$


沿着负梯度更新：学习率是最重要的一个要调整的超参数。太小太大都不好。如果步长很大，则会爆炸的。

效率问题：您可能已经注意到，评估数值梯度的参数数量具有线性复杂性。 在我们的示例中，我们总共有30730个参数，因此必须执行30,731个损失函数的评估，以评估梯度并仅执行单个参数更新。 这个问题只会变得更糟，因为现代神经网络很容易拥有数以千万计的参数。 显然，这种策略不可扩展，我们需要更好的东西。（不能用数值方法进行求解，要解析）

计算梯度的第二种方法是使用微积分进行分析，这使得我们可以推导出梯度的直接公式（无近似值），计算速度也非常快。但是，与数值梯度不同，它可能更容易实现，这就是为什么在实践中计算分析梯度并将其与数值梯度进行比较以检查实现的正确性是很常见的。这称为梯度检查（ gradient check）

比如这个东西的梯度：

$$L_{i}=\sum_{j \neq y_{i}}\left[\max \left(0, w_{j}^{T} x_{i}-w_{y_{i}}^{T} x_{i}+\Delta\right)\right]$$

就可以这么求：

$$\nabla_{w_{y_{i}}} L_{i}=-\left(\sum_{j \neq y_{i}} 1\left(w_{j}^{T} x_{i}-w_{y_{i}}^{T} x_{i}+\Delta>0\right)\right) x_{i}$$

𝟙是指示函数，当里面的情况满足的时候为1，否则为0。虽然写起来很可怕的公式，但是实际上你只需计算不符合所需余量的类的数量（因此有助于损失函数）然后乘以数据$$x_i$$就可以了。

对于其他$$j \ne y_i$$的情况，梯度就变成了：

$$\nabla_{w_{j}} L_{i}=\mathbb{1}\left(w_{j}^{T} x_{i}-w_{y_{i}}^{T} x_{i}+\Delta>0\right) x_{i}$$

一旦为梯度派生表达式，就可以直接实现表达式并使用它们来执行梯度更新。

### Vanilla梯度更新

{% codeblock lang:python %}
# Vanilla Gradient Descent

while True:
  weights_grad = evaluate_gradient(loss_fun, data, weights)
  weights += - step_size * weights_grad # perform parameter update
{% endcodeblock %}

当然也有其他方法来执行优化，比如（LBFGS），但是梯度下降还是最常用的方法。

### 小批量梯度下降

{% codeblock lang:python %}
# Vanilla Minibatch Gradient Descent

while True:
  data_batch = sample_training_data(data, 256) # sample 256 examples
  weights_grad = evaluate_gradient(loss_fun, data_batch, weights)
  weights += - step_size * weights_grad # perform parameter update
{% endcodeblock %}

例如，在当前最先进的ConvNets中，典型的批次包含来自120万的整个训练集的256个示例。 然后，此批处理用于执行参数更新。这很好用的原因是训练数据中的例子是相关的。

然后很明显，我们为所有1200个相同副本计算的梯度都是相同的，当我们对所有120万个图像的数据丢失进行平均时，我们将得到完全相同的损失，就好像我们只评估了一小部分图像一样。

当然，在实践中，数据集不包含重复图像，来自小批量的梯度是完整目标的梯度的良好近似。因此，通过评估小批量梯度以执行更频繁的参数更新，在实践中可以实现更快的收敛。

如果模拟的批次只包含一个实例的话，就是SGD（Stochastic Gradient Descent），或者叫做在线梯度下降。

小批量的大小是一个超参数，但交叉验证并不常见。一般设置为2的倍数。

本节的核心内容是，计算损失函数梯度与其权重（并对其具有一些直观理解）的能力是设计，训练和理解神经网络所需的最重要技能。在下一节中，我们将开发使用链规则分析计算梯度的熟练程度，否则也称为反向传播。这将使我们能够有效地优化表达各种神经网络的相对任意的损失函数，包括卷积神经网络。

## 卷积神经网络

动机：从机理层面理解反向传播。链式法则。
如何给了输入$$x$$和函数$$f(x)$$，求得$$\nabla f(x)$$呢。

L表示函数f，x和W表示自变量。
每个变量的导数告诉您整个表达式对其值的敏感性。

反向传播是一个精美的本地过程。电路图中的每个门都有一些输入，可以立即计算两件事：1。输出值和2.输入相对于输出值的局部梯度。

加法，乘法，max

sigmoid：
$$f(w, x)=\frac{1}{1+e^{-\left(w_{0} x_{0}+w_{1} x_{1}+w_{2}\right)}}$$

每个元素的求导规则像这样：

$$\begin{array}{cc}{f(x)=\frac{1}{x}} & {\rightarrow} & {\frac{d f}{d x}=-1 / x^{2}} \\ {f_{c}(x)=c+x} & {\rightarrow} & {\frac{d f}{d x}=1} \\ {f(x)=e^{x}} & {\rightarrow} & {\frac{d f}{d x}=e^{x}} \\ {f_{a}(x)=a x} & {\rightarrow} & {\frac{d f}{d x}=a}\end{array}$$

sigmoid可以抽象成另一个函数：

$$\sigma(x)=\frac{1}{1+e^{-x}}$$

$$\rightarrow \quad \frac{d \sigma(x)}{d x}=\frac{e^{-x}}{\left(1+e^{-x}\right)^{2}}=\left(\frac{1+e^{-x}-1}{1+e^{-x}}\right)\left(\frac{1}{1+e^{-x}}\right)=(1-\sigma(x)) \sigma(x)$$

### 想一个根本没有任何意义的函数

$$f(x, y)=\frac{x+\sigma(y)}{\sigma(x)+(x+y)^{2}}$$

这个函数如何求以及如何求导？

{% codeblock lang:python %}
x = 3 # example values
y = -4

# forward pass
sigy = 1.0 / (1 + math.exp(-y)) # sigmoid in numerator   #(1)
num = x + sigy # numerator                               #(2)
sigx = 1.0 / (1 + math.exp(-x)) # sigmoid in denominator #(3)
xpy = x + y                                              #(4)
xpysqr = xpy**2                                          #(5)
den = sigx + xpysqr # denominator                        #(6)
invden = 1.0 / den                                       #(7)
f = num * invden # done!                                 #(8)
{% endcodeblock %}

中间的这些变量要保留到cache里面。
(sigy, num, sigx, xpy, xpysqr, den, invden)

然后就是求导吧：
{% codeblock lang:python %}
# backprop f = num * invden
dnum = invden # gradient on numerator                             #(8)
dinvden = num                                                     #(8)
# backprop invden = 1.0 / den 
dden = (-1.0 / (den**2)) * dinvden                                #(7)
# backprop den = sigx + xpysqr
dsigx = (1) * dden                                                #(6)
dxpysqr = (1) * dden                                              #(6)
# backprop xpysqr = xpy**2
dxpy = (2 * xpy) * dxpysqr                                        #(5)
# backprop xpy = x + y
dx = (1) * dxpy                                                   #(4)
dy = (1) * dxpy                                                   #(4)
# backprop sigx = 1.0 / (1 + math.exp(-x))
dx += ((1 - sigx) * sigx) * dsigx # Notice += !! See notes below  #(3)
# backprop num = x + sigy
dx += (1) * dnum                                                  #(2)
dsigy = (1) * dnum                                                #(2)
# backprop sigy = 1.0 / (1 + math.exp(-y))
dy += ((1 - sigy) * sigy) * dsigy                                 #(1)
# done! phew
{% endcodeblock %}

注意两点事情：
1. 把所有前向的东西都保留下来。
2. 把所有分支的梯度最后都加起来。

加门：反向递推的时候，均等分配每个分支。路由器
max门：只允许一个分支通过，
乘门：另一个分支的值乘以当前分支的梯度。

乘法可能不太直观，如果乘法门的一个输入非常小而另一路非常大的话，他会为小的输入分配一个非常大的梯度，而大输入分配一个小的梯度。数据的大小对权重有很大的影响，如果对所有输入数据乘以1000的话，权重的梯度就会增加1000倍，必须降低学习率来使得效果得到补偿。这就是为什么预处理很重要，有时是微妙的方式！ 了解渐变流程的直观理解可以帮助您调试其中的一些情况。

### 向量操作：

{% codeblock lang:python %}
# forward pass
W = np.random.randn(5, 10)
X = np.random.randn(10, 3)
D = W.dot(X)

# now suppose we had the gradient on D from above in the circuit
dD = np.random.randn(*D.shape) # same shape as D
dW = dD.dot(X.T) #.T gives the transpose of the matrix
dX = W.T.dot(dD)
{% endcodeblock %}

这里也讲了从矩阵维数来推断矩阵乘法的形式。但是脑容量太小了qwq

甩给一个[参考文献](http://cs231n.stanford.edu/vecDerivs.pdf)

将表达式分解为阶段，以便您可以独立区分每个阶段（阶段将是矩阵向量乘法，或最大运算，或求和运算等），然后一步一步地通过变量进行反向提取。

### 单神经元作为分类器：

可以用二元softmax和二元svm。

激活函数： ReLU, sigmoid, tanh。

sigmoid：表达式： $$\sigma(x)=\frac{1}{(1+e^{-x}})$$
缺点：
1. 容易饱和，饱和的区间没有梯度
2. 不是零均值的

tanh： $$\tan h(x)=2\sigma(2x)−1$$

ReLU：收敛速度是tanh的6倍。
优点：
1. 收敛速度块，非线性更强，并且没有饱和
2. 计算简单，不用计算指数。

缺点：
1. 非常脆弱容易死。例如，流过ReLU神经元的大梯度可能导致权重更新，使得神经元永远不会再次激活任何数据点。如果发生这种情况，那么流经该单元的梯度将从该点开始永远为零。也就是说，ReLU单元可以在训练期间不可逆转地死亡，因为它们可以从数据流形中被淘汰。例如，如果学习率设置得太高，您可能会发现多达40％的网络可能“死”（即永远不会在整个训练数据集中激活的神经元）。通过适当设置学习率通常可以解决这个问题。

Leaky ReLU：
1. 为了解决容易死这个问题。在负梯度加了0.01的权重。这个在有的模型上很好但是有的模型不行qwq。

Maxout：
1. ReLU和Leaky ReLU都是此形式的特例（例如，对于ReLU，我们有w1，b1 = 0
）。因此，Maxout神经元享有ReLU单元的所有优点（线性操作方式，无饱和），并且没有其缺点（死于ReLU）。然而，与ReLU神经元不同，它使每个神经元的参数数量加倍，导致参数总数高。

TLDR：我应该使用什么类型的神经元？

使用ReLU非线性，小心你的学习率，并可能监控网络中“死”单位的比例。
如果效果不好，请尝试Leaky ReLU或Maxout。
切勿使用sigmoid。 尝试tanh，但期望它比ReLU / Maxout更糟糕。

### 神经网络结构：

全连接层，层间没有连接，层与层之间全连接
输出层通常没有激活函数，这是因为输出层经常用来表示得分（在分类问题当中），或者一个实数目标（在回归问题当中）
神经网络的大小：神经网络层的数量，以及每层单元的数量。计算有多少要学习的神经元。
计算公式：前一层神经元数量*下一层神经元数量 + 下一层神经元数量。

现代卷积神经网络一般有1亿个参数qwq

### 前馈计算：

正向传递就是乘法和激活函数的应用。完全连接层的正向通过对应于一个矩阵乘法，其后是偏置偏移和激活函数。

表示能力：

神经网络可以逼近任意连续函数。
如果一个隐藏层足以逼近任何函数，为什么要使用更多层并更深入？ 
双层神经网络是一种通用逼近器的事实，虽然在数学上很可爱，但在实践中却是一种相对较弱且无用的陈述。
同样，更深层网络的事实（ 尽管事实上它们的代表能力是相等的，但是单个隐藏层网络可以比单个隐藏层更好地工作。
另外，在实践中通常情况下，3层神经网络将胜过2层网络，但更深层次（4,5,6层）很少有助于更多。
这个性质和卷积神经网络相反。

如何选择隐含层数量和每层的神经元数量？

神经元数量越多越有可能出现过拟合。
如果数据不够复杂以防止过度拟合，似乎可以优先选择较小的神经网络。然而，这是不正确的。
这背后的微妙原因是较小的网络难以使用诸如梯度下降之类的局部方法进行训练：很明显，它们的损失函数具有相对较少的局部最小值，但事实证明，这些最小值中的许多更易于收敛，并且他们很糟糕（即损失很大）。相反，较大的神经网络包含明显更多的局部最小值，但这些最小值在实际损失方面变得更好。
调整正则化强度是控制神经网络过拟合的首选方法。

### 总结

1. 生物神经元
2. 激活函数
3. 全连接层
4. 神经网络是万能的函数近似器
5. 数量越多，神经网络的效果肯定是越好的。但是随着模型越复杂，越需要正则化强度来进行调整。

### 初始化数据和模型

数据预处理：减去均值，除以标准差。
对数据来说，减去一行的均值
{% codeblock lang:python %}
X -= np.mean(X, axis = 0)
{% endcodeblock %}
对图像来说，减去整幅图的均值
{% codeblock lang:python %}
X -= np.mean(X)
{% endcodeblock %}

除以标准差：
{% codeblock lang:python %}
X /= np.std(X, axis = 0)
{% endcodeblock %}

### PCA和白化

首先预处理：零均值化，得到协方差矩阵。
{% codeblock lang:python %}
# Assume input data matrix X of size [N x D]
X -= np.mean(X, axis = 0) # zero-center the data (important)
cov = np.dot(X.T, X) / X.shape[0] # get the data covariance matrix
{% endcodeblock %}

协方差矩阵是正定然后对称的。协方差矩阵的对角线上的元素就是方差。然后SVD分解

{% codeblock lang:python %}
U,S,V = np.linalg.svd(cov)
{% endcodeblock %}

U是特征向量，S是每个特征向量对应的特征值，为了去相关，把原始向量投影到特征向量上面去。
{% codeblock lang:python %}
Xrot = np.dot(X, U) # decorrelate the data
{% endcodeblock %}

U是正交向量，所以可以视为一组基。np.linalg.svd这个函数有一个很好的性质，也就是这些特征向量都是按照特征值排好序的，所以只需要降维就可以了。比如下面就是取前100项。
{% codeblock lang:python %}
Xrot_reduced = np.dot(X, U[:,:100]) # Xrot_reduced becomes [N x 100]
{% endcodeblock %}

这样我们就把原来的[N,D]的向量变成了[N,100]的向量。
最后是白化，把每一维的数据都除以对应项的特征值。该变换的几何解释是，如果输入数据是多变量高斯分布，则白化数据将是具有零均值和单位协方差矩阵的高斯分布。 此步骤采用以下形式：
{% codeblock lang:python %}
# whiten the data:
# divide by the eigenvalues (which are square roots of the singular values)
Xwhite = Xrot / np.sqrt(S + 1e-5)
{% endcodeblock %}

警告：夸大噪音。 请注意，我们添加1e-5（或小常数）以防止除零。 这种转换的一个弱点是它可以极大地夸大数据中的噪声，因为它将所有维度（包括主要是噪声的微小方差的无关维度）拉伸到输入中的相同大小。 实际上，这可以通过更强的平滑（即，将1e-5增加为更大的数量）来减轻。

我们还可以尝试使用CIFAR-10图像可视化这些转换。 CIFAR-10的训练集大小为50,000 x 3072，其中每个图像都被拉伸成3072维行向量。 然后我们可以计算[3072 x 3072]协方差矩阵并计算其SVD分解（这可能相对昂贵）。 计算出的特征向量在视觉上是什么样的？ 图像可能会有所帮助：

pca也能找到图像的一些特征吧。

在实践中。 我们在这些注释中提到了PCA / Whitening的完整性，但这些转换不适用于Convolutional Networks。 但是，将数据置零中心非常重要，并且通常也会看到每个像素的归一化。

常见的陷阱。 关于预处理的重要一点是，任何预处理统计（例如数据均值）必须仅在训练数据上计算，然后应用于验证/测试数据。 例如。 计算平均值并从整个数据集中的每个图像中减去它，然后将数据拆分为train / val / test splits将是一个错误。 相反，必须仅在训练数据上计算平均值，然后从所有分组（训练/值/测试）中平均减去平均值。

### 权重怎么初始化？

1. 全0：很不好，因为每个权重都会得到相同的结果。如果神经元的权重被初始化为相同，则神经元之间不存在不对称的来源。
2. 小随机数。因此，我们仍然希望权重非常接近于零，但正如我们上面所论述的那样，权重不是相同的零。作为一种解决方案，通常将神经元的权重初始化为较小的数量，并将其称为对称性破坏。
  较小的数字不一定能更好地运作。例如，在反向传播期间具有非常小权重的神经网络层将在其数据上计算非常小的梯度（因为该梯度与权重的值成比例）。这可以大大减少通过网络向后流动的“梯度信号”，并且可能成为深度网络的关注点。
3. 随机初始化神经元的输出的分布具有随输入数量增长的方差。事实证明，我们可以通过将其权重向量乘以其扇入的平方根（即其输入数）来将每个神经元输出的方差归一化为1。也就是说，推荐的启发式方法是将每个神经元的权重向量初始化为：w = np.random.randn（n）/ sqrt（n），其中n是其输入的数量。这确保了网络中的所有神经元最初具有大致相同的输出分布并且凭经验提高了收敛速度。

$$\begin{aligned} \operatorname{Var}(s) &=\operatorname{Var}\left(\sum_{i}^{n} w_{i} x_{i}\right) \\ &=\sum_{i}^{n} \operatorname{Var}\left(w_{i} x_{i}\right) \\ &=\sum_{i}^{n}\left[E\left(w_{i}\right)\right]^{2} \operatorname{Var}\left(x_{i}\right)+E\left[\left(x_{i}\right)\right]^{2} \operatorname{Var}\left(w_{i}\right)+\operatorname{Var}\left(x_{i}\right) \operatorname{Var}\left(w_{i}\right) \\ &=\sum_{i}^{n} \operatorname{Var}\left(x_{i}\right) \operatorname{Var}\left(x_{i}\right)+E\left[\left(x_{i}\right)\right]^{2} \operatorname{Var}\left(w_{i}\right)+\operatorname{Var}\left(x_{i}\right) \operatorname{Var}\left(w_{i}\right) \\ &=(n \operatorname{Var}(w)) \operatorname{Var}\left(x_{i}\right) ] \end{aligned}$$

方差的性质，因为已经零均值化了，所以$$E\left[x_{i}\right]=E\left[w_{i}\right]=0$$。所以，
{% codeblock lang:python %}
w = np.random.randn(n) / sqrt(n)
{% endcodeblock %}
ps：时刻记住方差和均值的公式qwq。

又有一个人推荐初始化的值为：$$\operatorname{Var}(w)=2 /\left(n_{i n}+n_{o u t}\right)$$，也就是说应该是这么初始化的：
{% codeblock lang:python %}
 w = np.random.randn(n) * sqrt(2.0/n)
{% endcodeblock %}

稀疏初始化。 解决未校准方差问题的另一种方法是将所有权重矩阵设置为零，但是为了打破对称性，每个神经元随机连接（从如上所述的小高斯采样的权重）到其下方的固定数量的神经元。 连接的典型神经元数量可能小到10。

初始化b。 将偏差初始化为零是可能和常见的，因为不对称性破坏由权重中的小随机数提供。 对于ReLU非线性，有些人喜欢对所有偏差使用小的常数值，例如0.01，因为这可以确保所有ReLU单元在开始时触发，从而获得并传播一些梯度。 但是，目前尚不清楚这是否能提供一致的改进（实际上有些结果似乎表明这种情况更糟）并且更常见的是简单地使用0偏置初始化。

实际上我们操作的方法还是：
{% codeblock lang:python %}
 w = np.random.randn(n) * sqrt(2.0/n)
{% endcodeblock %}

### batchnorm

最近由Ioffe和Szegedy开发的称为批量标准化的技术通过明确强制整个网络中的激活在训练开始时采用单位高斯分布来适当地初始化神经网络，从而缓解了许多令人头痛的问题。核心观察是这是可能的，因为归一化是一种简单的可微分运算。在实现中，应用此技术通常相当于在完全连接的层（或卷积层，我们很快就会看到）之后以及非线性之前立即插入BatchNorm层。我们在这里没有扩展这种技术，因为它在链接文章中有很好的描述，但请注意，在神经网络中使用批量标准化已经成为一种非常普遍的做法。实际上，使用批量标准化的网络对于错误的初始化更加健壮。另外，批量标准化可以被解释为在网络的每个层进行预处理，但是以可区分的方式集成到网络本身中。整齐！

### 正则化

#### L2 正则化

每个W是这么下降的：
{% codeblock lang:python %}
W += -lambda * W
{% endcodeblock %}

#### L1正则化

L1正则化具有引人注目的特性，即它导致权重向量在优化期间变得稀疏（即非常接近于零）。 换句话说，具有L1正则化的神经元最终仅使用其最重要输入的稀疏子集并且变得几乎不变于“噪声”输入。 相比之下，来自L2正则化的最终权重向量通常是漫反射的，小数目。 实际上，如果您不关心明确的特征选择，可以期望L2正则化比L1具有更好的性能。

#### L$$\inf$$正则化，最大范数正则化

其吸引人的特性之一是，即使学习率设置得太高，网络也不会“爆炸”，因为更新总是有限的。

#### dropout层

最牛逼的层了，实现起来一看就懂：下面是不推荐的版本注意一下

{% codeblock lang:python %}
""" Vanilla Dropout: Not recommended implementation (see notes below) """

p = 0.5 # probability of keeping a unit active. higher = less dropout

def train_step(X):
  """ X contains the data """
  
  # forward pass for example 3-layer neural network
  H1 = np.maximum(0, np.dot(W1, X) + b1)
  U1 = np.random.rand(*H1.shape) < p # first dropout mask
  H1 *= U1 # drop!
  H2 = np.maximum(0, np.dot(W2, H1) + b2)
  U2 = np.random.rand(*H2.shape) < p # second dropout mask
  H2 *= U2 # drop!
  out = np.dot(W3, H2) + b3
  
  # backward pass: compute gradients... (not shown)
  # perform parameter update... (not shown)
  
def predict(X):
  # ensembled forward pass
  H1 = np.maximum(0, np.dot(W1, X) + b1) * p # NOTE: scale the activations
  H2 = np.maximum(0, np.dot(W2, H1) + b2) * p # NOTE: scale the activations
  out = np.dot(W3, H2) + b3
{% endcodeblock %}

这个是改进之后的版本，就把原来的乘号，后面加了一个除号而已：

这么考虑的原因是：训练的时候我们可以激活或者抑制这个神经元，也就是概率是`px+(1-p)0`，但是在测试阶段我们肯定是把这些神经元全部激活的，所以要把测试集合当中的元素还要映射成`x-> px`，但是如果采用下面的方案的话，我们就不用这么做了qwq

{% codeblock lang:python %}
""" 
Inverted Dropout: Recommended implementation example.
We drop and scale at train time and don't do anything at test time.
"""

p = 0.5 # probability of keeping a unit active. higher = less dropout

def train_step(X):
  # forward pass for example 3-layer neural network
  H1 = np.maximum(0, np.dot(W1, X) + b1)
  U1 = (np.random.rand(*H1.shape) < p) / p # first dropout mask. Notice /p!
  H1 *= U1 # drop!
  H2 = np.maximum(0, np.dot(W2, H1) + b2)
  U2 = (np.random.rand(*H2.shape) < p) / p # second dropout mask. Notice /p!
  H2 *= U2 # drop!
  out = np.dot(W3, H2) + b3
  
  # backward pass: compute gradients... (not shown)
  # perform parameter update... (not shown)
  
def predict(X):
  # ensembled forward pass
  H1 = np.maximum(0, np.dot(W1, X) + b1) # no scaling necessary
  H2 = np.maximum(0, np.dot(W2, H1) + b2)
  out = np.dot(W3, H2) + b3
{% endcodeblock %}

###  前向传播中的噪声问题

使用dropout，在前向传递的时候引入随机行为，测试期间，噪声被p边缘化掉了（使用推荐方案）。或者通过数值：通过随机采样之后平均……好吧这段没看懂qwq

一般不对偏差进行正则化，但是加了也不是不可以hhhhh

每层正则化。 将不同的层规则化为不同的量（除了输出层之外）并不常见。 关于这个想法的相对较少的结果已经在文献中公布。

在实践中：最常见的是使用交叉验证的单一全局L2正则化强度。 将此与在所有层之后应用的丢失相结合也是常见的。 p = 0.5的值是合理的默认值，但可以在验证数据上进行调整。

### 损失函数

我们之前是讨论了正则项损失，现在我们开始看数据损失了，也就是

$$L=\frac{1}{N} \sum_{i} L_{i}$$

我们到目前为止已经详细讨论过分类。 在这里，我们假设每个示例的示例数据集和单个正确的标签（在固定集合之外）。 在这种情况下，两种最常见的成本函数之一是SVM（例如Weston Watkins公式），使用hinge loss：

$$L_{i}=\sum_{j \neq y_{i}} \max \left(0, f_{j}-f_{y_{i}}+1\right)$$

有的人说使用平方会好一点。

另一个是softmax，使用交叉熵：
$$L_{i}=-\log \left(\frac{e^{f_{y_{i j}}}}{\sum_{j} e^{f_{j}}}\right)$$

但是会遇到问题：分类的数量太多了怎么办？解决办法之一是使用层次softmax，先把结果分成大类，然后逐渐分成小类酱。构建成一棵树，每个标签都可以变成一条路径。🌲的结构一般是根据问题不同而不同。

### 特征分类器

上面的情况都是针对$$y_i$$有具体值的情况，但是如果$$y_i$$是一个向量表示会怎么办呢？$$y_i$$的结果可能不是只有一个。比如一张图当中可能既有🐱又有🌲。在这种情况下，一种明智的方法是独立地`为每个单独的属性构建二元分类器`。 例如，每个类别的二元分类器将独立采用以下形式：

$$L_{i}=\sum_{j} \max \left(0,1-y_{i j} f_{j}\right)$$

这个损失函数，累计正的小于1的部分，也累计错误的大于-1的部分。

同样也可以使用交叉熵来干这个事情：

$$P(y=1 \vert x ; w, b)=\frac{1}{1+e^{-\left(w^{T} x+b\right)}}=\sigma\left(w^{T} x+b\right)$$

因为1和0加起来的概率应该是1，所以y=0的概率为$$1- P(y=1 \vert x ; w, b)$$。

这个损失可以用这个形式来表示，推的话就是概率乘以所对应的值：

$$L_{i}=\sum_{j} y_{i j} \log \left(\sigma\left(f_{j}\right)\right)+\left(1-y_{i j}\right) \log \left(1-\sigma\left(f_{j}\right)\right)$$

### 回归任务

比如说是预测房价，或者图片当中的某些信息。所以这个损失就可以被定义为和真值的L1距离或者L2距离。

比如L2距离：

$$L_{i}=\left\vertf-y_{i}\right\vert_{2}^{2}$$

L1距离就是吧每个轴偏差的绝对值加起来。

L_{i}=\left\vertf-y_{i}\right\vert_{1}=\sum_{j}\left\vertf_{j}-\left(y_{i}\right)_{j}\right\vert

分数的梯度就是和偏差直接成正比（L2)，或者就直接是偏差的符号（L1）

L2损失比Softmax等更稳定的损耗更难以优化。

此外，L2损失不太稳健，异常值会引入巨大的梯度。

当遇到回归问题时，首先考虑是否绝对不足以将输出量化为二进制数。例如，如果您正在预测某个产品的星级评分，那么使用5个独立分类器对1-5个星级的评级而不是回归损失可能会好得多。

分类具有额外的好处，它可以为您提供回归输出的分布，而不仅仅是单个输出而不表示其置信度。如果您确定分类不合适，请使用L2但要小心：例如，L2更脆弱，并且在网络中应用丢失（特别是在L2丢失之前的层中）并不是一个好主意。

面对回归任务时，首先要考虑是否绝对必要。相反，我们强烈倾向于将您的产出离散化为桶，并尽可能对其进行分类。

结构化预测？不懂啥意思。

### 总结：

1. 将数据居中置为0
2. 其比例标准化为每个特征的[-1,1]
3. 初始化权重用这个来初始化：`w = np.random.randn(n) * sqrt(2.0/n`
4. 使用L2正则化，使用dropout（那个inverse版本的，除以p的）
5. 使用batch norm
6. 讨论了多种输出的结果。分类问题，还是回归问题，还是结构化问题。

这些都是预处理的内容，下面会介绍怎么运行整个网络。

## 学习和评估过程

### 检查梯度到底对不对

使用中心的这个函数
$$\frac{d f(x)}{d x}=\frac{f(x+h)-f(x-h)}{2 h} \text { (use instead) }$$

而不是这个函数
$$\frac{d f(x)}{d x}=\frac{f(x+h)-f(x)}{h}(\text { bad, do not use })$$

因为把上面两个式子泰勒展开，就可以得到，第一个是指的误差是O(h^2)，第二个是O(h)，第一个的O(h)项被消掉了。

### 比较数值方法和解析方法计算出的梯度：

$$\frac{\left\vertf_{a}^{\prime}-f_{n}^{\prime}\right\vert}{\max \left(\left\vertf_{a}^{\prime}\right\vert,\left\vertf_{n}^{\prime}\right\vert\right)}$$

一般情况下有如下经验：

1. 如果梯度>1e-2，那肯定就是算错了
2. 1e-2到1e-4之间，那可能是算错了
3. 如果有扭结kinks的话，小于1e-4就应该差不多了，否则应该是算错了(比如计算tanh这种)。kink
4. 小于1e-7应该是算对了

如果网络越深的话，相对误差肯定是越大才对。因此，如果您对10层网络的输入数据进行梯度检查，则1e-2的相对误差可能是正常的，因为错误会在路上积累。 相反，单个可微函数的1e-2误差可能表示不正确的梯度。

使用双精度：常见的缺陷是使用单精度浮点来计算梯度检查。 通常情况下，即使使用正确的梯度实现，您也可能会遇到较高的相对误差（高达1e-2）。 根据我的经验，我有时会看到我的相对误差从1e-2下降到1e-8，切换到双精度。

坚持活跃的浮点范围。 阅读“每个计算机科学家应该知道的关于浮点算术的内容”是一个好主意，因为它可以揭开您的错误的神秘面纱并使您能够编写更仔细的代码。 例如，在神经网络中，通常可以对批次上的损失函数进行标准化。 但是，如果每个数据点的渐变非常小，那么另外将它们除以数据点的数量开始给出非常小的数字，这反过来将导致更多的数值问题。 这就是为什么我喜欢`始终打印原始数值/分析梯度`，并确保您`比较的数字不是非常小`（例如，绝对值大约为1e-10且更小是令人担忧的）。 如果它们是你可能想要暂时将你的损失函数增加一个常数，以使它们达到浮点数更密集的“更好”范围 - 理想情况是大约为1.0，其浮点指数为0。

kink项：比如在求导的函数当中，存在导数不连续的项的话，比如max(0,x),或者SVM loss的话，比如在-1e-6的地方求导，这个导数为0。但是这个数值梯度就有可能不是0。这个可能从直观上来看很少会发生，但是cifar10数据集当中 ，有450 000个max(0,x)，因为对每个样本都有9个max(0,x)，这个发生的可能性就很大了。

但是这个kink是能够被预见的。这可以通过在形式max（x，y）的函数中跟踪所有“获胜者”的身份来完成; 也就是说，在前进过程中x或y更高。 如果在评估f（x + h）然后评估f（x-h）时至少一个获胜者的身份发生变化，则交叉扭结并且数值梯度将不准确。

仅使用少量数据点。 针对上述扭结问题的一个解决方法是使用更少的数据点，因为包含扭结的损失函数（ReLU或margin损失等）将具有更少的扭结和更少的数据点，因此您不太可能跨越。

当你执行有限的不同近似。 此外，如果您的梯度检查只有2或3个数据点，那么您几乎肯定会对整批进行梯度检查。 使用极少的数据点也可以使您的渐变检查更快更有效。

注意这个h的选择，较小的情况并不一定好，因为当h小得多时，您可能会开始遇到数值精度问题。 有时，当梯度未检查时，您可能将h更改为1e-4或1e-6，并且突然梯度将是正确的。 这篇维基百科文章包含一个图表，该图表绘制了x轴上的h值和y轴上的数值梯度误差。

重要的是要意识到在参数空间中的特定（并且通常是随机的）单点处执行梯度检查。即使在此时梯度检查成功，也不能立即确定梯度是否在全局范围内正确实现。

随机初始化可能不是参数空间中最“特征”的点，并且实际上可能引入梯度似乎正确实现但不是正确实现的病理情况。

例如，具有非常小的权重初始化的SVM将为所有数据点分配几乎完全零的分数，并且梯度将在所有数据点上呈现特定模式。梯度的不正确实现仍然可以产生这种模式，而不是推广到更具特色的操作模式，其中一些分数大于其他分数。

因此，为了安全起见，最好使用较短的老化时间，在此期间允许网络学习并在损失开始下降后执行梯度检查。在第一次迭代中执行它的危险在于，这可能引入病态边缘情况并掩盖梯度的不正确实现。

不要让正则项压倒数据。通常情况是，损失函数是数据丢失和正则化损失的总和（例如，权重上的L2惩罚）。需要注意的一个危险是`正则化损失可能会压倒数据丢失`，在这种情况下，梯度将主要来自正则化项（通常具有更简单的梯度表达式）。这可以掩盖数据丢失梯度的错误实现。因此，建议`先关闭正则化并单独检查数据丢失，然后再将正则化项独立检测`。执行后者的一种方法是破解代码以消除数据丢失贡献。另一种方法是增加正则化强度，以确保其在梯度检查中的效果不可忽略，并且可以发现不正确的实现。

请记得关闭dropout/argmentation。在执行梯度检查时，请记住关闭网络中的任何非确定性效果，例如丢失，随机数据增强等。否则，在估计数值梯度时，这些可能会明显引入巨大错误。关闭这些效果的缺点是你不会对它们进行梯度检查（例如，可能是丢失没有正确反向传播）。因此，更好的解决方案可能是在评估和评估分析梯度之前强制使用特定的随机种子。

只检查几个尺寸。在实践中，梯度可以具有百万个参数的大小。在这些情况下，检查梯度的某些尺寸并假设其他尺寸是正确的是切实可行的。注意：要注意的一个问题是确保对每个单独的参数进行渐变检查。在一些应用中，为方便起见，人们将参数组合成单个大参数矢量。例如，在这些情况下，偏差只能从整个矢量中获取少量参数，因此不要随意采样，但要考虑到这一点并检查所有参数是否接收到正确的梯度。

### 在开始训练之前，sanity check

在您投入昂贵的优化之前，您可能会考虑进行一些健全性检查：

寻找正确的损失性能。使用小参数初始化时，请确保获得预期的损失。最好先检查数据丢失（因此将正则化强度设置为零）。例如，对于具有Softmax分类器的CIFAR-10，我们预计初始损失为`2.302`，因为我们期望每个类的扩散概率为0.1（因为有10个类），而Softmax损失是负的对数概率。正确的类所以：-ln（0.1）= 2.302。对于SVM，我们期望所有期望的边际都被违反（因为所有分数大约为零），因此预期损失`9`（因为每个错误的等级的边际为1）。如果您没有看到这些损失，则可能存在初始化问题。

作为第二次理智检查，`增加正规化强度应该增加损失`。过度填充一小部分数据。最后也是最重要的是，在对完整数据集进行培训之前，尝试对您数据的一小部分（例如20个示例）进行培训，并确保您可以实现零成本。对于此实验，最好将正则化设置为零，否则这可能会阻止您获得零成本。除非您使用小数据集通过此完整性检查，否则不值得继续使用完整数据集。请注意，您可能会遇到非常小的数据集，但仍然有不正确的实现。例如，如果您的数据点的功能由于某些错误而是随机的，那么您可能会过度填充您的小型训练集，但是当您将完整数据集折叠时，您将永远不会注意到任何概括。

### 还是在开始训练之前，学习率的选择

损失的形状和学习率的关系：

左图：描绘不同学习率影响的漫画。 学习率低时，改进将是线性的。 
随着高学习率，他们将开始看起来更具指数性。 
较高的学习率会更快地减少损失，但是它们会陷入更糟糕的损失值（绿线）。
这是因为优化中存在太多的“能量”，参数在混乱中反弹，无法在优化环境中找到一个好位置。 
右图：在CIFAR-10数据集上训练小型网络时，典型损失函数随时间变化的示例。 
这个损失函数看起来合理（它可能表示根据其衰减速度略微过小的学习率，但很难说），并且还表明批量大小可能有点太低（因为成本有点太高） 吵）。

损失中的“摆动”量与批量大小有关。 当批量大小为1时，摆动将相对较高。 当批量大小是完整数据集时，摆动将是最小的，因为每个梯度更新应该单调地改善损失函数（除非学习率设置得太高）。

有些人喜欢在日志域中绘制他们的损失函数。 由于学习进度通常采用指数形式，因此情节显示为稍微可解释的直线，而不是曲棍球棒。 此外，如果在同一损失图上绘制了多个交叉验证模型，则它们之间的差异将变得更加明显。

### 训练，验证精度

训练和验证准确度之间的差距表明过度拟合的数量。 左侧图中显示了两种可能的情况。 与训练精度相比，蓝色验证误差曲线显示非常小的验证精度，表明强过度拟合（注意，验证精度甚至可能在某个点之后开始下降）。 当你在实践中看到这一点时，你可能想要增加正则化（更强的L2权重惩罚，更多的辍学等）或收集更多数据。 另一种可能的情况是验证准确性相当好地跟踪训练准确性。 这种情况表明您的模型容量不够高：通过增加参数数量使模型更大。

### 权重比：更新

您可能想要跟踪的最后一个数量是更新幅度与值幅度的比率。 注意：更新，而不是原始渐变（例如，在vanilla sgd中，这将是梯度乘以学习速率）。 您可能希望独立地为每组参数评估和跟踪此比率。 粗略的启发式是这个比例应该在1e-3左右。 如果低于此值，那么学习率可能太低。 如果它更高那么学习率可能太高。 这是一个具体的例子：

{% codeblock lang:python %}
# assume parameter vector W and its gradient vector dW
param_scale = np.linalg.norm(W.ravel())
update = -learning_rate*dW # simple SGD update
update_scale = np.linalg.norm(update.ravel())
W += update # the actual update
print update_scale / param_scale # want ~1e-3
{% endcodeblock %}

更喜欢去跟踪权重的norm值，而不是最大最小值。

### 激活函数/每层的梯度分配。

不正确的初始化可能会减慢甚至完全停止学习过程。 幸运的是，这个问题可以相对容易地诊断出来。 一种方法是为网络的所有层绘制激活/梯度直方图。 直观地说，看到任何奇怪的分布并不是一个好兆头 - 例如 对于tanh神经元，我们希望在[-1,1]的整个范围内看到神经元激活的分布，而不是看到所有神经元输出为零，或者所有神经元在-1或1处完全饱和。

对图像的操作，我们希望能够看到中间的处理结果是怎么样的。

神经网络的第一层的可视化权重的示例。 左：`嘈杂的功能表明可能是一个症状：未融合的网络，不正确的学习率，非常低的权重正则化惩罚`。 右图：`良好，流畅，清洁和多样化的特征是培训进展顺利的良好迹象`。

### 参数更新方法

最简单的方法：Vanilla update

{% codeblock lang:python %}
x += - learning_rate * dx
{% endcodeblock %}

学习率是一个超参数，是一个固定值。

动量更新：几乎在所有的网络有更好的性能。可以从优化问题的物理角度来激发该更新。特别是，损失可以解释为丘陵地形的高度。使用随机数初始化参数相当于在某个位置设置初始速度为零的粒子。然后可以将优化过程视为等效于模拟参数矢量（即粒子）在景观上滚动的过程。 所以（负）梯度在这个视图中与粒子的加速度成比例。请注意，这与上面显示的SGD更新不同，其中渐变直接整合了位置。相反，物理视图建议更新，其中渐变仅直接影响速度，而速度反过来影响位置：

{% codeblock lang:python %}
# Momentum update
v = mu * v - learning_rate * dx # integrate velocity
x += v # integrate position
{% endcodeblock %}

在这里，我们看到一个初始化为零的v变量的引入，以及一个额外的超参数（mu）。 作为一个令人遗憾的误称，这个变量在优化中被称为动量（其典型值约为0.9），但其物理意义与摩擦系数更为一致。 实际上，这个变量会抑制速度并降低系统的动能，否则粒子永远不会停在山脚下。 交叉验证时，此参数通常设置为[0.5,0.9,0.95,0.99]等值。 与学习率的退火计划类似（稍后讨论），优化有时可以从动量计划中获益，其中动量在学习的后期阶段增加。 典型的设置是以大约0.5的动量开始并且在多个时期将其退火到0.99左右。

通过Momentum更新，参数向量将在具有一致梯度的任何方向上建立速度。就是相当于加了一个积分吧qwq

### Nesterov Momentum

是与动量更新略有不同的版本，最近越来越受欢迎。 它对凸函数具有更强的理论收敛保证，在实践中它也比标准动量稍微好一些。

Nesterov动量背后的核心思想是当当前参数向量位于某个位置x时，然后查看上面的动量更新，我们知道单独的动量项（即忽略带梯度的第二项）即将推动参数 矢量by mu * v。因此，如果我们要计算梯度，我们可以将未来的近似位置x + mu * v视为“前瞻” - 这是我们即将结束的地方附近的一个点起来。 因此，计算x + mu * v处的梯度而不是“旧/陈旧”位置x是有意义的。

也就是使用的数据是提前版本，不是计算当前时刻，而是计算下一时刻。

{% codeblock lang:python %}
x_ahead = x + mu * v
# evaluate dx_ahead (the gradient at x_ahead instead of at x)
# 估计x_ahead的梯度，而不是x的梯度。
v = mu * v - learning_rate * dx_ahead
x += v
{% endcodeblock %}

但是，在实践中，人们更愿意将更新表达为与vanilla SGD或之前的动量更新类似。 这可以通过用变量x_ahead = x + mu * v操纵上面的更新来实现，然后用x_ahead而不是x表示更新。 也就是说，我们`实际存储的参数向量总是提前版本`。 根据x_ahead（但将其重命名为x）的等式变为：

{% codeblock lang:python %}
v_prev = v # back this up
v = mu * v - learning_rate * dx # velocity update stays the same
x += -mu * v_prev + (1 + mu) * v # position update changes form
{% endcodeblock %}

有些参考材料，参考[这里](http://arxiv.org/pdf/1212.0901v2.pdf)和[这里](http://www.cs.utoronto.ca/~ilya/pubs/ilya_sutskever_phd_thesis.pdf)

### 退出学习率

在训练深度网络时，随着时间的推移退出学习率通常是有帮助的。要记住的良好直觉是，在高学习率的情况下，系统包含太多的动能，并且参数矢量在混乱中反弹，无法稳定到损失函数的更深，但更窄的部分。知道何时衰减学习速度可能会很棘手：慢慢腐烂，你会浪费计算在混乱中蹦蹦跳跳，很长一段时间没有什么改进。但是它过于腐蚀并且系统会冷却太快，无法达到最佳位置。实现学习率衰减有三种常见类型：

步骤衰减：每隔几个时期将学习率降低一些因素。典型值可能是每5个时期将学习率降低一半，或者每20个时期降低0.1。这些数字在很大程度上取决于问题的类型和模型。您可能在实践中看到的一种启发式方法是在以固定学习速率训练时观察验证错误，并在验证错误停止改善时将学习速率降低常数（例如0.5）。

指数衰减：$$\alpha=\alpha_{0} e^{-k t}$$

1/t衰减：$$\alpha=\alpha_{0} /(1+k t)$$

在实践中，我们发现步骤衰减稍微有点优选，因为它涉及的超参数（衰减的分数和以时期为单位的步长定时）比超参数k更容易解释。 最后，如果你能负担得起计算预算，那么在衰减较慢的情况下会犯错误并且训练时间较长。

### 二阶方法

更新方法：$$x \leftarrow x-[H f(x)]^{-1} \nabla f(x)$$

直观地说，Hessian描述了损失函数的局部曲率，这使我们能够执行更有效的更新。特别地，乘以逆Hessian导致优化以在浅曲率的方向上采取更积极的步骤并且在陡曲率的方向上采取更短的步骤。请注意，重要的是，更新公式中没有任何学习率超参数，这些方法的支持者认为这是一阶方法的一大优势。

然而，上述更新对于大多数深度学习应用来说是不切实际的，因为以明确的形式计算（和反转）Hessian在空间和时间上都是一个非常昂贵的过程。例如，具有一百万个参数的神经网络将具有大小为[1,000,000 x 1,000,000]的Hessian矩阵，占用大约3725千兆字节的RAM。因此，已经开发了各种各样的准牛顿方法，其寻求逼近逆Hessian。其中，最受欢迎的是L-BFGS，其使用梯度随时间的信息来隐式地形成近似（即，从不计算全矩阵）。

然而，即使我们消除了内存问题，L-BFGS的天真应用的一大缺点是必须在整个训练集上计算，这可能包含数百万个示例。与小批量SGD不同，让L-BFGS在迷你批次上工作更加棘手，也是一个活跃的研究领域。

在实践中，目前通常不会将L-BFGS或类似的二阶方法应用于大规模深度学习和卷积神经网络。相反，基于（Nesterov）动量的SGD变体更为标准，因为它们更简单，更容易扩展。

其他参考：

大规模分布式深度网络是Google Brain团队的一篇论文，在大规模分布式优化中比较L-BFGS和SGD变体。
SFO算法力求将SGD的优势与L-BFGS的优势结合起来。

### 参数自适应学习率方法

到目前为止，我们讨论过的所有先前方法都在全局范围内操纵学习率，并且对所有参数均等。 调整学习率是一个昂贵的过程，因此在设计能够自适应地调整学习速率的方法方面做了大量工作，甚至每个参数都这样做。 这些方法中的许多方法可能仍然需要其他超参数设置，但其论点是它们对于比原始学习速率更广泛的超参数值表现良好。 在本节中，我们将重点介绍您在实践中可能遇到的一些常见自适应方法。

Adagrad：

{% codeblock lang:python %}
# Assume the gradient dx and parameter vector x
cache += dx**2
x += - learning_rate * dx / (np.sqrt(cache) + eps)
{% endcodeblock %}

请注意，变量缓存的大小等于渐变的大小，并跟踪每个参数的平方渐变和。 然后，这用于按元素方式标准化参数更新步骤。 请注意，接收高梯度的权重将降低其有效学习率，而接收较少或不经常更新的权重将使其有效学习率增加。 有趣的是，平方根操作变得非常重要，没有它，算法执行得更糟。 平滑项eps（通常设置在1e-4到1e-8的范围内）避免了除零。 Adagrad的一个缺点是，在深度学习的情况下，单调学习率通常证明过于激进并且过早地停止学习。

RMSprop：

RMSprop是一种非常有效但目前尚未发布的自适应学习速率方法。 有趣的是，每个在他们的工作中使用这种方法的人目前都引用了Geoff Hinton的Coursera课程第6讲的幻灯片29。 RMSProp更新以非常简单的方式调整Adagrad方法，以尝试降低其积极的，单调递减的学习速率。 特别是，它使用平方梯度的移动平均值

{% codeblock lang:python %}
cache = decay_rate * cache + (1 - decay_rate) * dx**2
x += - learning_rate * dx / (np.sqrt(cache) + eps)
{% endcodeblock %}

Adam：

{% codeblock lang:python %}
m = beta1*m + (1-beta1)*dx
v = beta2*v + (1-beta2)*(dx**2)
x += - learning_rate * m / (np.sqrt(v) + eps)
{% endcodeblock %}

请注意，更新看起来与RMSProp更新完全相同，除了使用渐变m的“平滑”版本而不是原始（可能是有噪声的）梯度向量dx。 本文中的推荐值为eps = 1e-8，beta1 = 0.9，beta2 = 0.999。 实际上，Adam目前被推荐作为默认算法使用，并且通常比RMSProp稍好一些。 然而，通常也值得尝试SGD + Nesterov Momentum作为替代方案。 完整的Adam更新还包括偏差校正机制，该机制补偿了这样的事实，即在前几个步骤中，矢量m，v在它们完全“预热”之前都被初始化并因此被偏置为零。 使用偏差校正机制，更新如下：

{% codeblock lang:python %}
# t is your iteration counter going from 1 to infinity
m = beta1*m + (1-beta1)*dx
mt = m / (1-beta1**t)
v = beta2*v + (1-beta2)*(dx**2)
vt = v / (1-beta2**t)
x += - learning_rate * mt / (np.sqrt(vt) + eps)
{% endcodeblock %}

动画可能有助于您对学习过程动态的直觉。 左图：损失曲面的轮廓和不同优化算法的时间演变。 注意基于动量的方法的“过冲”行为，这使得优化看起来像一个滚下山的球。 右图：优化环境中鞍点的可视化，其中沿不同维度的曲率具有不同的符号（一维向上弯曲，另一向向下弯曲）。 请注意，SGD很难打破对称性并陷入困境。 相反，诸如RMSprop之类的算法将在鞍座方向上看到非常低的梯度。 由于RMSprop更新中的分母术语，这将提高此方向的有效学习率，从而帮助RMSProp继续进行。

### 超参数优化：

超参数优化

正如我们所见，训练神经网络可能涉及许多超参数设置。神经网络背景下最常见的超参数包括：

初始学习率
学习率衰减时间表（如衰变常数）
正则化强度（L2惩罚，dropout强度）
但正如我们所看到的，还有更多相对不太敏感的超参数，例如在每个参数的自适应学习方法，动量的设置及其时间表等。在本节中，我们描述了执行超参数搜索的一些额外提示和技巧：

实现：

较大的神经网络通常需要很长时间来训练，因此执行超参数搜索可能需要很多天/周。记住这一点非常重要，因为它会影响代码库的设计。一个特定的设计是让工作人员连续采样随机超参数并执行优化。在培训期间，工作人员将在每个时期之后跟踪验证性能，并将模型检查点（以及诸如随时间的损失的各种训练统计信息）写入文件，优选地在共享文件系统上。将验证性能直接包含在文件名中非常有用，因此检查和排序进度很简单。然后还有第二个程序，我们称之为主程序，它可以在计算集群中启动或杀死工作人员，还可以检查工作人员编写的检查点并绘制他们的训练统计数据等。

首选一个验证折叠以进行交叉验证：

在大多数情况下，可观大小的单个验证集大大简化了代码库，而无需使用多个折叠进行交叉验证。你会听到人们说他们“交叉验证”了一个参数，但很多时候人们都认为他们仍然只使用了一个验证集。

超参数范围：

在对数刻度上搜索超参数。例如，学习率的典型采样如下：`learning_rate = 10 ** uniform（-6,1）`。也就是说，我们从均匀分布中生成一个随机数，然后将其提升到10的幂。同样的策略应该用于正则化强度。直觉上，这是因为学习率和正则化强度对训练动力学具有乘法效应。例如，如果学习率为0.001，则对学习率加0.01的固定变化会对动态产生巨大影响，但如果学习率为10时几乎没有影响。这是因为学习率乘以计算的梯度更新。因此，考虑将学习率乘以或除以某个值的范围比将某个值加上或减去的学习率的范围更自然。相反，通常以原始比例搜索一些参数（例如，丢失）（例如，丢失=均匀（0,1））。

首选随机搜索到网格搜索:

正如Bergstra和Bengio在随机搜索超参数优化中所论述的那样，“随机选择的试验对于超参数优化比在网格上的试验更有效”。事实证明，这通常也更容易实现。

小心边界上最好的价值观:

有时，您可能会在不良范围内搜索超参数（例如学习率）。例如，假设我们使用`learning_rate = 10 ** uniform（-6,1）`。一旦我们收到结果，重要的是要仔细检查最终学习率是否不在此间隔的边缘，否则您可能会错过超出间隔的更优化的超参数设置。

将您的搜索从粗略变为精细

在实践中，首先搜索粗略范围（例如10 ** [ -  6,1]），然后根据最佳结果出现的位置，缩小范围可能会有所帮助。此外，执行初始粗略搜索同时仅训练1个纪元甚至更少，这可能是有帮助的，因为许多超参数设置可能导致模型根本不学习，或者立即以无限的成本爆炸。然后第二阶段可以用5个时期执行更窄的搜索，并且最后阶段可以在更多时期（例如）的最终范围内执行详细搜索。

贝叶斯超参数优化是一个致力于提出算法的研究的整个领域，该算法试图更有效地导航超参数空间。核心思想是在查询不同超参数的性能时适当平衡探索 - 开发权衡。基于这些模型已经开发了多个库，其中一些比较着名的库是Spearmint，SMAC和Hyperopt。但是，在使用ConvNets的实际设置中，在精心选择的时间间隔内击败随机搜索仍然相对困难。请参阅此处的其他一些讨论。

### 评估结果

模型总体。

在实践中，一种将神经网络性能提高几个百分点的可靠方法是训练多个独立模型，并在测试时平均预测。随着整体中模型的数量增加，性能通常会单调改善（尽管收益递减）。此外，随着整体模型的更多变化，这些改进更加引人注目。有几种形成整体的方法：

1. 相同的模型，不同的初始化参数：
  使用交叉验证来确定最佳超参数，然后使用最佳超参数集训练多个模型，但使用不同的随机初始化。这种方法的危险在于变化只是由于初始化。

2. 选择在交叉验证期间发现的最好的模型：
  使用交叉验证来确定最佳超参数，然后选择前几个（例如10个）模型来形成整体。这改善了整体的多样性，但存在包括次优模型的危险。在实践中，这可以更容易执行，因为在交叉验证之后不需要额外的模型再训练单个模型的不同检查点。
  
3. 在训练期间选择不同的checkpoint。
  如果训练非常昂贵，那么有些人在一段时间内（例如在每个时期之后）采用单个网络的不同检查点并使用这些检查点形成整体的成功有限。显然，这种情况有些缺乏，但在实践中仍然可以很好地运作。这种方法的优点是非常便宜。

4. 在训练期间选择平均的参数
  与最后一点相关，几乎总是获得额外的一两个性能的廉价方法是在内存中维持网络权重的第二个副本，其在训练期间保持指数衰减的先前权重之和。这样，您可以在过去几次迭代中平均网络状态。您会发现，在最后几步中，这种“平滑”的权重版本几乎总能获得更好的验证错误。要记住的粗略直觉是目标是碗状的，你的网络在模式中跳跃，所以平均值有更高的机会在更接近模式的地方。

模型集合的一个缺点是它们需要更长的时间来评估测试示例。感兴趣的读者可能会发现Geoff Hinton最近的作品对“黑暗知识”的启发，其中的想法是通过将整体对数可能性纳入修改后的目标，将一个好的集合“提炼”回单个模型。

### 总结

训练神经网络：

1. Gradient使用一小批数据检查您的实现，并了解陷阱。
2. 作为一个完整性检查，确保您的初始损失是合理的，并且您可以在极小部分数据上实现100％的培训准确性
3. 在训练期间，监控损失，训练/验证准确性，如果你感觉更漂亮，与参数值相关的更新幅度（应该是~1e-3），以及在处理ConvNets时，第一层权重。
4. 建议使用的两个更新是SGD + Nesterov Momentum或Adam。
5. 在训练期间衰减您的学习率。例如，在固定数量的时期之后，或者每当验证准确度最高时，将学习率减半。
6. 使用随机搜索（不是网格搜索）搜索好的超参数。将您的搜索从粗略（宽超参数范围，仅训练1-5个时期）到精细（较窄的游侠，更多时期的训练）进行搜索
7. 使用模型集合以获得额外的性能

## 实例：做一个小玩具

### 首先生成一些数据：

{% codeblock lang:python %}
N = 100 # number of points per class
D = 2 # dimensionality
K = 3 # number of classes
X = np.zeros((N*K,D)) # data matrix (each row = single example)
y = np.zeros(N*K, dtype='uint8') # class labels
for j in xrange(K):
  ix = range(N*j,N*(j+1))
  r = np.linspace(0.0,1,N) # radius
  t = np.linspace(j*4,(j+1)*4,N) + np.random.randn(N)*0.2 # theta
  X[ix] = np.c_[r*np.sin(t), r*np.cos(t)]
  y[ix] = j
# lets visualize the data:
plt.scatter(X[:, 0], X[:, 1], c=y, s=40, cmap=plt.cm.Spectral)
plt.show()
{% endcodeblock %}

### 准备softmax分类器

{% codeblock lang:python %}
# initialize parameters randomly
W = 0.01 * np.random.randn(D,K)
b = np.zeros((1,K))
{% endcodeblock %}

这里D=2，K=3

### 计算分数

{% codeblock lang:python %}
# compute class scores for a linear classifier
scores = np.dot(X, W) + b
{% endcodeblock %}

### 计算损失

{% codeblock lang:python %}
num_examples = X.shape[0]
# get unnormalized probabilities
exp_scores = np.exp(scores)
# normalize them for each example
probs = exp_scores / np.sum(exp_scores, axis=1, keepdims=True)
correct_logprobs = -np.log(probs[range(num_examples),y])
# compute the loss: average cross-entropy loss and regularization
data_loss = np.sum(correct_logprobs)/num_examples
reg_loss = 0.5*reg*np.sum(W*W)
loss = data_loss + reg_loss
{% endcodeblock %}

### 计算解析梯度，使用后向传播的方法

{% codeblock lang:python %}
dscores = probs
dscores[range(num_examples),y] -= 1
dscores /= num_examples

dW = np.dot(X.T, dscores)
db = np.sum(dscores, axis=0, keepdims=True)
dW += reg*W # don't forget the regularization gradient

{% endcodeblock %}

### 执行参数更新

{% codeblock lang:python %}
# perform a parameter update
W += -step_size * dW
b += -step_size * db
{% endcodeblock %}

放在一起，就成了一个简单的softmax分类器了：
{% codeblock lang:python %}
#Train a Linear Classifier

# initialize parameters randomly
W = 0.01 * np.random.randn(D,K)
b = np.zeros((1,K))

# some hyperparameters
step_size = 1e-0
reg = 1e-3 # regularization strength

# gradient descent loop
num_examples = X.shape[0]
for i in xrange(200):
  
  # evaluate class scores, [N x K]
  scores = np.dot(X, W) + b 
  
  # compute the class probabilities
  exp_scores = np.exp(scores)
  probs = exp_scores / np.sum(exp_scores, axis=1, keepdims=True) # [N x K]
  
  # compute the loss: average cross-entropy loss and regularization
  correct_logprobs = -np.log(probs[range(num_examples),y])
  data_loss = np.sum(correct_logprobs)/num_examples
  reg_loss = 0.5*reg*np.sum(W*W)
  loss = data_loss + reg_loss
  if i % 10 == 0:
    print "iteration %d: loss %f" % (i, loss)
  
  # compute the gradient on scores
  dscores = probs
  dscores[range(num_examples),y] -= 1
  dscores /= num_examples
  
  # backpropate the gradient to the parameters (W,b)
  dW = np.dot(X.T, dscores)
  db = np.sum(dscores, axis=0, keepdims=True)
  
  dW += reg*W # regularization gradient
  
  # perform a parameter update
  W += -step_size * dW
  b += -step_size * db
{% endcodeblock %}

输出loss在0.78左右：

{% codeblock lang:python %}

# evaluate training set accuracy
scores = np.dot(X, W) + b
predicted_class = np.argmax(scores, axis=1)
print 'training accuracy: %.2f' % (np.mean(predicted_class == y))

{% endcodeblock %}

准确率在49%左右。

### 这次训练一个神经网络，初始化参数

{% codeblock lang:python %}
# initialize parameters randomly
h = 100 # size of hidden layer
W = 0.01 * np.random.randn(D,h)
b = np.zeros((1,h))
W2 = 0.01 * np.random.randn(h,K)
b2 = np.zeros((1,K))
{% endcodeblock %}

前向计算分数

{% codeblock lang:python %}
# evaluate class scores with a 2-layer Neural Network
hidden_layer = np.maximum(0, np.dot(X, W) + b) # note, ReLU activation
scores = np.dot(hidden_layer, W2) + b2
{% endcodeblock %}

然后是计算梯度

{% codeblock lang:python %}
# backpropate the gradient to the parameters
# first backprop into parameters W2 and b2
dW2 = np.dot(hidden_layer.T, dscores)
db2 = np.sum(dscores, axis=0, keepdims=True)
dhidden = np.dot(dscores, W2.T)
# backprop the ReLU non-linearity
dhidden[hidden_layer <= 0] = 0
# finally into W,b
dW = np.dot(X.T, dhidden)
db = np.sum(dhidden, axis=0, keepdims=True)
{% endcodeblock %}

完整代码如下：

{% codeblock lang:python %}
# initialize parameters randomly
h = 100 # size of hidden layer
W = 0.01 * np.random.randn(D,h)
b = np.zeros((1,h))
W2 = 0.01 * np.random.randn(h,K)
b2 = np.zeros((1,K))

# some hyperparameters
step_size = 1e-0
reg = 1e-3 # regularization strength

# gradient descent loop
num_examples = X.shape[0]
for i in xrange(10000):
  
  # evaluate class scores, [N x K]
  hidden_layer = np.maximum(0, np.dot(X, W) + b) # note, ReLU activation
  scores = np.dot(hidden_layer, W2) + b2
  
  # compute the class probabilities
  exp_scores = np.exp(scores)
  probs = exp_scores / np.sum(exp_scores, axis=1, keepdims=True) # [N x K]
  
  # compute the loss: average cross-entropy loss and regularization
  correct_logprobs = -np.log(probs[range(num_examples),y])
  data_loss = np.sum(correct_logprobs)/num_examples
  reg_loss = 0.5*reg*np.sum(W*W) + 0.5*reg*np.sum(W2*W2)
  loss = data_loss + reg_loss
  if i % 1000 == 0:
    print "iteration %d: loss %f" % (i, loss)
  
  # compute the gradient on scores
  dscores = probs
  dscores[range(num_examples),y] -= 1
  dscores /= num_examples
  
  # backpropate the gradient to the parameters
  # first backprop into parameters W2 and b2
  dW2 = np.dot(hidden_layer.T, dscores)
  db2 = np.sum(dscores, axis=0, keepdims=True)
  # next backprop into hidden layer
  dhidden = np.dot(dscores, W2.T)
  # backprop the ReLU non-linearity
  dhidden[hidden_layer <= 0] = 0
  # finally into W,b
  dW = np.dot(X.T, dhidden)
  db = np.sum(dhidden, axis=0, keepdims=True)
  
  # add regularization gradient contribution
  dW2 += reg * W2
  dW += reg * W
  
  # perform a parameter update
  W += -step_size * dW
  b += -step_size * db
  W2 += -step_size * dW2
  b2 += -step_size * db2
{% endcodeblock %}

输出loss为0.24左右，正确率98%左右。

### 看优化问题

Hessian矩阵。

使用Hessian矩阵的方法：

泰勒展开（二阶）：

$$J(\boldsymbol{\theta}) \approx J\left(\boldsymbol{\theta}_{0}\right)+\left(\boldsymbol{\theta}-\boldsymbol{\theta}_{0}\right)^{\top} \nabla_{\boldsymbol{\theta}} J\left(\boldsymbol{\theta}_{0}\right)+\frac{1}{2}\left(\boldsymbol{\theta}-\boldsymbol{\theta}_{0}\right)^{\top} \boldsymbol{H}\left(\boldsymbol{\theta}-\boldsymbol{\theta}_{0}\right)$$

$$\boldsymbol{\theta}^{*}=\boldsymbol{\theta}_{0}-\boldsymbol{H}^{-1} \nabla_{\boldsymbol{\theta}} J\left(\boldsymbol{\theta}_{0}\right)$$

优点：
1. 没有超参数
2. 没有学习率

缺点：
1. Hessian矩阵的计算是O(N^2)的复杂度
2. 求逆矩阵是O(N^3)的复杂度
3. N一般很大，几千万吧

一些优化：

BGFS，拟牛顿法，不直接求Hessian矩阵O(N^3)，而是求Hessian矩阵的rank为1的迭代更新，每次只要O(N^2)的复杂度。

L-BFGS：

不储存全部Hessian矩阵的逆矩阵。
优点：
1. 在full batch当中非常有用，单个，确定性的f(x)的话可能会有很好的效果。

2. 一般在mini-batch当中效果不咋地。

一般Adam就挺好的了。如果你能负担得起全批量的更新的话，可以试试L-BFGS（别忘掉关掉所有的输入噪声。

一般来说训练loss越小越好，但是我们真正关心的是训练集正确率和验证集正确率的gap。

模型配合选择：

1. 训练很多独立的模型？还是固定时间保存一个快照会更好。

2. 在测试阶段测试两个独立的值。在测试阶段，不是使用实际的参数向量，而是使用一个移动的平均参数向量并且使用它。因为测试的时候不能对模型再做什么改动了吧qwq。

3. 如果训练正确率和验证集正确率的gap太大的话，那说明正则强度不够。在损失函数后面添加额外的正则项，L2，L1，Elastic net都可以。

很牛逼的方法：dropout，在每个前向传递的过程中，随机使得一些神经元为0。一般这个值可以设定为0.5。

ReLU后面dropout酱紫，`U1 = np.random.rand(*H1.shape) < p`

dropout的意义是使得一些特征不同时出现也能够表示一些特征。比如猫可能看到眼睛看到尾巴看到有毛有眼睑，但是比如遮挡一些元素的话，可能还是猫酱紫😯

dropout是在训练模型的参数之间的关系，共享参数这种。每个二进制数位都表示一个模型的话，一个有4096个神经元的全连接层，可能有10^1233种不同的模型，而宇宙的原子数量只有10^82个。

dropout在训练阶段的p=0.5的话，输出的期望应该是原来的值乘0.5，所以在测试阶段，测试结果的期望应该乘以0.5。

但是这样做的话，需要在测试阶段吧这个概率p传进去，但其实是不必要的，在训练阶段除以p就可以了qwq。

所以是训练阶段加入某种随机，测试阶段把这些随机平均掉（有时是近似掉）

应该有2%的额外的性能提升。

### 数据增强

输入是一个图片和一个标签（🐱），水平翻转一下也还是个🐱。

有一种方式叫做随机剪裁和缩放。

训练时：

1. 比如在[256x480]范围里面随机选择一个数字L
2. 选择一个，重新裁剪这个图片，短边长度保持为L
3. 在reshape这个图片patch为224x224

测试时：

1. 把图片缩放为5个尺度：{224,256,384,480,640}
2. 对每个大小采用10个224x224的crop：四角+中间，然后翻转。

其他方法：调整对比度或者调整亮度。

更复杂的方法：
1. 对于每个像素的RGB通道上使用PCA降维。
2. 在主成分上面采样一个颜色偏差
3. 在所有的像素上面添加这个偏差，然后再训练图片。

随机混合下面的一些方法：

1. 位移
2. 旋转
3. 伸展
4. 裁剪
5. 镜头扭曲，疯了吧

正则化方法：

1. dropout
2. batch normalization
3. data augmentation
4. drop connect
5. Fractional max pooling
6. 随机深度。stochastic depth


