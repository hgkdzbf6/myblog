---
layout: post
title:  "cs231n CNN"
date:   2019-03-29 14:28:46 +0800
categories: AI
---

<script type="text/javascript" async
  src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/MathJax.js?config=TeX-MML-AM_CHTML">
</script>

## 0.1. 终于开始CNN了

好像最近三巨头是拿了图灵奖是么，虽然我也不怎么关心qwq

CNN明确假设输入是图像，这样能够显著减少参数的数量。

为什么会想到用cnn？常规神经网络无法扩展到完整图像，全连接的话不能扩展到更大的图像，并且很快会导致过拟合。

经典结构：输入 --> 卷积 --> ReLU --> Pool --> FC

输入： 32x32x3
卷积层： 32x32x12，使用12个filter
ReLU： 元素级别的max(0,x)
Pool: 最大值Pooling，输出为16x16x12
FC：1x1x10，代表输出的类别

某些层包含参数，但是有些层不含。
每个图层都接受输入3D体积，并通过可微分函数将其转换为输出3D体积。
每个层可能有也可能没有参数（例如CONV / FC有参数，RELU / POOL没有参数）
每个层可能有也可能没有额外的超参数（例如CONV / FC / POOL有，RELU没有）

感受野很关键：就是滤波器大小

示例1.例如，假设输入数据具有[32x32x3]的大小（例如RGB CIFAR-10图像）。如果感受野（或滤波器大小）是5x5，那么Conv层中的每个神经元将具有输入体积中[5x5x3]区域的权重，总共5 * 5 * 3 = 75个权重（和+1）bias参数）。请注意，沿深度轴的连接范围必须为3，因为这是输入数据的深度。也就是一次卷积操作，只有76个参数需要调整，大大减少了参数的数目。

示例2.假设输入数据大小为[16x16x20]。然后使用3x3的示例感受区域大小，Conv层中的每个神经元现在将具有到输入音量的总共3 * 3 * 20 = 180个连接。请注意，同样，连接在空间中是局部的（例如3x3），但在输入深度（20）上是完整的。

卷积层的超参数：

深度depth，步伐stride和零填充大小

输入W，感受野大小F，零padding大小P，stride大小S。
适合的神经元数量为：$$(W-F+2 P) / S+1$$

如果设置为3.14的话，

如果一个特征对于在某个空间位置（x，y）计算是有用的，那么在不同位置计算它也应该是有用的（x2 ，y2）上。

将单个二维深度切片表示为深度切片（例如，大小为[55x55x96]的体积具有96个深度切片，每个切片大小为[55x55]）
在反向传播期间的实践中，体积中的每个神经元将计算其权重的梯度，但是这些梯度将在每个深度切片上相加并且仅更新每个切片的单个权重集。

请注意，如果单个深度切片中的所有神经元都使用相同的权重向量，那么CONV层的正向通道可以在每个深度切片中计算为神经元权重与输入体积的卷积（因此名称：卷积层）。 这就是为什么通常将权重集称为与输入卷积的过滤器（或内核）。

这是针对特征的位置和特征本身没有强关系，才有这个假设。有时候会失效，比如头发就是应该出现在头上，眼睛，嘴都应该出现在他的位置上。 

在这种情况下，通常放松参数共享方案，而是简单地将该层称为局部连接层。

### 0.1.1. Numpy例子表示

设一个Volume为`X[x,y,d]`
分别表示长，宽和深度。
一个像素的深度fiber可以被描述为`X[x,y,:]`
一个面可以被描述为`X[:,:,d]`

变成矩阵乘法

CNN里面用的是点乘，而不是卷积操作。

`im2col`这个是啥？输入图像中的局部区域伸展成列的操作是`im2col`。

输入是`227x227x3`，用`11x11x3`的滤波器，然后stride是4，K是(227-11)/4+1=55，
输出`X_col`是[363,3025]，但是由于感受野重叠，导致了很多区域都是重复的。

权重`W_row`应该是[96x363]

结果应该是`np.dot(W_row, X_col)`

但是用的内存太多了，使用BLAS可以。

1x1卷积。另外，一些论文使用1x1卷积，首先由Network in Network调查。有些人最初很难看到1x1卷积，特别是当它们来自信号处理背景时。通常信号是二维的，因此1x1卷积没有意义（它只是逐点缩放）。然而，在ConvNets中并非如此，因为必须记住我们在三维体积上操作，并且滤波器总是延伸到输入体积的整个深度。例如，如果输入为[32x32x3]，则执行1x1卷积将有效地执行三维点积（因为输入深度为3个通道）。

扩张的卷积。最近的一项发展（例如参见Fisher Yu和Vladlen Koltun撰写的论文）是在CONV层引入另一个称为扩张的超参数。到目前为止，我们只讨论了连续的CONV过滤器。但是，可以使每个单元格之间有空格的过滤器称为扩张。作为示例，在一个维度中，大小为3的滤波器w将在输入x上计算以下：w [0] * x [0] + w [1] * x [1] + w [2] * x [2] 。这是0的扩张。对于扩张1，滤波器将改为计算w [0] * x [0] + w [1] * x [2] + w [2] * x [4];换句话说，应用程序之间存在1的差距。这在某些设置中非常有用，可以与0扩展滤波器结合使用，因为它允许您使用更少的层更加积极地合并输入中的空间信息。例如，如果你将两个3x3 CONV层堆叠在一起，那么你可以说服自己第二层的神经元是输入的5x5补丁的函数（我们可以说这些神经元的有效感受野是5×5）。如果我们使用扩张的卷积，那么这个有效的感受野会增长得更快。

操作：

输入：W1,H1,D1
两个超参数：F，S

输出： 
$$\begin{aligned} W_{2} &=\left(W_{1}-F\right) / S+1 \\ H_{2} &=\left(H_{1}-F\right) / S+1 \\ D_{2} &=D_{1} \end{aligned}$$

使用例如变分自动编码器（VAE）或生成性对抗网络（GAN）可以代替池化层。

未来的架构很可能只​​有很少甚至没有池化层。

Normalization已经被抛弃，在CNN当中没啥用

全连接层

把全连接层变成卷积层：FC和CONV层之间的唯一区别是CONV层中的神经元仅连接到输入中的局部区域，并且CONV卷中的许多神经元共享参数。然而，两层中的神经元仍然计算点积，因此它们的功能形式是相同的。因此，事实证明，可以在FC和CONV层之间进行转换：

对于任何CONV层，都有一个实现相同前向功能的FC层。 权重矩阵将是大的矩阵，除了在某些块（由于本地连接）之外，其大部分为零，其中许多块中的权重相等（由于参数共享）。
相反，任何FC层都可以转换为CONV层。 例如，观察一些尺寸为7×7×512的输入体积的K = 4096的FC层可以等效地表示为具有F = 7，P = 0，S = 1，K = 4096的CONV层。 换句话说，我们将滤波器大小设置为输入音量的大小，因此输出将只是1×1×4096，因为只有一个深度列“适合”输入音量，从而得到相同的结果 最初的FC层。

也就是整个大的图像范围作为卷积核。

### 0.1.2. 卷积层结构：

`INPUT -> [[CONV -> RELU]*N -> POOL?]*M -> [FC -> RELU]*K -> FC`

`*`代表重复，`POOL?`表示池化层可有可无，

一般情况下，N>=0, N<=3, M>=0, K>=0, K<3

常用结构：

- `INPUT -> FC`，表示线性分类器，这里`N=M=K=0`
- `INPUT -> CONV -> RELU -> FC`
- `INPUT -> [CONV -> RELU -> POOL]*2 -> FC -> RELU -> FC`,这里每个池化层之间只有一个卷积层
- `INPUT -> [CONV -> RELU -> CONV -> RELU -> POOL]*3 -> [FC -> RELU]*2 -> FC`，这里在池化层之前有两个卷积层，对于更大的更深的网络来说，这是个好的方法，因为在池化之前，多层堆叠在一起的卷积层可以生成更多的复杂特征。

小的卷积核堆叠成大感受野的卷积层：假设你堆叠3个3x3的卷积层（当然引入非线性），在排列当中，第一层的每个神经元有3x3的输入视野。
所以第二层就有5x5的感受野。同样，第三层就有7x7的感受野。但是有诸多缺点：

1. 每个神经元都会计算一次输入的线性函数，三层卷积的话包含非线性，使得得到特征的难度增大。
2. 假设所有数据的通道数为C，可以看到，7x7的卷积层包含$$7*7*C*C=49C^2$$个参数，但是三层卷积层包含的参数数量为：$$3*(C*(3*3*C))=27C^2$$个参数。直觉上看，使用小的滤波器堆叠的若干CONV层，而不是直接使用大滤波器，一个CONV层，能够表示更加强大的输入特征，而且具有更小的参数。但是实际上，如果我们打算要进行反向传播的话，我们可能需要更多的内存来储存中间过程。

值得说明的一点是，传统的线性列表层已经被扔掉了qwq。
像谷歌的Inception架构，以及当前的state-of-art残差网络都用了更复杂的结构。

实际上是什么好就用什么。如果你觉得这种架构什么的很难理解，你应该很高兴知道，90%以上的应用你不需要担心这个问题。`不要成为一个英雄`：你应该看那个架构在ImageNet上面很好用，下载预训练的模型最后fine-tune这个模型以适应你的数据，而不是针对你的问题提出你新的架构。
（说的真好qwq）

### 0.1.3. 层大小匹配

现在我们没怎么提这个CONV的超参数，我们先提出一般性的原则，然后详细说明这个问题。

- 输入层：这个大小应该能够被2整除很多次，比如常见的大小有：32，64，96，224，384，512.

- 卷积层：应该使用小的卷积核，步长设定为1。**非常关键**：输入应该补充0，使得卷积层不改变输入的空间结构。当F=5，P=2时，对于一般的F来说，P=(F-1)/2的话和输入是一样的。如果必须使用很大的卷积核的话，只有在第一层进行如此操作比较常见。

- 池化层：一般用2x2的感受野，也就是F=2，stride=2。注意到这实际上丢弃了75%的输入激活结果（因为同时对宽度和高度进行下采样）。另一个不常用的是使用3x3的感受野，stride还是2，但是也有这么做的。非常不常见的方法是池化层的感受野大小大于3的，因为这样的池化损失太大了，太aggressive了，这一般会使得性能下降qwq。

一些问题：

- 注意每个连接的大小吧。 上面提出的方案是令人愉快的，因为所有CONV层都保留了它们输入的空间大小，而POOL层单独负责在空间上对卷进行下采样。 在另一种方案中，我们使用大于1的步幅或者不对CONV层中的输入进行零填充，我们必须非常仔细地跟踪整个CNN架构中的输入量，并确保所有步幅和过滤器“正常工作” out“和ConvNet架构很好地对称连接。

- 为什么在CONV当中的stride设为1？实践上更好。另外stride设为1可以把所有的下采样工作交给POOL层，CONV层只需要转化输入volume就可以了。

- 为什么用padding？为了保持空间大小。如果不用零填充的话，输出大小会变小，边缘的信息会被很快洗掉。

- 折中内存约束。ConvNet架构的早期，使用上述经验法则可以非常快速建立内存量。比如少使用三个3x3的Conv层过滤224x224x3的图像，每个图层包含64个滤波器，并且padding为1，创建3个224x224x64的激活volume。相当于1000w次激活，或者72MB内存。由于GPU经常是受到内存的制约，可能需要折中。比如一个折中可能是使用7x7的滤波器，stride为2.另一个例子是AlexNet的滤波器大小为11x11，stride为4。

经典网络：

- LeNet: [论文在此](http://yann.lecun.com/exdb/publis/pdf/lecun-98.pdf)

- AlexNet: [论文在此](http://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks)
参加这个2012年的比赛🏆。[ImageNet ILSVRC challenge](http://www.image-net.org/challenges/LSVRC/2014/)显著比第二名高，和LeNet的结构差不多，但是更深，特征卷积层堆叠在彼此之上（以前POOL层前面只有一个CONV层）

- ZF Net: 2013年ILSVRC的🏆。这个比[论文在此](http://arxiv.org/abs/1311.2901)改进了AlexNet，通过扭曲结构的超参数进行改进，特别是扩张了中间卷积层的数量，使得第一层的stride和卷积层大小更小。

- GoogLeNet: 2014年ILSVRC的🏆。[论文在此](http://arxiv.org/abs/1409.4842)。提出盗梦空间模型，创造性的把参数数量几十年少了很多（4百万，AlexNet6千万）。另外在ConvNet的顶端，使用平均池化层，而不是全连接层，消除了大量的，不是那么重要的参数。最新版本的是[Inception-v4](http://arxiv.org/abs/1602.07261)

- VGGNet: 2014年ILSVRC的🥈，[论文在此](http://www.robots.ox.ac.uk/~vgg/research/very_deep/)，特殊的贡献点是网络的深度对于效果来说是一个很大一个影响因素把。最好的工作是包含16个CONV/FC网络，并且从头到尾，结构都极为相似，只用3x3和2x2卷积。预训练模型用caffe表示[在此](http://www.robots.ox.ac.uk/~vgg/research/very_deep/)，但有个缺陷是这个网络成本太高了，要很多内存并且参数数量有1亿4千万qwq。这些参数主要是在第一个全连接层里面，后来发现这些FC可以被删除，并且没啥性能降低，大大减少了必要的参数数目。

- ResNet: 2015年ILSVRC的🏆，何凯明的。[论文在此](http://arxiv.org/abs/1512.03385)跳过了连接，大量使用[batchnorm](https://arxiv.org/abs/1502.03167)，在最后也没有使用全连接层，这里有他的[演讲ppt](http://research.microsoft.com/en-us/um/people/kahe/ilsvrc15/ilsvrc2015_deep_residual_learning_kaiminghe.pdf)和[视频](https://www.youtube.com/watch?v=1PGLj-uKT1w)。还有一些最近的[实验](https://github.com/gcr/torch-residual-networks)，用torch写的。ResNet现在已经是state-of-art了，在实际当中是首先被考虑的ConvNet。这是一些[最近的研究](https://arxiv.org/abs/1603.05027)。

VGG网络的细节：3x3卷积层，stride1，padding1，pool层2x2，stride2，没padding。

```yaml
- INPUT: [224x224x3]  
  内存: 224x224x3 = 150K
  权重: 0
- CONV3-64: [224x224x64]
  内存: 224x224x64 = 3.2M
  权重: 3x3x3x64=1728
- CONV3-64: [224x224x64]
  内存: 224x224x64 = 3.2M
  权重: 3x3x3x64=36864
- POOL2: [112x112x64]
  内存: 112x112x64 = 800K
  权重: 0
- CONV3-128: [112x112x128]
  内存: 112x112x128 = 1.6M
  权重: 3x3x64x128=73728
- CONV3-128: [112x112x128]
  内存: 112x112x128 = 1.6M
  权重: 3x3x3x128=147456
- POOL2: [56x56x128]
  内存: 56x56x128 = 400K
  权重: 0
- CONV3-256: [56x56x256]
  内存: 56x56x256 = 800K
  权重: 3x3x128x256=294912
- CONV3-256: [56x56x256]
  内存: 56x56x256 = 800K
  权重: 3x3x256x256=589824
- CONV3-256: [56x56x256]
  内存: 56x56x256 = 800K
  权重: 3x3x256x256=589824
- POOL2: [28x28x256]
  内存: 28x28x256 = 200K
  权重: 0
- CONV3-512: [28x28x512]
  内存: 28x28x512 = 400K
  权重: 3x3x256x512=1179648
- CONV3-512: [28x28x512]
  内存: 28x28x512 = 400K
  权重: 3x3x512x512=2359296
- CONV3-512: [28x28x512]
  内存: 28x28x512 = 400K
  权重: 3x3x512x512=2359296
- POOL2: [14x14x256]
  内存: 14x14x512 = 100K
  权重: 0
- CONV3-512: [14x14x512]
  内存: 14x14x512 = 100K
  权重: 3x3x512x512=2359296
- CONV3-512: [14x14x512]
  内存: 14x14x512 = 100K
  权重: 3x3x512x512=2359296
- CONV3-512: [14x14x512]
  内存: 14x14x512 = 100K
  权重: 3x3x512x512=2359296
- POOL2: [7x7x256]
  内存: 7x7x512 = 25K
  权重: 0
- FC: [1x1x4096]
  内存: 4096
  权重: 7x7x512x4096=102760448
- FC: [1x1x4096]
  内存: 4096
  权重: 4096x4096=16777216
- FC: [1x1x4096]
  内存: 1000
  权重: 4096x1000=4096000

总内存: 24M * 4byte ~= 93MB / img， 仅包含前向传播，如果加上后项传播的话要乘以2。

总参数数量: 1亿3千8百万
```

第一个全连接层包含了1亿多参数……

### 0.1.4. 计算的考虑

最大的瓶颈是内存瓶颈。许多现代GPU的版本是3/4/6/12G内存，内存的消耗有3个主要部分：

1. 中间volume的大小。
  上面的那些是原始激活的数量，包含他们的梯度（具有相同大小）。一般来说，所有这些激活函数都是在最早的几层ConvNet上才被激活，这些都需要被保留，因为反向传播的时候需要这些东西。但是只有在测试的时候才运行可以显著减少这个数量，只需要将当前的activation记录下来，并且在任何层丢下之前的激活。

2. 参数大小：因为更新权重需要内存，如果是动量方法，Adagrad，RMSProp方法的话需要储存上一个时刻的值。因此存储这些变量也需要对上面的参数数量乘以至少3左右吧。

3. 其他额外的乱七八糟的内存：比如图像分批次，增广啥的都需要内存。

一般来说先估计参数的总数，这个数字应该变成GB为单位的大小。这些值乘以4得到字节数，成1024得到KB，MB和GB。如果爆掉了的话，就试着减少batch大小，因为大多数内存都是在激活的啥时候被消耗的。

### 0.1.5. 牛逼的迁移学习

如果你用CNN的话，你就需要很多数据？No

训练ImageNet确实很费劲。

对于小数据集的话，可以只训练最后的全连接层，前面的参数冻结就可以了。

对于更大的数据集来说，也是训练最后的三层全连接层就好了（你的意思是训练那个一亿参数的那个全连接层吗qwq），但是初始值就从别人的模型开始就可以了啊。

对于fine tuning过程来说，学习率调整到原来的1/10就好了。

现在来写一个表格。

离输入层越近的层表示的特征越generic，越远的层越specific。

| |非常相似的数据集|非常不同的数据集|
|非常少的数据| 只需要训练最后一层全连接层就好啦| 那可能会有很大的问题|
|有很多的数据| fine tuning最后的几层就行了|需要重新训练很多数据|

迁移学习无处不在，比如物体检测（Fast-RCNN），图像标记（CNN+RNN)。都用到了从ImageNet当中训练出的CNN。图像标记当中还用到了预训练的单词向量word2vec。

到哪里去找这些预训练的模型呢？怎么用呢？

Caffe：<https://github.com/BVLC/caffe/wiki/Model-Zoo>
TensorFlow: <https://github.com/tensorflow/models>
PyTorch: <https://github.com/pytorch/vision>

总结：

1. 优化方法：动量法， RMSProp， Adam方法
2. 正则化方法： dropout
3. 迁移学习： 把他用在你的项目当中

下一节：深度学习软件。

## 深度学习软件，应该偏工程一点

CPU：更少的核，但是每个核运行速度更快，并且更加好用。对于序列任务来说比较适用。

GPU：更多核，但是每个核都慢一点，笨一点，比较适合并行任务。

### GPU编程

- CUDA（只有Nvidia有）
  - C语言风格的代码，直接在GPU上面运行
  - 高级API： cuBLAS， cuFFT，cuDNN。

- OpenCL
  - 和CUDA相似，但是能够运行在任何平台。
  - 一般来说比较慢qwq

- Udacity
  - 引入并行编程，看[cs344这门课](https://www.udacity.com/course/cs344)
  - 对于深度学习来说，只需要使用存在的库就可以了

### CPU与GPU性能差距：

这是使用cudnn和cpu的比较

|vgg-16|66倍|2.8倍|
|vgg-19|67倍|3.0倍|
|ResNet-18|71倍|3.1倍|
|ResNet-50|64倍|3.4倍|
|ResNet-200|76倍|2.8倍|

可能由于从硬盘当中读取数据产生性能瓶颈。

解决办法：
1. 把所有数据读到RAM中
2. 使用SSD而不是HDD
3. 使用多核CPU来获取数据

### 框架

*Caffe* -> *Caffe2*
Torch -> **PyTorch**
Theano ->**TensorFlow**
Paddle
CNTK
MXNet

一个框架应该做到：

1. 容易构建大型计算图
2. 容易在大型计算图当中计算梯度
3. 在GPU上面运行效率高，包括cuDNN，cuBLAS。

{% codeblock lang:python %}
import numpy as np
np.random.seed(0)

N, D = 3,4
x = np.random.randn(N, D)
y = np.random.randn(N, D)
z = np.random.randn(N, D)

a = x * y
b = a + z
c = np.sum(b)

grad_c = 1.0
grad_b = grad_c * np.ones((N ,D))
grad_a = grad_b.copy()
grad_z = grad_b.copy()
grad_x = grad_a * y
grad_y = grad_a * x

{% endcodeblock %}

Numpy的方法有如下缺点：
1. 不能再GPU上运行
2. 需要自己去计算梯度。

{% codeblock lang:python %}
import numpy as np
np.random.seed(0)
import tensorflow as tf

N, D = 3, 4
# 构建计算图
# 在这里可以选择使用gpu还是cpu，好像tf是默认找gpu，找不到的话用cpu
x = tf.placeHolder(tf.float32)
y = tf.placeHolder(tf.float32)
z = tf.placeHolder(tf.float32)

a = x * y
b = a + z
c = tf.reduce_sum(b)

# 计算梯度
grad_x, grad_y, grad_z = tf.gradients(c, [x, y, z])

with tf.Session() as sess:
  values = {
    x: np.random.randn(N,D),
    y: np.random.randn(N,D),
    z: np.random.randn(N,D),
  }
  out = sess.run([c, grad_x, grad_y, grad_z], feed_dict = values)
  c_val, grad_x_val, grad_y_val, grad_z_val = out
{% endcodeblock %}

PyTorch

{% codeblock lang:python %}
import torch
from torch.autograd import Variable

N, D = 3, 4

x = Variable(torch.randn(N, D), require_grad = True)
# x = Variable(torch.randn(N, D).cuda(), require_grad = True) 这是用GPU的情况
y = Variable(torch.randn(N, D), require_grad = True)
z = Variable(torch.randn(N, D), require_grad = True)

a = x * y
b = a + z
c = torch.sum(b)

c.backward()

print(x.grad.data)
print(y.grad.data)
print(z.grad.data)
{% endcodeblock %}

## Tensorflow训练一个2层神经网络

迭代一次版本：

{% codeblock lang:python %}
import tensorflow as tf
import numpy as np
N,D,H = 64,1000,100
x = tf.placeholder(tf.float32, shape=(N,D))
y = tf.placeholder(tf.float32, shape=(N,D))
w1 = tf.placeholder(tf.float32, shape=(D,H))
w2 = tf.placeholder(tf.float32, shape=(H,D))

h = tf.maximum(tf.matmul(x, w1), 0)
y_pred = tf.matmul(h, w2)
diff = y_pred - y
loss = tf.reduce_mean(tf.reduce_sum(diff ** 2, axis = 1))
grad_w1, grad_w2 = tf.gradients(loss, [w1, w2])

with tf.Session() as sess:
  values = {
    x: np.random.randn(N, D), 
    w1: np.random.randn(D, H), 
    w2: np.random.randn(H, D), 
    y: np.random.randn(N, D), 
  }
  out = sess.run([loss, grad_w1, grad_w2], feed_dict=values)
  loss_val, grad_w1_val, grad_w2_val = out
{% endcodeblock %}

多次迭代版本：

{% codeblock lang:python %}
with tf.Session() as sess:
  values = {
    x: np.random.randn(N, D), 
    w1: np.random.randn(D, H), 
    w2: np.random.randn(H, D), 
    y: np.random.randn(N, D), 
  }
  learning_rate = 1e-5
  for t in range(50):
    out = sess.run([loss, grad_w1, grad_w2], feed_dict=values)
    loss_val, grad_w1_val, grad_w2_val = out
    values[w1] -=learning_rate * grad_w1_val
    values[w2] -=learning_rate * grad_w2_val
{% endcodeblock %}

但是这样会在cpu和gpu之间频繁复制数据。于是有新的形式：

{% codeblock lang:python %}
import tensorflow as tf
import numpy as np
N,D,H = 64,1000,100
x = tf.placeholder(tf.float32, shape=(N,D))
y = tf.placeholder(tf.float32, shape=(N,D))
w1 = tf.Variable(tf.random_normal((D,H)))
w2 = tf.Variable(tf.random_normal((H,D)))

h = tf.maximum(tf.matmul(x, w1), 0)
y_pred = tf.matmul(h, w2)
diff = y_pred - y
loss = tf.reduce_mean(tf.reduce_sum(diff ** 2, axis = 1))
grad_w1, grad_w2 = tf.gradients(loss, [w1, w2])

learning_rate = 1e-5
new_w1 = w1.assign(w1 - learning_rate * grad_w1)
new_w2 = w2.assign(w2 - learning_rate * grad_w2)

with tf.Session() as sess:
  sess.run(tf.global_variables_initiializer())
  values = {
    x: np.random.randn(N, D),
    y: np.random.randn(N, D),
  }
  for t in range(50):
    loss_val, = sess.run([loss], feed_dict=values)
{% endcodeblock %}

这样的话，只有输入和输出时placeholder，其他都是中间变量。assign操作来更新这个w1和w2。sess.run(tf.global_variables_initiializer())这个就把所有的权重初始化了。

问题：loss不下降，assign操作其实没有被执行。怎么办？

解决办法：添加一个多余的dummy节点，这样就能够使得死掉的w1和w2动起来。

{% codeblock lang:python %}
import tensorflow as tf
import numpy as np
N,D,H = 64,1000,100
x = tf.placeholder(tf.float32, shape=(N,D))
y = tf.placeholder(tf.float32, shape=(N,D))
w1 = tf.Variable(tf.random_normal((D,H)))
w2 = tf.Variable(tf.random_normal((H,D)))

h = tf.maximum(tf.matmul(x, w1), 0)
y_pred = tf.matmul(h, w2)
diff = y_pred - y
loss = tf.reduce_mean(tf.reduce_sum(diff ** 2, axis = 1))
grad_w1, grad_w2 = tf.gradients(loss, [w1, w2])

learning_rate = 1e-5
new_w1 = w1.assign(w1 - learning_rate * grad_w1)
new_w2 = w2.assign(w2 - learning_rate * grad_w2)
updates = tf.group(new_w1, new_w2)

with tf.Session() as sess:
  sess.run(tf.global_variables_initiializer())
  values = {
    x: np.random.randn(N, D),
    y: np.random.randn(N, D),
  }
  for t in range(50):
    loss_val,_ = sess.run([loss,updates], feed_dict=values)
{% endcodeblock %}

更集成化，可以直接用tf自带的优化器来计算梯度和权重。

{% codeblock lang:python %}
import tensorflow as tf
import numpy as np
N,D,H = 64,1000,100
x = tf.placeholder(tf.float32, shape=(N,D))
y = tf.placeholder(tf.float32, shape=(N,D))
w1 = tf.Variable(tf.random_normal((D,H)))
w2 = tf.Variable(tf.random_normal((H,D)))

h = tf.maximum(tf.matmul(x, w1), 0)
y_pred = tf.matmul(h, w2)
diff = y_pred - y
loss = tf.reduce_mean(tf.reduce_sum(diff ** 2, axis = 1))

optimizer = tf.train.GradientDescentOptimizer(1e-5)
updates = optimizer.minimize(loss)

with tf.Session() as sess:
  sess.run(tf.global_variables_initiializer())
  values = {
    x: np.random.randn(N, D),
    y: np.random.randn(N, D),
  }
  for t in range(50):
    loss_val,_ = sess.run([loss,updates], feed_dict=values)
{% endcodeblock %}

还能进一步改进，比如我们用通常使用的loss其实也有方法能代替：


{% codeblock lang:python %}
import tensorflow as tf
import numpy as np
N,D,H = 64,1000,100
x = tf.placeholder(tf.float32, shape=(N,D))
y = tf.placeholder(tf.float32, shape=(N,D))
w1 = tf.Variable(tf.random_normal((D,H)))
w2 = tf.Variable(tf.random_normal((H,D)))

h = tf.maximum(tf.matmul(x, w1), 0)
y_pred = tf.matmul(h, w2)
diff = y_pred - y
loss = tf.losses.mean_squared_error(y_pred, y)

optimizer = tf.train.GradientDescentOptimizer(1e-5)
updates = optimizer.minimize(loss)

with tf.Session() as sess:
  sess.run(tf.global_variables_initiializer())
  values = {
    x: np.random.randn(N, D),
    y: np.random.randn(N, D),
  }
  for t in range(50):
    loss_val,_ = sess.run([loss,updates], feed_dict=values)
{% endcodeblock %}

一步步改的面目全非系列：


{% codeblock lang:python %}
import tensorflow as tf
import numpy as np
N,D,H = 64,1000,100
x = tf.placeholder(tf.float32, shape=(N,D))
y = tf.placeholder(tf.float32, shape=(N,D))

init = tf.contrib.layers.xavier_initializer()
h = tf.layers.dense(inputs=x,units=H, activation=tf.nn.relu,kernel_initializer = init)
y_pred = tf.layers.dense(inputs=h, units=D, kernel_initializer=init)

loss = tf.losses.mean_squared_error(y_pred, y)

optimizer = tf.train.GradientDescentOptimizer(1e0)
updates = optimizer.minimize(loss)

with tf.Session() as sess:
  sess.run(tf.global_variables_initiializer())
  values = {
    x: np.random.randn(N, D),
    y: np.random.randn(N, D),
  }
  for t in range(50):
    loss_val,_ = sess.run([loss,updates], feed_dict=values)
{% endcodeblock %}

Keras:更高级的包装纸（同样支持Theano后端）

{% codeblock lang:python %}
from keras.models import Sequential
from keras.layers.core import Dense, Activation
from keras.optimizers import SGD

N, D, H = 64, 1000, 100

model = Sequential()
model.add(Dense(input_dim = D, output_dim = H))
model.add(Activation('relu'))
model.add(Dense(input_dim = H, output_dim=D))

optimizer = SGD(lr=1e0)
model.compile(loss='mean_squared_error', optimizer=optimizer)

x = np.random.randn(N,D)
y = np.random.randn(N,D)
history = model.fit(x,y,nb_epoach=50, batch_size=N, verbose =0)
{% endcodeblock %}

其他的高级api：

- Keras,
- TFLearn,
- TensorLayer,
- tf.layers,
- TF-Slim,
- tf.contrib.learn
- Pretty Tensor
- Sonnet

一些预训练模型：

- [TF-Slim](https://github.com/tensorflow/models/tree/master/slim/nets)
- [Keras](https://github.com/fchollet/deep-learning-models)

使用tensorboard来记录数据
tensorflow的分布式版本：<https://www.tensorflow.org/deploy/distributed>

## PyTorch 三层的抽象

Tensor：要计算的节点，在gpu上面运行
Variable：存储数据和梯度
Module：神经网络层，很多都储存状态或者可学习的权重。

tensorflow中的等价元素：

numpy数组
Tensor, Variable, Placeholder
tf.layers, TFSlim, TFLearn, Sonnets

{% codeblock lang:python %}
import torch

dtype = torch.FloatTensor
# dtype = torch.cuda.FloatTensor 如果使用gpu的话

N, D_in, H, D_out = 64, 1000, 100, 10
x = torch.randn(N, D_in).type(dtype)
y = torch.randd(N, D_out).type(dtype)
w1 = torch.randn(D_in, H).type(dtype)
w2 = torch.randn(H, D_out).dtype(dtype)

learning_rate = 1e-6
for t in range(500):
  h = x.mm(w1)
  # 斜坡，最小值为0
  h_relu = h.clamp(min=0)
  y_pred = h_relu.mm(w2)
  loss = (y_pred - y).pow(2).sum()

  grad_y_pred = 2.0 * (y_pred - y)
  grad_w2 = h_relu.t().mm(grad_y_pred)
  grad_h_relu = grad_y_pred.mm(w2.t())
  grad_h = grad_h_relu.clone()
  grad_h[h<0]=0
  grad_w1 = x.t().mm(grad_h)

  w1 -= learning_rate * grad_w1
  w2 -= learning_rate * grad_w2

{% endcodeblock %}

自动求导：

{% codeblock lang:python %}
import torch
import torch.autograd import Variable

N, D_in, H, D_out = 64, 1000, 100, 10
x = Variable(torch.randn(N, D_in),requires_grad=False)
y = Variable(torch.randn(N, D_out),requires_grad=False)
w1 = Variable(torch.randn(D_in, H),requires_grad=True)
w2 = Variable(torch.randn(H, D_out),requires_grad=True)

learning_rate  =1e-6
for t in range(500):
  y_pred = x.mm(w1).clamp(min=0).mm(w2)
  loss = (y_pred - y).pow(2).sum()

  # 计算梯度的时候要先把这一项的梯度设为0，然后再去计算
  if w1.grad:
    w1.grad.data().zero_()
  if w2.grad:
    w2.grad.data().zero_()
  loss.backward()

  w1.data -= learning_rate * w1.grad.data
  w2.data -= learning_rate * w2.grad.data
{% endcodeblock %}

对于输入变量来说，不需要求导，x和y不需要求导。
所有的Tensor的操作对来说，对Variable也是相同的操作。

自定义函数来执行forward和backward：
{% codeblock lang:python %}
class ReLU(torch.autograd.Function):
  def forward(self, x):
    self.save_for_backward(x)
    return x.clamp(min=0)
  
  def backward(self, grad_y):
    x, = self.saved_tensors
    grad_input = grad_y.clone()
    grad_input[x<0] = 0
    return grad_input
{% endcodeblock %}

这里可以使用自己写的函数调用了。

{% codeblock lang:python %}
import torch
import torch.autograd import Variable

N, D_in, H, D_out = 64, 1000, 100, 10
x = Variable(torch.randn(N, D_in),requires_grad=False)
y = Variable(torch.randn(N, D_out),requires_grad=False)
w1 = Variable(torch.randn(D_in, H),requires_grad=True)
w2 = Variable(torch.randn(H, D_out),requires_grad=True)

learning_rate  =1e-6
for t in range(500):
  relu = ReLU()
  y_pred = relu(x.mm(w1)).mm(w2)
  loss = (y_pred - y).pow(2).sum()

  # 计算梯度的时候要先把这一项的梯度设为0，然后再去计算
  if w1.grad:
    w1.grad.data().zero_()
  if w2.grad:
    w2.grad.data().zero_()
  loss.backward()

  w1.data -= learning_rate * w1.grad.data
  w2.data -= learning_rate * w2.grad.data
{% endcodeblock %}

稍微弄高级了一点点

{% codeblock lang:python %}
import torch
import torch.autograd import Variable

N, D_in, H, D_out = 64, 1000, 100, 10
x = Variable(torch.randn(N, D_in),requires_grad=False)
y = Variable(torch.randn(N, D_out),requires_grad=False)

model = torch.nn.Sequential(
  torch.nn.Linear(D_in, H),
  torch.nn.ReLU(),
  torch.nn.Linear(H, D_out)
)
loss_fn = torch.nn.MSELoss(size_average = False)

learning_rate = 1e-4
for t in range(500):
  y_pred = model(x)
  loss = loss_fn(y_pred, y)

  model.zero_grad()
  loss.backward()

  for param in model.parameters():
    param.data -= learning_rate * param.grad.data
{% endcodeblock %}

使用优化器而不是直接算

{% codeblock lang:python %}
import torch
import torch.autograd import Variable

N, D_in, H, D_out = 64, 1000, 100, 10
x = Variable(torch.randn(N, D_in),requires_grad=False)
y = Variable(torch.randn(N, D_out),requires_grad=False)

model = torch.nn.Sequential(
  torch.nn.Linear(D_in, H),
  torch.nn.ReLU(),
  torch.nn.Linear(H, D_out)
)
loss_fn = torch.nn.MSELoss(size_average = False)

learning_rate = 1e-4
optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)
for t in range(500):
  y_pred = model(x)
  loss = loss_fn(y_pred, y)

  optimizer.zero_grad()
  loss.backward()

  optimizer.step()
{% endcodeblock %}

一个两层网络的例子

{% codeblock lang:python %}

import torch
import torch.autograd import Variable

class TwoLayerNet(torch.nn.Module):
  def __init__(self, D_in,H, D_out):
    super(TwoLayerNet, self).__init__()
    self.linear1 = torch.nn.Linear(D_in, H)
    self.linear2 = torch.nn.Linear(H, D_out)

  def forward(self, x):
    h_relu = self.linear1(x).clamp(min=0)
    y_pred = self.linear2(h_relu)
    return y_pred
  # 不用写backward，torch会自动计算qwq

N, D_in, H, D_out = 64, 1000, 100, 10
x = Variable(torch.randn(N, D_in),requires_grad=False)
y = Variable(torch.randn(N, D_out),requires_grad=False)

model = TwoLayerNet(D_in, H, D_out)

critierion = torch.nn.MSELoss(size_average=False)
optimizer = torch.optim.SGD(model.parameters(), lr=1e-4)
for t in range(500):
  y_pred = model(x)
  loss = criterion(y_pred, y)

  optimizer.zero_grad()
  loss.backward()
  optimizer.step()
{% endcodeblock %}

可以从外部导入数据：DataLoaders。当需要外部数据的时候，就写你的类就可以了。

{% codeblock lang:python %}

import torch
import torch.autograd import Variable
from torch.utils.data import TensorDataset, DataLoader

N, D_in, H, D_out = 64, 1000, 100, 10

loader = DataLoader(TensorDataset(x, y), batch_size = 8)

model = TwoLayerNet(D_in, H, D_out)

critierion = torch.nn.MSELoss(size_average=False)
optimizer = torch.optim.SGD(model.parameters(), lr=1e-4)
for epoch in range(10):
  for x_batch, y_batch in loader:
    x_var, y_var = Variable(x), Variable(y)
    y_pred = model(x_var)
    loss = criterion(y_pred, y_var)

    optimizer.zero_grad()
    loss.backward()
    optimizer.step()
{% endcodeblock %}

可以直接从[这个网址](https://github.com/pytorch/vision)里面找训练好的模型。

{% codeblock lang:python %}
import torch 
import torchvision

alexnet = torchvision.models.alexnet(pretrained = True)
vgg16 = torchvision.models.vgg16(pretrained = True)
resnet101 = torchvision.models.resnet101(pretrained = True)
{% endcodeblock %}

Visdom和Tensorboard有点像，可以记录数据等等等，但是暂时不能看到计算图？
<https://github.com/facebookresearch/visdom>

### Torch和PyTorch的对比：

|Torch|PyTorch|
|Lua，不咋会😭|Python，还算会😁|
|不能自动求导😭|自动求导😸|
|更稳定🙂|更新一点，还在不断改变🤦‍|
|有很多现成代码🙋‍|现在代码不多，但是现在应该也多了起来😂|
|差不多快🙅‍|差不多快|

结论可能PyTorch会好一点。

### Tensorflow和PyTorch的对比

Tensorflow是先构建计算图（一次），然后每次迭代。
PyTorch是每次使用新的图来迭代。

使用静态图的话，能够一开始就优化成等价的操作。

静态图的话一旦构建好，就可以序列化的运行，也就不需要构建图的那一部分代码了。

动态图的构建和执行是交织在一起的，需要保持代码。

tensorflow的fold操作能够使得构建动态图更加容易，通过dynamic batching

动态图的应用：
Recurrent networks（循环网络）
Recursive networks 递归神经网络
Modular network 模块网络？

更深的网络的loss很难降下去，不是因为过拟合产生的。

假说：这个问题是一个优化问题，更深的模型更难以被优化，更深的模型应该至少和浅一点的模型一样好的效果才对。

一个更好的方案是从浅层的网络当中复制学习的层，并且添加额外的层。

解决办法：使用网络层去拟合一个残差，而不是直接去拟合结果。

原来的网络结构：

X -> conv -> relu -> conv -> H(x)

残差模块：

x -> conv -> relu -> conv -> + -> relu
└────────────────────────────^

这个层是用来拟合残差的F(x) = H(x) - x，而不是直接算H(x)的。

整个ResNet结构：

- 堆叠residual block
- 每个residual block有两个3x3的conv层。
- 每两个滤波器和下采样空间使用两个stride
- 在一开始使用一个卷积层
- 没有使用全连接层，只有最后输出1000个类别的时候才用到全连接层。
- 使用全局平均池化层在最后的卷积层之后。

总的深度有34，50，101，152层。

对于更深的网络来说，使用瓶颈层来改善精度（和GoogLeNet差不多）

28x28x256, input -> 1x1x64, conv -> 3x3x64 conv -> 1x1x256 conv

实际的训练过程中：

- 每个conv层后面跟一个batchnorm
- Xavier/2 的初始化参数
- SGD + 动量算法（0.9）来更新
- 学习率为0.1，每次到达验证集检查点的高地的时候减少学习率为原来的1/10
- mini-batch大小256
- 权重衰变为1e-5
- 没有使用dropout层。

实验效果：
所有5项目标都是🏆：
ImageNet图像分类，图像检测，图像定位
COCO检测，分割

比人的效果都好qwq

Inception-v4: Resnet+Inception
VGG：用了最多的内存，
GoogLeNet：最高效
AlexNet：精度不高，内存用量大，准确率低。
ResNet：稳健的效率，依赖于模型，准确率高

其他网络：
Network in Network（NiN）

对ResNet的改进：
Identity Mappings in Deep Residual Networks
创建了更直接的，从底部往上层的通路

宽残差网络：并排这个残差的卷积层。
增加宽度而不是深度可以提高计算性能。

ResNeXt
灵感来自于Inception。

Deep Network with Stochastic Depth:
动机：减少梯度消失
在每次训练的过程当中，以同一个函数来随机丢掉子层。
在测试的时候使用整个网络

FractalNet： Ultra-Deep Neural Networks without Residuals.
残差的表示是没有意义的，从浅层到深层的信息传递才是有意义的。
这个结构可以从深层网络和浅层网络都有直连的通路。
训练的时候随机dropout一些通路
测试的时候使用整个网络。

DenseNet紧密联系的卷积神经网络。
优点：避免梯度消失，强化特征的传播，鼓励特征的重复使用。

有效率的网络：
SqueezeNet：AlexNet等级的精度但是参数数量是AlexNet的1/50。

squeeze层，由1x1和3x3的卷积层构成。

参数数量510倍缩小。

### 总结：

- VGG, GoogLeNet, ResNet 目前也在使用，在zoo里面。
- ResNet有最好的效果
- 趋势是非常深的神经网络
- 研究重点是围绕设计层，跳过连接层和改善梯度传播
- 深度，宽残差连接也是趋势。

下一节：

### Recurrent Neural Networks

Vanilla神经网络：一个接着一个

Recurrent NN: 处理序列

一对多，多对一，多对多

一对多：一张图片用若干单词表示

多对一：从一段话当中判断出说话人的情绪

多对多：机器翻译：一串单词变成另一串。

另一个多对多：帧级别的视频分类。

序列化处理非序列化图像：把这一系列图像变成一串，生成一个快照。一次生成图像的一块。

x -> RNN -> y
在某些时刻想要预测向量的结果。

递推公式：

$$h_t = f_W(h_{t-1}, x_t)$$

每一次都使用同样的函数，同样的参数集合。

比如这个公式：

$$h_{t}=f_{W}\left(h_{t-1}, x_{t}\right)$$

就能够变成这样：

$$h_{t}=\tanh \left(W_{h h} h_{t-1}+W_{x h} x_{t}\right)$$

$$y_{t}=W_{h y} h_{t}$$

函数不再仅以输入为自变量，自身的状态也参与计算，并且所有的时间步更新共享一个权重。

多到一：把输入向量编码成单个向量。
一到多：从输入得到的单个向量制造输出序列。

比如字符层面的语言模型：

词典： `h e l o`

训练的序列为：`hello`

每个输出得到的字符继续作为下一个的输入酱紫。

但是向后backpropogation不好弄，处理的东西太多了。

那就分批次的来，truncated。

看那个[min-char-rnn.py](https://gist.github.com/karpathy/d4dee566867f8291f086)

生成十四行诗`SONNETS`，生成数学公式，生成c代码。

找到解释的单元。引用检测单元。长度跟踪单元。if状态的单元，引用/注释单元，代码深度单元。

图像标注：CNN+RNN。

把图像的最后一层全连接层去掉，接到RNN上面去。

原来的隐含层为：

$$\mathrm{h}=\tanh \left(\mathrm{Wxh}^{*} \mathrm{x}+\mathrm{Whh}^{*} \mathrm{h}\right)$$

现在变成：

$$\mathrm{h}=\tanh \left(\mathrm{Wxh}^{*} \mathrm{x}+\mathrm{Whh}^{*} \mathrm{h}+\mathrm{Wih}^{*} \mathrm{v}\right)$$

Image Captioning With Attention
生成每个单词的时候，RNN只会注意在自己注意范围内的东西。

输入图像 -> 卷积特征提取 -> RNN + Attention堆叠在这个图像上 -> 一个一个单词生成。

给了权重的特征，给了权重的连接。
软注意，硬注意。

视觉问题回答：
卡车上有哪一种濒危动物？
如果司机↪️的话会去哪里？
这个照片是什么时候拍摄的？
雨伞下面有谁？

多层RNN：

$$h_{t}^{l}=\tanh W^{l} \left( \begin{array}{l}{h_{t}^{l-1}} \\ {h_{t-1}^{l}}\end{array}\right)$$

LSTM：

$$\left( \begin{array}{l}{i} \\ {f} \\ {o} \\ {g}\end{array}\right)=\left( \begin{array}{l}{\operatorname{sigm}} \\ {\operatorname{sigm}} \\ {\operatorname{sigm}} \\ {\operatorname{sigm}} \\ {\tanh }\end{array}\right) W^{l} \left( \begin{array}{c}{h_{t}^{l-1}} \\ {h_{t-1}^{l}}\end{array}\right)$$

$$c_{t}^{l}=f \odot c_{t-1}^{l}+i \odot g$$

$$h_{t}^{l}=o \odot \tanh \left(c_{t}^{l}\right)$$

Vanilla RNN梯度流：

$$\begin{aligned} h_{t} &=\tanh \left(W_{h h} h_{t-1}+W_{x h} x_{t}\right) \\ &=\tanh \left(\left(W_{h h} \quad W_{h x}\right) \left( \begin{array}{c}{h_{t-1}} \\ {x_{t}}\end{array}\right)\right) \\ &=\tanh \left(W \left( \begin{array}{c}{h_{t-1}} \\ {x_{t}}\end{array}\right)\right) \end{aligned}$$

就是把两个W堆叠在一起。

从上层流到下层这样子。
最大特征值>1，梯度爆掉。所以要裁剪掉这个梯度：如果这个梯度的norm太大的话就要成比例缩放。
最大特征值<1，梯度消失。这就要改变RNN的结构了

Vanilla RNN：
$$h_{t}=\tanh \left(W \left( \begin{array}{c}{h_{t-1}} \\ {x_{t}}\end{array}\right)\right)$$

长短时间记忆，LSTM：
$$\left( \begin{array}{l}{i} \\ {f} \\ {o} \\ {g}\end{array}\right)=\left( \begin{array}{l}{\operatorname{sigm}} \\ {\operatorname{sigm}} \\ {\operatorname{sigm}} \\ {\operatorname{sigm}} \\ {\tanh }\end{array}\right) W^{l} \left( \begin{array}{c}{h_{t}^{l-1}} \\ {h_{t-1}^{l}}\end{array}\right)$$

$$c_{t}^{l}=f \odot c_{t-1}^{l}+i \odot g$$

$$h_{t}^{l}=o \odot \tanh \left(c_{t}^{l}\right)$$

f: 遗忘门，告诉要不要擦掉这个单元
i: 输入门，决定要不要写道这个cell里面
g: 门门，要往这个cell里面写多少数据
o: 输出门，这个门的输出是多少。

门门的激活函数是tanh，其他门都是sigmoid。

无法解释的梯度流。
和ResNet很像
中间还有一个Highway Network

$$\begin{aligned} g &=T\left(x, W_{T}\right) \\ y &=g \odot H\left(x, W_{H}\right)+(1-g) \odot x \end{aligned}$$

其他RNN的变体：

GRU
$$\begin{aligned} r_{t} &=\sigma\left(W_{x r} x_{t}+W_{h r} h_{t-1}+b_{r}\right) \\ z_{t} &=\sigma\left(W_{x z} x_{t}+W_{h z} h_{t-1}+b_{z}\right) \\ \tilde{h}_{t} &=\tanh \left(W_{x h} x_{t}+W_{h h}\left(r_{t} \odot h_{t-1}\right)+b_{h}\right) \\ h_{t} &=z_{t} \odot h_{t-1}+\left(1-z_{t}\right) \odot \tilde{h}_{t} \end{aligned}
$$

总结：

- RNN引入了网络的很多灵活性
- Vanilla RNN很简单但是效果不咋地
- 一般用LSTM和GRU，他们的相互作用可以改进梯度流
- 向后传播梯度RNN有可能会爆炸或者消失。
- 爆炸的话可以使用梯度裁剪的方法进行控制
- 消失的话可以通过添加额外的交互来实现（比如用LSTM）
- 更好的架构和更简单的结构都是现在研究的热点问题
- 更好的理解理论和实验很必要。

## 检测和分割

CV任务：

- 像素级别语义分割
- 分类和定位
- 物体检测
- 实例分割

## 像素级别语义分割

不考虑实例之间的差异，只考虑像素的差异。

- 思路：滑动窗口法
  问题：性能太差了，在特征之间重叠的部分没有被重复使用。

- 全卷积：
  问题：在原图上面做卷积成本太高了qwq
  所以设计一串卷积层，包括下采样和上采样层。

输入3 * H * W
- 高分辨率：D1 * H/2 * W/2
- 中分辨率：D2 * H/4 * W/4
- 低分辨率：D3 * H/4 * H/4
- 中分辨率：D2 * H/4 * W/4
- 高分辨率：D1 * H/2 * W/2
预测： H * W

下采样： 有stride的卷积，池化
上采样： 反池化，有stride的转置卷积

下采样是取一个卷积核的最大值，上采样的话就是插值方法了，有最近邻插值法，也有只把左上角的设定为输入的值，其他都为0

最大池化层的话，每个元素都是卷积核的最大值。最大反池化的话需要记录最大值的位置。下采样和上采样是一一对应的。

然后学习反卷积把qwq。输入是4x4，输出也是4x4。
平常的stride为1，padding为1，3x3的卷积。

如果下采样的话，输入为4x4，输出为2x2。

输入为2x2的话，进行3x3的转置卷积，stride为2，padding为1。

输入：a,b
滤波器：x,y,z
输出：ax, ay, az+bx, by, bz

反正就是转置卷积
正常卷积，比如输入是2x6, 卷积展开成乘法是6x1，输出是2x1
转置卷积，输入是6x2, 反卷积展开成乘法是2x1，输出是6x1

## 分类和定位问题

思路：把定位问题当做一个回归问题来解决。

框的坐标：x,y,w,h。

分类标签：使用softmax loss来作为标签的输出。
输出框：使用L2 loss来作为损失函数。

所以是一个多任务的loss，两个loss加起来是总的loss。

通常使用已经训练好的ImageNet模型来迁移学习。

另外也有人类姿态估计。表示14个关节的位置。也是分别计算这14个点的L2 loss，最后加起来。

## 物体检测问题

PASCAL VOC。平均平均精度。
物体检测能够当做一个回归问题吗？这个变量也太多了。

所以就用滑动窗口来解决。
问题：需要把很多位置和尺度都放到CNN里面，计算成本太大了。

Region Proposals
两步法，第一步先能够找到可能包含物体的框。
相对快一点，CPU都能够在几秒完成。

### R-CNN：
先使用提出的方法生成Roi。
然后扭曲图像区域。
把每个区域都通过卷积网络。
最后用SVM来分类。
还有Bbox正则。

特别设定得到的训练目标：
- 使用softmax分类器fine-tune的网络，对数损失。
- 训练后的线性SVM分类器，hinge loss
- 训练后的bounding-box 回归（最小二乘）

训练速度很慢，用了很多磁盘空间

处理很慢，每个图片47秒。
这个问题在SPPnet中得到修复

# Fast R-CNN
整幅图像通过卷积网络
生成RoI，
roi pooling层
然后是全连接层
softmax分类器+线性分类器回归bbox
最后也是多任务损失。log loss + smooth l1 loss。

# Faster R-CNN

RoI Pooling。
高分辨率图像输入3x640x480，然后区域提出。
通过CNN
高分辨率特征：512x20x15
投影的特征：512x18x8，随着不同的proposal变化。
分配到7x7的网格当中。最大池化每个cell
对每一个proposal来说，最后roi conv的特征是512x7x7
全连接层：512x7x7

训练时间，测试时间都很短。
分别测试了单任务和多任务。
可以看到fast rcnn当中的region proposal占用了大部分的时间。

## Faster R-CNN
让CNN来做proposal：
插入Region Proposal Network层，来预测特征的proposal。

这里有4个loss

1. RPN能不能分割出目标？能或者不能。
2. RPN回归的框的坐标
3. 最后的分类分数（物体检测）
4. 最后的框的坐标

Faster R-CNN的测试时间更短0.2秒。

## 没有proposal的检测

Yolo/SSD

SSD:
单个shot，多框检测。
输入图像3xHxW
输出图像分成7x7 grid

image（塑造）一系列base boxes，在每个网格的中心，这里的B=3

在这些框当中，回归5个数字：dx,dy,dh,dw和confidence。
预测分数对这C个分类
背景也作为一个分类。

最后的输出为7x7x(5xB + C)

物体检测：

- 基础网络：
  - VGG16
  - ResNet-101
  - Inception V2,V3
  - Inception
  - ResNet
  - MobileNet 

- 目标检测的结构
  - Faster R-CNN
  - R-FCN
  - SSD

图像大小： Region Proposals

折中: Faster R-CNN 慢一点但是更精确
SSD快一点但不那么精确。

另外，稠密标记：物体检测+标记(不看了看不懂qwq)

## 实例分割

两只狗之间怎么分割开？

### Mask R-CNN

输入图像 -> CNN -> Roi对齐 -> Conv -> Conv -> 为每一个分类预测一个mask。

结果超级好qwq，给大佬跪了。

Mask R-CNN也能够做姿态。

输入图像 -> CNN -> Roi对齐 -> Conv -> Conv -> 为每一个分类预测一个mask。

在Roi对齐后面的那个卷积层里面，进行类别分类，有C个参数。每个框都有4*C个参数。然后关节坐标。

总结一下，语义分割的话没有物体，只有像素。
分类和定位的话是单个物体
物体检测和实例分割的话是针对多个物体。

## 下一节：看到CNN的特征，深梦+风格转换。

想要知道卷积层当中的特征长什么样子。

第一层：可视化滤波器。
第一层权重：16x3x7x7
第二层：20x16x7x7
第三层：20x20x7x7

第一层因为有三个通道所以是彩色的。
在更高的层当中能够看到，但是看不出啥东西qwq。

最后一层是4096的向量，把这个网络运行很多图片，收集这些特征向量。

在特征空间测试图像。

通过降维技术能够把4096维的向量降成2维。
简单的方法：PCA
复杂一点的：t-SNE。

conv5的特征是一个128x13x13的向量，我们把他看成128个13x13组成的灰度图像。
然后最白的地方就是权重最强的地方。

最大激活patch。比如conv5是128x13x13的，选择17个通道。通过这个网络运行许多图片，把这些相应最大的图片输出，就能够看出这些权重对应了什么特征了。

遮挡实验：遮挡图像的一部分，然后在遮挡的地方看热力图。

凸显图：如何告诉哪些像素对分类有用？

对每个图像像素，计算没有归一化的分类分数，去绝对值，并且在RGB通道上叠加。

无监督的分割：使用这个凸显图。

通过导向的向后传播来找到中间特征：

原来的ReLU计算backprop的时候是根据前向传播的值是正的还是负的来决定向后传播不传播的。
现在是直接根据梯度的值来决定到底向后传不传。

这样只记录大于0的权重的话，输出的图会好看一点。

可视化CNN的特征：
梯度下降法：对图像求这一个神经元的响应。
梯度上升法：创造一个假的图像，能够最大的激活你的神经元。

$$I^{*}=\arg \max _{\mathrm{I}}[\mathrm{f}(\mathrm{I})]+[\mathrm{R}(\mathrm{I})]$$

神经元的值，和自然图像正则项。

生成假图的算法：
- 首先是生成一张图，全部是0
- 重复以下操作
  1. 前向传递这张图，算当前的梯度
  2. 后向来得到对应像素点的神经元的梯度
  3. 更新这张图

简单的正则项：对新的图像进行L2惩罚
更好的正则项：对图像进行L2惩罚，并且周期性的做以下事情：
- 对图像高斯模糊
- 把图像裁剪到最小值为0
- 把图像裁剪到最小梯度为0

同样的方法可以看到其他层的图像。

添加多面可视化能够得到更好的结果。
添加更多细致的正则规则，中心偏置。

对于全连接层的话不在像素空间，而在潜在空间就可以了qwq

从一个随机的图像开始。
选择任意的类别
改变图像并且使得这个相应最大
重复直到这个网络完全被欺骗。

一般的例子，本来是一个🐘，但是被神经网络认为是一个🐨。差别还是不大的，但是放大10倍之后差别就很大了。

## DeepDream： 放大现有的特征

不是创建一个假图，而是试着放大某些层的每个神经元的响应。

算法：
- 选择一张图，循环执行以下操作
  - 前向传递：计算选定的层的激活函数值
  - 设定这个梯度的值等于他的激活值
  - 向后传播：计算图像的梯度
  - 更新图像

等价于：
$$\mathrm{I}^{*}=\arg \max _{\mathrm{I}} \sum_{\mathrm{i}} \mathrm{f}_{\mathrm{i}}(\mathrm{I})^{2}$$

[代码看这里](https://github.com/google/deepdream)：很简单但是有一堆trick：

- 比如使用np.roll(np.roll(...))来使得图像`抖动`。

- 梯度是进行L1正则化的。
- 得到偏差之后会减去平均值，对每个像素进行裁剪

## 特征翻转

给一个CNN特征向量，找到一个新的图像，使得：
- 匹配这个特征向量
- 看起来自然（加入图像的先验正则）

$$\mathbf{x}^{*}=\underset{\mathbf{x} \in \mathbb{R}^{H \times W \times C}{\operatorname{argmin}}}{\operatorname{argmin}} \ell\left(\Phi(\mathbf{x}), \widehat{\Phi_{0}}\right)+\lambda \mathcal{R}(\mathbf{x})$$

$$\Phi(x)$$是新图像上的特征。
$$\Phi_0$$是给定的图像特征。

$$\ell\left(\Phi(\mathbf{x}), \Phi_{0}\right)=\left \vert \Phi(\mathbf{x})-\Phi_{0}\right \vert ^{2}$$

$$\mathcal{R}_{V^{\beta}}(\mathbf{x})=\sum_{i, j}\left(\left(x_{i, j+1}-x_{i j}\right)^{2}+\left(x_{i+1, j}-x_{i j}\right)^{2}\right)^{\frac{\beta}{2}}$$

这个是正则项，beta作为指数是鼓励空间更加平滑。对x和y两个方向。

### 纹理造假：

给一小块纹理，能生成更大的纹理吗？

按照扫描次序生成像素，从输入当中复制最近的邻居已经生成的像素。这个就有点不自然了qwq

神经网络生成假纹理：
gram矩阵。每层的CNN都是CxHxW的向量。HxW个网格，C维的向量

外积两个C维的向量，CxC矩阵，测量共生关系，得到所有的高度和宽度对，得到的这个就是克矩阵。

把特征向量从CxHxW变成CxHW，计算G=FF^T

步骤：
1. 预训练一个CNN ImageNet
2. 把这个纹理前向通过CNN，记录每层的激活值，层i的特征映射的形状为C_i * H_i * W_i
3. 每层计算克矩阵，得到输出特征的乘积

  $$G_{i j}^{l}=\sum_{k} F_{i k}^{l} F_{j k}^{l}\left(\text { shape } \mathrm{C}_{\mathrm{i}} \times \mathrm{C}_{\mathrm{i}}\right)$$

4. 白噪声初始化生成图像
5. 把生成的这张图放到克矩阵当中，计算每层的克矩阵
6. 计算loss：克矩阵的加权的L2距离

  $$E_{l}=\frac{1}{4 N_{l}^{2} M_{l}^{2}} \sum_{i, j}\left(G_{i j}^{l}-\hat{G}_{i j}^{l}\right)^{2}$$

  $$\mathcal{L}(\vec{x}, \hat{\vec{x}})=\sum_{l=0}^{L} w_{l} E_{l}$$

7. Backprop得到图像梯度。
8. 梯度作用在图像上
9. goto 5

重构高层的纹理，恢复了更大的输入纹理。

纹理就是艺术品！
风格迁移！特征+克重构。

内容图片：给出内容目标
风格图片：给出风格目标

输出图像初始是噪声。

更大的权重给内容的话，会更像内容……好吧是废话。
风格图片的大小会影响特征的种类。
多种特征可以生成混合风格的图像。

问题：风格转换需要在VGG上进行大量的前向、后项传播，非常慢。

解决办法：训练另一个神经网络，帮助我们进行风格迁移。

快速风格迁移：
1. 对每种风格训练一个前馈网络
2. 和以前一样，使用预训练的CNN计算相同的loss
3. 训练过后，使用单个前向传递进行图片的风格化。

[地址在这里](https://github.com/jcjohnson/fast-neural-style)

一个网络，多种风格。

### 总结：有很多方式能够表示CNN的权重

- 激活函数：最近邻，降维，最大patch，遮挡看相应热力图
- 梯度：显著性映射，类别可视化，愚弄神经网络的图，特征翻转。
- 深梦，风格迁移。
