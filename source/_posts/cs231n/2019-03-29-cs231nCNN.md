---
layout: post
title:  "cs231n CNN"
date:   2019-03-29 14:28:46 +0800
categories: AI
---

<script type="text/javascript" async
  src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/MathJax.js?config=TeX-MML-AM_CHTML">
</script>

## 0.1. ç»ˆäºå¼€å§‹CNNäº†

å¥½åƒæœ€è¿‘ä¸‰å·¨å¤´æ˜¯æ‹¿äº†å›¾çµå¥–æ˜¯ä¹ˆï¼Œè™½ç„¶æˆ‘ä¹Ÿä¸æ€ä¹ˆå…³å¿ƒqwq

CNNæ˜ç¡®å‡è®¾è¾“å…¥æ˜¯å›¾åƒï¼Œè¿™æ ·èƒ½å¤Ÿæ˜¾è‘—å‡å°‘å‚æ•°çš„æ•°é‡ã€‚

ä¸ºä»€ä¹ˆä¼šæƒ³åˆ°ç”¨cnnï¼Ÿå¸¸è§„ç¥ç»ç½‘ç»œæ— æ³•æ‰©å±•åˆ°å®Œæ•´å›¾åƒï¼Œå…¨è¿æ¥çš„è¯ä¸èƒ½æ‰©å±•åˆ°æ›´å¤§çš„å›¾åƒï¼Œå¹¶ä¸”å¾ˆå¿«ä¼šå¯¼è‡´è¿‡æ‹Ÿåˆã€‚

ç»å…¸ç»“æ„ï¼šè¾“å…¥ --> å·ç§¯ --> ReLU --> Pool --> FC

è¾“å…¥ï¼š 32x32x3
å·ç§¯å±‚ï¼š 32x32x12ï¼Œä½¿ç”¨12ä¸ªfilter
ReLUï¼š å…ƒç´ çº§åˆ«çš„max(0,x)
Pool: æœ€å¤§å€¼Poolingï¼Œè¾“å‡ºä¸º16x16x12
FCï¼š1x1x10ï¼Œä»£è¡¨è¾“å‡ºçš„ç±»åˆ«

æŸäº›å±‚åŒ…å«å‚æ•°ï¼Œä½†æ˜¯æœ‰äº›å±‚ä¸å«ã€‚
æ¯ä¸ªå›¾å±‚éƒ½æ¥å—è¾“å…¥3Dä½“ç§¯ï¼Œå¹¶é€šè¿‡å¯å¾®åˆ†å‡½æ•°å°†å…¶è½¬æ¢ä¸ºè¾“å‡º3Dä½“ç§¯ã€‚
æ¯ä¸ªå±‚å¯èƒ½æœ‰ä¹Ÿå¯èƒ½æ²¡æœ‰å‚æ•°ï¼ˆä¾‹å¦‚CONV / FCæœ‰å‚æ•°ï¼ŒRELU / POOLæ²¡æœ‰å‚æ•°ï¼‰
æ¯ä¸ªå±‚å¯èƒ½æœ‰ä¹Ÿå¯èƒ½æ²¡æœ‰é¢å¤–çš„è¶…å‚æ•°ï¼ˆä¾‹å¦‚CONV / FC / POOLæœ‰ï¼ŒRELUæ²¡æœ‰ï¼‰

æ„Ÿå—é‡å¾ˆå…³é”®ï¼šå°±æ˜¯æ»¤æ³¢å™¨å¤§å°

ç¤ºä¾‹1.ä¾‹å¦‚ï¼Œå‡è®¾è¾“å…¥æ•°æ®å…·æœ‰[32x32x3]çš„å¤§å°ï¼ˆä¾‹å¦‚RGB CIFAR-10å›¾åƒï¼‰ã€‚å¦‚æœæ„Ÿå—é‡ï¼ˆæˆ–æ»¤æ³¢å™¨å¤§å°ï¼‰æ˜¯5x5ï¼Œé‚£ä¹ˆConvå±‚ä¸­çš„æ¯ä¸ªç¥ç»å…ƒå°†å…·æœ‰è¾“å…¥ä½“ç§¯ä¸­[5x5x3]åŒºåŸŸçš„æƒé‡ï¼Œæ€»å…±5 * 5 * 3 = 75ä¸ªæƒé‡ï¼ˆå’Œ+1ï¼‰biaså‚æ•°ï¼‰ã€‚è¯·æ³¨æ„ï¼Œæ²¿æ·±åº¦è½´çš„è¿æ¥èŒƒå›´å¿…é¡»ä¸º3ï¼Œå› ä¸ºè¿™æ˜¯è¾“å…¥æ•°æ®çš„æ·±åº¦ã€‚ä¹Ÿå°±æ˜¯ä¸€æ¬¡å·ç§¯æ“ä½œï¼Œåªæœ‰76ä¸ªå‚æ•°éœ€è¦è°ƒæ•´ï¼Œå¤§å¤§å‡å°‘äº†å‚æ•°çš„æ•°ç›®ã€‚

ç¤ºä¾‹2.å‡è®¾è¾“å…¥æ•°æ®å¤§å°ä¸º[16x16x20]ã€‚ç„¶åä½¿ç”¨3x3çš„ç¤ºä¾‹æ„Ÿå—åŒºåŸŸå¤§å°ï¼ŒConvå±‚ä¸­çš„æ¯ä¸ªç¥ç»å…ƒç°åœ¨å°†å…·æœ‰åˆ°è¾“å…¥éŸ³é‡çš„æ€»å…±3 * 3 * 20 = 180ä¸ªè¿æ¥ã€‚è¯·æ³¨æ„ï¼ŒåŒæ ·ï¼Œè¿æ¥åœ¨ç©ºé—´ä¸­æ˜¯å±€éƒ¨çš„ï¼ˆä¾‹å¦‚3x3ï¼‰ï¼Œä½†åœ¨è¾“å…¥æ·±åº¦ï¼ˆ20ï¼‰ä¸Šæ˜¯å®Œæ•´çš„ã€‚

å·ç§¯å±‚çš„è¶…å‚æ•°ï¼š

æ·±åº¦depthï¼Œæ­¥ä¼strideå’Œé›¶å¡«å……å¤§å°

è¾“å…¥Wï¼Œæ„Ÿå—é‡å¤§å°Fï¼Œé›¶paddingå¤§å°Pï¼Œstrideå¤§å°Sã€‚
é€‚åˆçš„ç¥ç»å…ƒæ•°é‡ä¸ºï¼š$$(W-F+2 P) / S+1$$

å¦‚æœè®¾ç½®ä¸º3.14çš„è¯ï¼Œ

å¦‚æœä¸€ä¸ªç‰¹å¾å¯¹äºåœ¨æŸä¸ªç©ºé—´ä½ç½®ï¼ˆxï¼Œyï¼‰è®¡ç®—æ˜¯æœ‰ç”¨çš„ï¼Œé‚£ä¹ˆåœ¨ä¸åŒä½ç½®è®¡ç®—å®ƒä¹Ÿåº”è¯¥æ˜¯æœ‰ç”¨çš„ï¼ˆx2 ï¼Œy2ï¼‰ä¸Šã€‚

å°†å•ä¸ªäºŒç»´æ·±åº¦åˆ‡ç‰‡è¡¨ç¤ºä¸ºæ·±åº¦åˆ‡ç‰‡ï¼ˆä¾‹å¦‚ï¼Œå¤§å°ä¸º[55x55x96]çš„ä½“ç§¯å…·æœ‰96ä¸ªæ·±åº¦åˆ‡ç‰‡ï¼Œæ¯ä¸ªåˆ‡ç‰‡å¤§å°ä¸º[55x55]ï¼‰
åœ¨åå‘ä¼ æ’­æœŸé—´çš„å®è·µä¸­ï¼Œä½“ç§¯ä¸­çš„æ¯ä¸ªç¥ç»å…ƒå°†è®¡ç®—å…¶æƒé‡çš„æ¢¯åº¦ï¼Œä½†æ˜¯è¿™äº›æ¢¯åº¦å°†åœ¨æ¯ä¸ªæ·±åº¦åˆ‡ç‰‡ä¸Šç›¸åŠ å¹¶ä¸”ä»…æ›´æ–°æ¯ä¸ªåˆ‡ç‰‡çš„å•ä¸ªæƒé‡é›†ã€‚

è¯·æ³¨æ„ï¼Œå¦‚æœå•ä¸ªæ·±åº¦åˆ‡ç‰‡ä¸­çš„æ‰€æœ‰ç¥ç»å…ƒéƒ½ä½¿ç”¨ç›¸åŒçš„æƒé‡å‘é‡ï¼Œé‚£ä¹ˆCONVå±‚çš„æ­£å‘é€šé“å¯ä»¥åœ¨æ¯ä¸ªæ·±åº¦åˆ‡ç‰‡ä¸­è®¡ç®—ä¸ºç¥ç»å…ƒæƒé‡ä¸è¾“å…¥ä½“ç§¯çš„å·ç§¯ï¼ˆå› æ­¤åç§°ï¼šå·ç§¯å±‚ï¼‰ã€‚ è¿™å°±æ˜¯ä¸ºä»€ä¹ˆé€šå¸¸å°†æƒé‡é›†ç§°ä¸ºä¸è¾“å…¥å·ç§¯çš„è¿‡æ»¤å™¨ï¼ˆæˆ–å†…æ ¸ï¼‰ã€‚

è¿™æ˜¯é’ˆå¯¹ç‰¹å¾çš„ä½ç½®å’Œç‰¹å¾æœ¬èº«æ²¡æœ‰å¼ºå…³ç³»ï¼Œæ‰æœ‰è¿™ä¸ªå‡è®¾ã€‚æœ‰æ—¶å€™ä¼šå¤±æ•ˆï¼Œæ¯”å¦‚å¤´å‘å°±æ˜¯åº”è¯¥å‡ºç°åœ¨å¤´ä¸Šï¼Œçœ¼ç›ï¼Œå˜´éƒ½åº”è¯¥å‡ºç°åœ¨ä»–çš„ä½ç½®ä¸Šã€‚ 

åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œé€šå¸¸æ”¾æ¾å‚æ•°å…±äº«æ–¹æ¡ˆï¼Œè€Œæ˜¯ç®€å•åœ°å°†è¯¥å±‚ç§°ä¸ºå±€éƒ¨è¿æ¥å±‚ã€‚

### 0.1.1. Numpyä¾‹å­è¡¨ç¤º

è®¾ä¸€ä¸ªVolumeä¸º`X[x,y,d]`
åˆ†åˆ«è¡¨ç¤ºé•¿ï¼Œå®½å’Œæ·±åº¦ã€‚
ä¸€ä¸ªåƒç´ çš„æ·±åº¦fiberå¯ä»¥è¢«æè¿°ä¸º`X[x,y,:]`
ä¸€ä¸ªé¢å¯ä»¥è¢«æè¿°ä¸º`X[:,:,d]`

å˜æˆçŸ©é˜µä¹˜æ³•

CNNé‡Œé¢ç”¨çš„æ˜¯ç‚¹ä¹˜ï¼Œè€Œä¸æ˜¯å·ç§¯æ“ä½œã€‚

`im2col`è¿™ä¸ªæ˜¯å•¥ï¼Ÿè¾“å…¥å›¾åƒä¸­çš„å±€éƒ¨åŒºåŸŸä¼¸å±•æˆåˆ—çš„æ“ä½œæ˜¯`im2col`ã€‚

è¾“å…¥æ˜¯`227x227x3`ï¼Œç”¨`11x11x3`çš„æ»¤æ³¢å™¨ï¼Œç„¶åstrideæ˜¯4ï¼ŒKæ˜¯(227-11)/4+1=55ï¼Œ
è¾“å‡º`X_col`æ˜¯[363,3025]ï¼Œä½†æ˜¯ç”±äºæ„Ÿå—é‡é‡å ï¼Œå¯¼è‡´äº†å¾ˆå¤šåŒºåŸŸéƒ½æ˜¯é‡å¤çš„ã€‚

æƒé‡`W_row`åº”è¯¥æ˜¯[96x363]

ç»“æœåº”è¯¥æ˜¯`np.dot(W_row, X_col)`

ä½†æ˜¯ç”¨çš„å†…å­˜å¤ªå¤šäº†ï¼Œä½¿ç”¨BLASå¯ä»¥ã€‚

1x1å·ç§¯ã€‚å¦å¤–ï¼Œä¸€äº›è®ºæ–‡ä½¿ç”¨1x1å·ç§¯ï¼Œé¦–å…ˆç”±Network in Networkè°ƒæŸ¥ã€‚æœ‰äº›äººæœ€åˆå¾ˆéš¾çœ‹åˆ°1x1å·ç§¯ï¼Œç‰¹åˆ«æ˜¯å½“å®ƒä»¬æ¥è‡ªä¿¡å·å¤„ç†èƒŒæ™¯æ—¶ã€‚é€šå¸¸ä¿¡å·æ˜¯äºŒç»´çš„ï¼Œå› æ­¤1x1å·ç§¯æ²¡æœ‰æ„ä¹‰ï¼ˆå®ƒåªæ˜¯é€ç‚¹ç¼©æ”¾ï¼‰ã€‚ç„¶è€Œï¼Œåœ¨ConvNetsä¸­å¹¶éå¦‚æ­¤ï¼Œå› ä¸ºå¿…é¡»è®°ä½æˆ‘ä»¬åœ¨ä¸‰ç»´ä½“ç§¯ä¸Šæ“ä½œï¼Œå¹¶ä¸”æ»¤æ³¢å™¨æ€»æ˜¯å»¶ä¼¸åˆ°è¾“å…¥ä½“ç§¯çš„æ•´ä¸ªæ·±åº¦ã€‚ä¾‹å¦‚ï¼Œå¦‚æœè¾“å…¥ä¸º[32x32x3]ï¼Œåˆ™æ‰§è¡Œ1x1å·ç§¯å°†æœ‰æ•ˆåœ°æ‰§è¡Œä¸‰ç»´ç‚¹ç§¯ï¼ˆå› ä¸ºè¾“å…¥æ·±åº¦ä¸º3ä¸ªé€šé“ï¼‰ã€‚

æ‰©å¼ çš„å·ç§¯ã€‚æœ€è¿‘çš„ä¸€é¡¹å‘å±•ï¼ˆä¾‹å¦‚å‚è§Fisher Yuå’ŒVladlen Koltunæ’°å†™çš„è®ºæ–‡ï¼‰æ˜¯åœ¨CONVå±‚å¼•å…¥å¦ä¸€ä¸ªç§°ä¸ºæ‰©å¼ çš„è¶…å‚æ•°ã€‚åˆ°ç›®å‰ä¸ºæ­¢ï¼Œæˆ‘ä»¬åªè®¨è®ºäº†è¿ç»­çš„CONVè¿‡æ»¤å™¨ã€‚ä½†æ˜¯ï¼Œå¯ä»¥ä½¿æ¯ä¸ªå•å…ƒæ ¼ä¹‹é—´æœ‰ç©ºæ ¼çš„è¿‡æ»¤å™¨ç§°ä¸ºæ‰©å¼ ã€‚ä½œä¸ºç¤ºä¾‹ï¼Œåœ¨ä¸€ä¸ªç»´åº¦ä¸­ï¼Œå¤§å°ä¸º3çš„æ»¤æ³¢å™¨wå°†åœ¨è¾“å…¥xä¸Šè®¡ç®—ä»¥ä¸‹ï¼šw [0] * x [0] + w [1] * x [1] + w [2] * x [2] ã€‚è¿™æ˜¯0çš„æ‰©å¼ ã€‚å¯¹äºæ‰©å¼ 1ï¼Œæ»¤æ³¢å™¨å°†æ”¹ä¸ºè®¡ç®—w [0] * x [0] + w [1] * x [2] + w [2] * x [4];æ¢å¥è¯è¯´ï¼Œåº”ç”¨ç¨‹åºä¹‹é—´å­˜åœ¨1çš„å·®è·ã€‚è¿™åœ¨æŸäº›è®¾ç½®ä¸­éå¸¸æœ‰ç”¨ï¼Œå¯ä»¥ä¸0æ‰©å±•æ»¤æ³¢å™¨ç»“åˆä½¿ç”¨ï¼Œå› ä¸ºå®ƒå…è®¸æ‚¨ä½¿ç”¨æ›´å°‘çš„å±‚æ›´åŠ ç§¯æåœ°åˆå¹¶è¾“å…¥ä¸­çš„ç©ºé—´ä¿¡æ¯ã€‚ä¾‹å¦‚ï¼Œå¦‚æœä½ å°†ä¸¤ä¸ª3x3 CONVå±‚å †å åœ¨ä¸€èµ·ï¼Œé‚£ä¹ˆä½ å¯ä»¥è¯´æœè‡ªå·±ç¬¬äºŒå±‚çš„ç¥ç»å…ƒæ˜¯è¾“å…¥çš„5x5è¡¥ä¸çš„å‡½æ•°ï¼ˆæˆ‘ä»¬å¯ä»¥è¯´è¿™äº›ç¥ç»å…ƒçš„æœ‰æ•ˆæ„Ÿå—é‡æ˜¯5Ã—5ï¼‰ã€‚å¦‚æœæˆ‘ä»¬ä½¿ç”¨æ‰©å¼ çš„å·ç§¯ï¼Œé‚£ä¹ˆè¿™ä¸ªæœ‰æ•ˆçš„æ„Ÿå—é‡ä¼šå¢é•¿å¾—æ›´å¿«ã€‚

æ“ä½œï¼š

è¾“å…¥ï¼šW1,H1,D1
ä¸¤ä¸ªè¶…å‚æ•°ï¼šFï¼ŒS

è¾“å‡ºï¼š 
$$\begin{aligned} W_{2} &=\left(W_{1}-F\right) / S+1 \\ H_{2} &=\left(H_{1}-F\right) / S+1 \\ D_{2} &=D_{1} \end{aligned}$$

ä½¿ç”¨ä¾‹å¦‚å˜åˆ†è‡ªåŠ¨ç¼–ç å™¨ï¼ˆVAEï¼‰æˆ–ç”Ÿæˆæ€§å¯¹æŠ—ç½‘ç»œï¼ˆGANï¼‰å¯ä»¥ä»£æ›¿æ± åŒ–å±‚ã€‚

æœªæ¥çš„æ¶æ„å¾ˆå¯èƒ½åªâ€‹â€‹æœ‰å¾ˆå°‘ç”šè‡³æ²¡æœ‰æ± åŒ–å±‚ã€‚

Normalizationå·²ç»è¢«æŠ›å¼ƒï¼Œåœ¨CNNå½“ä¸­æ²¡å•¥ç”¨

å…¨è¿æ¥å±‚

æŠŠå…¨è¿æ¥å±‚å˜æˆå·ç§¯å±‚ï¼šFCå’ŒCONVå±‚ä¹‹é—´çš„å”¯ä¸€åŒºåˆ«æ˜¯CONVå±‚ä¸­çš„ç¥ç»å…ƒä»…è¿æ¥åˆ°è¾“å…¥ä¸­çš„å±€éƒ¨åŒºåŸŸï¼Œå¹¶ä¸”CONVå·ä¸­çš„è®¸å¤šç¥ç»å…ƒå…±äº«å‚æ•°ã€‚ç„¶è€Œï¼Œä¸¤å±‚ä¸­çš„ç¥ç»å…ƒä»ç„¶è®¡ç®—ç‚¹ç§¯ï¼Œå› æ­¤å®ƒä»¬çš„åŠŸèƒ½å½¢å¼æ˜¯ç›¸åŒçš„ã€‚å› æ­¤ï¼Œäº‹å®è¯æ˜ï¼Œå¯ä»¥åœ¨FCå’ŒCONVå±‚ä¹‹é—´è¿›è¡Œè½¬æ¢ï¼š

å¯¹äºä»»ä½•CONVå±‚ï¼Œéƒ½æœ‰ä¸€ä¸ªå®ç°ç›¸åŒå‰å‘åŠŸèƒ½çš„FCå±‚ã€‚ æƒé‡çŸ©é˜µå°†æ˜¯å¤§çš„çŸ©é˜µï¼Œé™¤äº†åœ¨æŸäº›å—ï¼ˆç”±äºæœ¬åœ°è¿æ¥ï¼‰ä¹‹å¤–ï¼Œå…¶å¤§éƒ¨åˆ†ä¸ºé›¶ï¼Œå…¶ä¸­è®¸å¤šå—ä¸­çš„æƒé‡ç›¸ç­‰ï¼ˆç”±äºå‚æ•°å…±äº«ï¼‰ã€‚
ç›¸åï¼Œä»»ä½•FCå±‚éƒ½å¯ä»¥è½¬æ¢ä¸ºCONVå±‚ã€‚ ä¾‹å¦‚ï¼Œè§‚å¯Ÿä¸€äº›å°ºå¯¸ä¸º7Ã—7Ã—512çš„è¾“å…¥ä½“ç§¯çš„K = 4096çš„FCå±‚å¯ä»¥ç­‰æ•ˆåœ°è¡¨ç¤ºä¸ºå…·æœ‰F = 7ï¼ŒP = 0ï¼ŒS = 1ï¼ŒK = 4096çš„CONVå±‚ã€‚ æ¢å¥è¯è¯´ï¼Œæˆ‘ä»¬å°†æ»¤æ³¢å™¨å¤§å°è®¾ç½®ä¸ºè¾“å…¥éŸ³é‡çš„å¤§å°ï¼Œå› æ­¤è¾“å‡ºå°†åªæ˜¯1Ã—1Ã—4096ï¼Œå› ä¸ºåªæœ‰ä¸€ä¸ªæ·±åº¦åˆ—â€œé€‚åˆâ€è¾“å…¥éŸ³é‡ï¼Œä»è€Œå¾—åˆ°ç›¸åŒçš„ç»“æœ æœ€åˆçš„FCå±‚ã€‚

ä¹Ÿå°±æ˜¯æ•´ä¸ªå¤§çš„å›¾åƒèŒƒå›´ä½œä¸ºå·ç§¯æ ¸ã€‚

### 0.1.2. å·ç§¯å±‚ç»“æ„ï¼š

`INPUT -> [[CONV -> RELU]*N -> POOL?]*M -> [FC -> RELU]*K -> FC`

`*`ä»£è¡¨é‡å¤ï¼Œ`POOL?`è¡¨ç¤ºæ± åŒ–å±‚å¯æœ‰å¯æ— ï¼Œ

ä¸€èˆ¬æƒ…å†µä¸‹ï¼ŒN>=0, N<=3, M>=0, K>=0, K<3

å¸¸ç”¨ç»“æ„ï¼š

- `INPUT -> FC`ï¼Œè¡¨ç¤ºçº¿æ€§åˆ†ç±»å™¨ï¼Œè¿™é‡Œ`N=M=K=0`
- `INPUT -> CONV -> RELU -> FC`
- `INPUT -> [CONV -> RELU -> POOL]*2 -> FC -> RELU -> FC`,è¿™é‡Œæ¯ä¸ªæ± åŒ–å±‚ä¹‹é—´åªæœ‰ä¸€ä¸ªå·ç§¯å±‚
- `INPUT -> [CONV -> RELU -> CONV -> RELU -> POOL]*3 -> [FC -> RELU]*2 -> FC`ï¼Œè¿™é‡Œåœ¨æ± åŒ–å±‚ä¹‹å‰æœ‰ä¸¤ä¸ªå·ç§¯å±‚ï¼Œå¯¹äºæ›´å¤§çš„æ›´æ·±çš„ç½‘ç»œæ¥è¯´ï¼Œè¿™æ˜¯ä¸ªå¥½çš„æ–¹æ³•ï¼Œå› ä¸ºåœ¨æ± åŒ–ä¹‹å‰ï¼Œå¤šå±‚å †å åœ¨ä¸€èµ·çš„å·ç§¯å±‚å¯ä»¥ç”Ÿæˆæ›´å¤šçš„å¤æ‚ç‰¹å¾ã€‚

å°çš„å·ç§¯æ ¸å †å æˆå¤§æ„Ÿå—é‡çš„å·ç§¯å±‚ï¼šå‡è®¾ä½ å †å 3ä¸ª3x3çš„å·ç§¯å±‚ï¼ˆå½“ç„¶å¼•å…¥éçº¿æ€§ï¼‰ï¼Œåœ¨æ’åˆ—å½“ä¸­ï¼Œç¬¬ä¸€å±‚çš„æ¯ä¸ªç¥ç»å…ƒæœ‰3x3çš„è¾“å…¥è§†é‡ã€‚
æ‰€ä»¥ç¬¬äºŒå±‚å°±æœ‰5x5çš„æ„Ÿå—é‡ã€‚åŒæ ·ï¼Œç¬¬ä¸‰å±‚å°±æœ‰7x7çš„æ„Ÿå—é‡ã€‚ä½†æ˜¯æœ‰è¯¸å¤šç¼ºç‚¹ï¼š

1. æ¯ä¸ªç¥ç»å…ƒéƒ½ä¼šè®¡ç®—ä¸€æ¬¡è¾“å…¥çš„çº¿æ€§å‡½æ•°ï¼Œä¸‰å±‚å·ç§¯çš„è¯åŒ…å«éçº¿æ€§ï¼Œä½¿å¾—å¾—åˆ°ç‰¹å¾çš„éš¾åº¦å¢å¤§ã€‚
2. å‡è®¾æ‰€æœ‰æ•°æ®çš„é€šé“æ•°ä¸ºCï¼Œå¯ä»¥çœ‹åˆ°ï¼Œ7x7çš„å·ç§¯å±‚åŒ…å«$$7*7*C*C=49C^2$$ä¸ªå‚æ•°ï¼Œä½†æ˜¯ä¸‰å±‚å·ç§¯å±‚åŒ…å«çš„å‚æ•°æ•°é‡ä¸ºï¼š$$3*(C*(3*3*C))=27C^2$$ä¸ªå‚æ•°ã€‚ç›´è§‰ä¸Šçœ‹ï¼Œä½¿ç”¨å°çš„æ»¤æ³¢å™¨å †å çš„è‹¥å¹²CONVå±‚ï¼Œè€Œä¸æ˜¯ç›´æ¥ä½¿ç”¨å¤§æ»¤æ³¢å™¨ï¼Œä¸€ä¸ªCONVå±‚ï¼Œèƒ½å¤Ÿè¡¨ç¤ºæ›´åŠ å¼ºå¤§çš„è¾“å…¥ç‰¹å¾ï¼Œè€Œä¸”å…·æœ‰æ›´å°çš„å‚æ•°ã€‚ä½†æ˜¯å®é™…ä¸Šï¼Œå¦‚æœæˆ‘ä»¬æ‰“ç®—è¦è¿›è¡Œåå‘ä¼ æ’­çš„è¯ï¼Œæˆ‘ä»¬å¯èƒ½éœ€è¦æ›´å¤šçš„å†…å­˜æ¥å‚¨å­˜ä¸­é—´è¿‡ç¨‹ã€‚

å€¼å¾—è¯´æ˜çš„ä¸€ç‚¹æ˜¯ï¼Œä¼ ç»Ÿçš„çº¿æ€§åˆ—è¡¨å±‚å·²ç»è¢«æ‰”æ‰äº†qwqã€‚
åƒè°·æ­Œçš„Inceptionæ¶æ„ï¼Œä»¥åŠå½“å‰çš„state-of-artæ®‹å·®ç½‘ç»œéƒ½ç”¨äº†æ›´å¤æ‚çš„ç»“æ„ã€‚

å®é™…ä¸Šæ˜¯ä»€ä¹ˆå¥½å°±ç”¨ä»€ä¹ˆã€‚å¦‚æœä½ è§‰å¾—è¿™ç§æ¶æ„ä»€ä¹ˆçš„å¾ˆéš¾ç†è§£ï¼Œä½ åº”è¯¥å¾ˆé«˜å…´çŸ¥é“ï¼Œ90%ä»¥ä¸Šçš„åº”ç”¨ä½ ä¸éœ€è¦æ‹…å¿ƒè¿™ä¸ªé—®é¢˜ã€‚`ä¸è¦æˆä¸ºä¸€ä¸ªè‹±é›„`ï¼šä½ åº”è¯¥çœ‹é‚£ä¸ªæ¶æ„åœ¨ImageNetä¸Šé¢å¾ˆå¥½ç”¨ï¼Œä¸‹è½½é¢„è®­ç»ƒçš„æ¨¡å‹æœ€åfine-tuneè¿™ä¸ªæ¨¡å‹ä»¥é€‚åº”ä½ çš„æ•°æ®ï¼Œè€Œä¸æ˜¯é’ˆå¯¹ä½ çš„é—®é¢˜æå‡ºä½ æ–°çš„æ¶æ„ã€‚
ï¼ˆè¯´çš„çœŸå¥½qwqï¼‰

### 0.1.3. å±‚å¤§å°åŒ¹é…

ç°åœ¨æˆ‘ä»¬æ²¡æ€ä¹ˆæè¿™ä¸ªCONVçš„è¶…å‚æ•°ï¼Œæˆ‘ä»¬å…ˆæå‡ºä¸€èˆ¬æ€§çš„åŸåˆ™ï¼Œç„¶åè¯¦ç»†è¯´æ˜è¿™ä¸ªé—®é¢˜ã€‚

- è¾“å…¥å±‚ï¼šè¿™ä¸ªå¤§å°åº”è¯¥èƒ½å¤Ÿè¢«2æ•´é™¤å¾ˆå¤šæ¬¡ï¼Œæ¯”å¦‚å¸¸è§çš„å¤§å°æœ‰ï¼š32ï¼Œ64ï¼Œ96ï¼Œ224ï¼Œ384ï¼Œ512.

- å·ç§¯å±‚ï¼šåº”è¯¥ä½¿ç”¨å°çš„å·ç§¯æ ¸ï¼Œæ­¥é•¿è®¾å®šä¸º1ã€‚**éå¸¸å…³é”®**ï¼šè¾“å…¥åº”è¯¥è¡¥å……0ï¼Œä½¿å¾—å·ç§¯å±‚ä¸æ”¹å˜è¾“å…¥çš„ç©ºé—´ç»“æ„ã€‚å½“F=5ï¼ŒP=2æ—¶ï¼Œå¯¹äºä¸€èˆ¬çš„Fæ¥è¯´ï¼ŒP=(F-1)/2çš„è¯å’Œè¾“å…¥æ˜¯ä¸€æ ·çš„ã€‚å¦‚æœå¿…é¡»ä½¿ç”¨å¾ˆå¤§çš„å·ç§¯æ ¸çš„è¯ï¼Œåªæœ‰åœ¨ç¬¬ä¸€å±‚è¿›è¡Œå¦‚æ­¤æ“ä½œæ¯”è¾ƒå¸¸è§ã€‚

- æ± åŒ–å±‚ï¼šä¸€èˆ¬ç”¨2x2çš„æ„Ÿå—é‡ï¼Œä¹Ÿå°±æ˜¯F=2ï¼Œstride=2ã€‚æ³¨æ„åˆ°è¿™å®é™…ä¸Šä¸¢å¼ƒäº†75%çš„è¾“å…¥æ¿€æ´»ç»“æœï¼ˆå› ä¸ºåŒæ—¶å¯¹å®½åº¦å’Œé«˜åº¦è¿›è¡Œä¸‹é‡‡æ ·ï¼‰ã€‚å¦ä¸€ä¸ªä¸å¸¸ç”¨çš„æ˜¯ä½¿ç”¨3x3çš„æ„Ÿå—é‡ï¼Œstrideè¿˜æ˜¯2ï¼Œä½†æ˜¯ä¹Ÿæœ‰è¿™ä¹ˆåšçš„ã€‚éå¸¸ä¸å¸¸è§çš„æ–¹æ³•æ˜¯æ± åŒ–å±‚çš„æ„Ÿå—é‡å¤§å°å¤§äº3çš„ï¼Œå› ä¸ºè¿™æ ·çš„æ± åŒ–æŸå¤±å¤ªå¤§äº†ï¼Œå¤ªaggressiveäº†ï¼Œè¿™ä¸€èˆ¬ä¼šä½¿å¾—æ€§èƒ½ä¸‹é™qwqã€‚

ä¸€äº›é—®é¢˜ï¼š

- æ³¨æ„æ¯ä¸ªè¿æ¥çš„å¤§å°å§ã€‚ ä¸Šé¢æå‡ºçš„æ–¹æ¡ˆæ˜¯ä»¤äººæ„‰å¿«çš„ï¼Œå› ä¸ºæ‰€æœ‰CONVå±‚éƒ½ä¿ç•™äº†å®ƒä»¬è¾“å…¥çš„ç©ºé—´å¤§å°ï¼Œè€ŒPOOLå±‚å•ç‹¬è´Ÿè´£åœ¨ç©ºé—´ä¸Šå¯¹å·è¿›è¡Œä¸‹é‡‡æ ·ã€‚ åœ¨å¦ä¸€ç§æ–¹æ¡ˆä¸­ï¼Œæˆ‘ä»¬ä½¿ç”¨å¤§äº1çš„æ­¥å¹…æˆ–è€…ä¸å¯¹CONVå±‚ä¸­çš„è¾“å…¥è¿›è¡Œé›¶å¡«å……ï¼Œæˆ‘ä»¬å¿…é¡»éå¸¸ä»”ç»†åœ°è·Ÿè¸ªæ•´ä¸ªCNNæ¶æ„ä¸­çš„è¾“å…¥é‡ï¼Œå¹¶ç¡®ä¿æ‰€æœ‰æ­¥å¹…å’Œè¿‡æ»¤å™¨â€œæ­£å¸¸å·¥ä½œâ€ outâ€œå’ŒConvNetæ¶æ„å¾ˆå¥½åœ°å¯¹ç§°è¿æ¥ã€‚

- ä¸ºä»€ä¹ˆåœ¨CONVå½“ä¸­çš„strideè®¾ä¸º1ï¼Ÿå®è·µä¸Šæ›´å¥½ã€‚å¦å¤–strideè®¾ä¸º1å¯ä»¥æŠŠæ‰€æœ‰çš„ä¸‹é‡‡æ ·å·¥ä½œäº¤ç»™POOLå±‚ï¼ŒCONVå±‚åªéœ€è¦è½¬åŒ–è¾“å…¥volumeå°±å¯ä»¥äº†ã€‚

- ä¸ºä»€ä¹ˆç”¨paddingï¼Ÿä¸ºäº†ä¿æŒç©ºé—´å¤§å°ã€‚å¦‚æœä¸ç”¨é›¶å¡«å……çš„è¯ï¼Œè¾“å‡ºå¤§å°ä¼šå˜å°ï¼Œè¾¹ç¼˜çš„ä¿¡æ¯ä¼šè¢«å¾ˆå¿«æ´—æ‰ã€‚

- æŠ˜ä¸­å†…å­˜çº¦æŸã€‚ConvNetæ¶æ„çš„æ—©æœŸï¼Œä½¿ç”¨ä¸Šè¿°ç»éªŒæ³•åˆ™å¯ä»¥éå¸¸å¿«é€Ÿå»ºç«‹å†…å­˜é‡ã€‚æ¯”å¦‚å°‘ä½¿ç”¨ä¸‰ä¸ª3x3çš„Convå±‚è¿‡æ»¤224x224x3çš„å›¾åƒï¼Œæ¯ä¸ªå›¾å±‚åŒ…å«64ä¸ªæ»¤æ³¢å™¨ï¼Œå¹¶ä¸”paddingä¸º1ï¼Œåˆ›å»º3ä¸ª224x224x64çš„æ¿€æ´»volumeã€‚ç›¸å½“äº1000wæ¬¡æ¿€æ´»ï¼Œæˆ–è€…72MBå†…å­˜ã€‚ç”±äºGPUç»å¸¸æ˜¯å—åˆ°å†…å­˜çš„åˆ¶çº¦ï¼Œå¯èƒ½éœ€è¦æŠ˜ä¸­ã€‚æ¯”å¦‚ä¸€ä¸ªæŠ˜ä¸­å¯èƒ½æ˜¯ä½¿ç”¨7x7çš„æ»¤æ³¢å™¨ï¼Œstrideä¸º2.å¦ä¸€ä¸ªä¾‹å­æ˜¯AlexNetçš„æ»¤æ³¢å™¨å¤§å°ä¸º11x11ï¼Œstrideä¸º4ã€‚

ç»å…¸ç½‘ç»œï¼š

- LeNet: [è®ºæ–‡åœ¨æ­¤](http://yann.lecun.com/exdb/publis/pdf/lecun-98.pdf)

- AlexNet: [è®ºæ–‡åœ¨æ­¤](http://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks)
å‚åŠ è¿™ä¸ª2012å¹´çš„æ¯”èµ›ğŸ†ã€‚[ImageNet ILSVRC challenge](http://www.image-net.org/challenges/LSVRC/2014/)æ˜¾è‘—æ¯”ç¬¬äºŒåé«˜ï¼Œå’ŒLeNetçš„ç»“æ„å·®ä¸å¤šï¼Œä½†æ˜¯æ›´æ·±ï¼Œç‰¹å¾å·ç§¯å±‚å †å åœ¨å½¼æ­¤ä¹‹ä¸Šï¼ˆä»¥å‰POOLå±‚å‰é¢åªæœ‰ä¸€ä¸ªCONVå±‚ï¼‰

- ZF Net: 2013å¹´ILSVRCçš„ğŸ†ã€‚è¿™ä¸ªæ¯”[è®ºæ–‡åœ¨æ­¤](http://arxiv.org/abs/1311.2901)æ”¹è¿›äº†AlexNetï¼Œé€šè¿‡æ‰­æ›²ç»“æ„çš„è¶…å‚æ•°è¿›è¡Œæ”¹è¿›ï¼Œç‰¹åˆ«æ˜¯æ‰©å¼ äº†ä¸­é—´å·ç§¯å±‚çš„æ•°é‡ï¼Œä½¿å¾—ç¬¬ä¸€å±‚çš„strideå’Œå·ç§¯å±‚å¤§å°æ›´å°ã€‚

- GoogLeNet: 2014å¹´ILSVRCçš„ğŸ†ã€‚[è®ºæ–‡åœ¨æ­¤](http://arxiv.org/abs/1409.4842)ã€‚æå‡ºç›—æ¢¦ç©ºé—´æ¨¡å‹ï¼Œåˆ›é€ æ€§çš„æŠŠå‚æ•°æ•°é‡å‡ åå¹´å°‘äº†å¾ˆå¤šï¼ˆ4ç™¾ä¸‡ï¼ŒAlexNet6åƒä¸‡ï¼‰ã€‚å¦å¤–åœ¨ConvNetçš„é¡¶ç«¯ï¼Œä½¿ç”¨å¹³å‡æ± åŒ–å±‚ï¼Œè€Œä¸æ˜¯å…¨è¿æ¥å±‚ï¼Œæ¶ˆé™¤äº†å¤§é‡çš„ï¼Œä¸æ˜¯é‚£ä¹ˆé‡è¦çš„å‚æ•°ã€‚æœ€æ–°ç‰ˆæœ¬çš„æ˜¯[Inception-v4](http://arxiv.org/abs/1602.07261)

- VGGNet: 2014å¹´ILSVRCçš„ğŸ¥ˆï¼Œ[è®ºæ–‡åœ¨æ­¤](http://www.robots.ox.ac.uk/~vgg/research/very_deep/)ï¼Œç‰¹æ®Šçš„è´¡çŒ®ç‚¹æ˜¯ç½‘ç»œçš„æ·±åº¦å¯¹äºæ•ˆæœæ¥è¯´æ˜¯ä¸€ä¸ªå¾ˆå¤§ä¸€ä¸ªå½±å“å› ç´ æŠŠã€‚æœ€å¥½çš„å·¥ä½œæ˜¯åŒ…å«16ä¸ªCONV/FCç½‘ç»œï¼Œå¹¶ä¸”ä»å¤´åˆ°å°¾ï¼Œç»“æ„éƒ½æä¸ºç›¸ä¼¼ï¼Œåªç”¨3x3å’Œ2x2å·ç§¯ã€‚é¢„è®­ç»ƒæ¨¡å‹ç”¨caffeè¡¨ç¤º[åœ¨æ­¤](http://www.robots.ox.ac.uk/~vgg/research/very_deep/)ï¼Œä½†æœ‰ä¸ªç¼ºé™·æ˜¯è¿™ä¸ªç½‘ç»œæˆæœ¬å¤ªé«˜äº†ï¼Œè¦å¾ˆå¤šå†…å­˜å¹¶ä¸”å‚æ•°æ•°é‡æœ‰1äº¿4åƒä¸‡qwqã€‚è¿™äº›å‚æ•°ä¸»è¦æ˜¯åœ¨ç¬¬ä¸€ä¸ªå…¨è¿æ¥å±‚é‡Œé¢ï¼Œåæ¥å‘ç°è¿™äº›FCå¯ä»¥è¢«åˆ é™¤ï¼Œå¹¶ä¸”æ²¡å•¥æ€§èƒ½é™ä½ï¼Œå¤§å¤§å‡å°‘äº†å¿…è¦çš„å‚æ•°æ•°ç›®ã€‚

- ResNet: 2015å¹´ILSVRCçš„ğŸ†ï¼Œä½•å‡¯æ˜çš„ã€‚[è®ºæ–‡åœ¨æ­¤](http://arxiv.org/abs/1512.03385)è·³è¿‡äº†è¿æ¥ï¼Œå¤§é‡ä½¿ç”¨[batchnorm](https://arxiv.org/abs/1502.03167)ï¼Œåœ¨æœ€åä¹Ÿæ²¡æœ‰ä½¿ç”¨å…¨è¿æ¥å±‚ï¼Œè¿™é‡Œæœ‰ä»–çš„[æ¼”è®²ppt](http://research.microsoft.com/en-us/um/people/kahe/ilsvrc15/ilsvrc2015_deep_residual_learning_kaiminghe.pdf)å’Œ[è§†é¢‘](https://www.youtube.com/watch?v=1PGLj-uKT1w)ã€‚è¿˜æœ‰ä¸€äº›æœ€è¿‘çš„[å®éªŒ](https://github.com/gcr/torch-residual-networks)ï¼Œç”¨torchå†™çš„ã€‚ResNetç°åœ¨å·²ç»æ˜¯state-of-artäº†ï¼Œåœ¨å®é™…å½“ä¸­æ˜¯é¦–å…ˆè¢«è€ƒè™‘çš„ConvNetã€‚è¿™æ˜¯ä¸€äº›[æœ€è¿‘çš„ç ”ç©¶](https://arxiv.org/abs/1603.05027)ã€‚

VGGç½‘ç»œçš„ç»†èŠ‚ï¼š3x3å·ç§¯å±‚ï¼Œstride1ï¼Œpadding1ï¼Œpoolå±‚2x2ï¼Œstride2ï¼Œæ²¡paddingã€‚

```yaml
- INPUT: [224x224x3]  
  å†…å­˜: 224x224x3 = 150K
  æƒé‡: 0
- CONV3-64: [224x224x64]
  å†…å­˜: 224x224x64 = 3.2M
  æƒé‡: 3x3x3x64=1728
- CONV3-64: [224x224x64]
  å†…å­˜: 224x224x64 = 3.2M
  æƒé‡: 3x3x3x64=36864
- POOL2: [112x112x64]
  å†…å­˜: 112x112x64 = 800K
  æƒé‡: 0
- CONV3-128: [112x112x128]
  å†…å­˜: 112x112x128 = 1.6M
  æƒé‡: 3x3x64x128=73728
- CONV3-128: [112x112x128]
  å†…å­˜: 112x112x128 = 1.6M
  æƒé‡: 3x3x3x128=147456
- POOL2: [56x56x128]
  å†…å­˜: 56x56x128 = 400K
  æƒé‡: 0
- CONV3-256: [56x56x256]
  å†…å­˜: 56x56x256 = 800K
  æƒé‡: 3x3x128x256=294912
- CONV3-256: [56x56x256]
  å†…å­˜: 56x56x256 = 800K
  æƒé‡: 3x3x256x256=589824
- CONV3-256: [56x56x256]
  å†…å­˜: 56x56x256 = 800K
  æƒé‡: 3x3x256x256=589824
- POOL2: [28x28x256]
  å†…å­˜: 28x28x256 = 200K
  æƒé‡: 0
- CONV3-512: [28x28x512]
  å†…å­˜: 28x28x512 = 400K
  æƒé‡: 3x3x256x512=1179648
- CONV3-512: [28x28x512]
  å†…å­˜: 28x28x512 = 400K
  æƒé‡: 3x3x512x512=2359296
- CONV3-512: [28x28x512]
  å†…å­˜: 28x28x512 = 400K
  æƒé‡: 3x3x512x512=2359296
- POOL2: [14x14x256]
  å†…å­˜: 14x14x512 = 100K
  æƒé‡: 0
- CONV3-512: [14x14x512]
  å†…å­˜: 14x14x512 = 100K
  æƒé‡: 3x3x512x512=2359296
- CONV3-512: [14x14x512]
  å†…å­˜: 14x14x512 = 100K
  æƒé‡: 3x3x512x512=2359296
- CONV3-512: [14x14x512]
  å†…å­˜: 14x14x512 = 100K
  æƒé‡: 3x3x512x512=2359296
- POOL2: [7x7x256]
  å†…å­˜: 7x7x512 = 25K
  æƒé‡: 0
- FC: [1x1x4096]
  å†…å­˜: 4096
  æƒé‡: 7x7x512x4096=102760448
- FC: [1x1x4096]
  å†…å­˜: 4096
  æƒé‡: 4096x4096=16777216
- FC: [1x1x4096]
  å†…å­˜: 1000
  æƒé‡: 4096x1000=4096000

æ€»å†…å­˜: 24M * 4byte ~= 93MB / imgï¼Œ ä»…åŒ…å«å‰å‘ä¼ æ’­ï¼Œå¦‚æœåŠ ä¸Šåé¡¹ä¼ æ’­çš„è¯è¦ä¹˜ä»¥2ã€‚

æ€»å‚æ•°æ•°é‡: 1äº¿3åƒ8ç™¾ä¸‡
```

ç¬¬ä¸€ä¸ªå…¨è¿æ¥å±‚åŒ…å«äº†1äº¿å¤šå‚æ•°â€¦â€¦

### 0.1.4. è®¡ç®—çš„è€ƒè™‘

æœ€å¤§çš„ç“¶é¢ˆæ˜¯å†…å­˜ç“¶é¢ˆã€‚è®¸å¤šç°ä»£GPUçš„ç‰ˆæœ¬æ˜¯3/4/6/12Gå†…å­˜ï¼Œå†…å­˜çš„æ¶ˆè€—æœ‰3ä¸ªä¸»è¦éƒ¨åˆ†ï¼š

1. ä¸­é—´volumeçš„å¤§å°ã€‚
  ä¸Šé¢çš„é‚£äº›æ˜¯åŸå§‹æ¿€æ´»çš„æ•°é‡ï¼ŒåŒ…å«ä»–ä»¬çš„æ¢¯åº¦ï¼ˆå…·æœ‰ç›¸åŒå¤§å°ï¼‰ã€‚ä¸€èˆ¬æ¥è¯´ï¼Œæ‰€æœ‰è¿™äº›æ¿€æ´»å‡½æ•°éƒ½æ˜¯åœ¨æœ€æ—©çš„å‡ å±‚ConvNetä¸Šæ‰è¢«æ¿€æ´»ï¼Œè¿™äº›éƒ½éœ€è¦è¢«ä¿ç•™ï¼Œå› ä¸ºåå‘ä¼ æ’­çš„æ—¶å€™éœ€è¦è¿™äº›ä¸œè¥¿ã€‚ä½†æ˜¯åªæœ‰åœ¨æµ‹è¯•çš„æ—¶å€™æ‰è¿è¡Œå¯ä»¥æ˜¾è‘—å‡å°‘è¿™ä¸ªæ•°é‡ï¼Œåªéœ€è¦å°†å½“å‰çš„activationè®°å½•ä¸‹æ¥ï¼Œå¹¶ä¸”åœ¨ä»»ä½•å±‚ä¸¢ä¸‹ä¹‹å‰çš„æ¿€æ´»ã€‚

2. å‚æ•°å¤§å°ï¼šå› ä¸ºæ›´æ–°æƒé‡éœ€è¦å†…å­˜ï¼Œå¦‚æœæ˜¯åŠ¨é‡æ–¹æ³•ï¼ŒAdagradï¼ŒRMSPropæ–¹æ³•çš„è¯éœ€è¦å‚¨å­˜ä¸Šä¸€ä¸ªæ—¶åˆ»çš„å€¼ã€‚å› æ­¤å­˜å‚¨è¿™äº›å˜é‡ä¹Ÿéœ€è¦å¯¹ä¸Šé¢çš„å‚æ•°æ•°é‡ä¹˜ä»¥è‡³å°‘3å·¦å³å§ã€‚

3. å…¶ä»–é¢å¤–çš„ä¹±ä¸ƒå…«ç³Ÿçš„å†…å­˜ï¼šæ¯”å¦‚å›¾åƒåˆ†æ‰¹æ¬¡ï¼Œå¢å¹¿å•¥çš„éƒ½éœ€è¦å†…å­˜ã€‚

ä¸€èˆ¬æ¥è¯´å…ˆä¼°è®¡å‚æ•°çš„æ€»æ•°ï¼Œè¿™ä¸ªæ•°å­—åº”è¯¥å˜æˆGBä¸ºå•ä½çš„å¤§å°ã€‚è¿™äº›å€¼ä¹˜ä»¥4å¾—åˆ°å­—èŠ‚æ•°ï¼Œæˆ1024å¾—åˆ°KBï¼ŒMBå’ŒGBã€‚å¦‚æœçˆ†æ‰äº†çš„è¯ï¼Œå°±è¯•ç€å‡å°‘batchå¤§å°ï¼Œå› ä¸ºå¤§å¤šæ•°å†…å­˜éƒ½æ˜¯åœ¨æ¿€æ´»çš„å•¥æ—¶å€™è¢«æ¶ˆè€—çš„ã€‚

### 0.1.5. ç‰›é€¼çš„è¿ç§»å­¦ä¹ 

å¦‚æœä½ ç”¨CNNçš„è¯ï¼Œä½ å°±éœ€è¦å¾ˆå¤šæ•°æ®ï¼ŸNo

è®­ç»ƒImageNetç¡®å®å¾ˆè´¹åŠ²ã€‚

å¯¹äºå°æ•°æ®é›†çš„è¯ï¼Œå¯ä»¥åªè®­ç»ƒæœ€åçš„å…¨è¿æ¥å±‚ï¼Œå‰é¢çš„å‚æ•°å†»ç»“å°±å¯ä»¥äº†ã€‚

å¯¹äºæ›´å¤§çš„æ•°æ®é›†æ¥è¯´ï¼Œä¹Ÿæ˜¯è®­ç»ƒæœ€åçš„ä¸‰å±‚å…¨è¿æ¥å±‚å°±å¥½äº†ï¼ˆä½ çš„æ„æ€æ˜¯è®­ç»ƒé‚£ä¸ªä¸€äº¿å‚æ•°çš„é‚£ä¸ªå…¨è¿æ¥å±‚å—qwqï¼‰ï¼Œä½†æ˜¯åˆå§‹å€¼å°±ä»åˆ«äººçš„æ¨¡å‹å¼€å§‹å°±å¯ä»¥äº†å•Šã€‚

å¯¹äºfine tuningè¿‡ç¨‹æ¥è¯´ï¼Œå­¦ä¹ ç‡è°ƒæ•´åˆ°åŸæ¥çš„1/10å°±å¥½äº†ã€‚

ç°åœ¨æ¥å†™ä¸€ä¸ªè¡¨æ ¼ã€‚

ç¦»è¾“å…¥å±‚è¶Šè¿‘çš„å±‚è¡¨ç¤ºçš„ç‰¹å¾è¶Šgenericï¼Œè¶Šè¿œçš„å±‚è¶Šspecificã€‚

| |éå¸¸ç›¸ä¼¼çš„æ•°æ®é›†|éå¸¸ä¸åŒçš„æ•°æ®é›†|
|éå¸¸å°‘çš„æ•°æ®| åªéœ€è¦è®­ç»ƒæœ€åä¸€å±‚å…¨è¿æ¥å±‚å°±å¥½å•¦| é‚£å¯èƒ½ä¼šæœ‰å¾ˆå¤§çš„é—®é¢˜|
|æœ‰å¾ˆå¤šçš„æ•°æ®| fine tuningæœ€åçš„å‡ å±‚å°±è¡Œäº†|éœ€è¦é‡æ–°è®­ç»ƒå¾ˆå¤šæ•°æ®|

è¿ç§»å­¦ä¹ æ— å¤„ä¸åœ¨ï¼Œæ¯”å¦‚ç‰©ä½“æ£€æµ‹ï¼ˆFast-RCNNï¼‰ï¼Œå›¾åƒæ ‡è®°ï¼ˆCNN+RNN)ã€‚éƒ½ç”¨åˆ°äº†ä»ImageNetå½“ä¸­è®­ç»ƒå‡ºçš„CNNã€‚å›¾åƒæ ‡è®°å½“ä¸­è¿˜ç”¨åˆ°äº†é¢„è®­ç»ƒçš„å•è¯å‘é‡word2vecã€‚

åˆ°å“ªé‡Œå»æ‰¾è¿™äº›é¢„è®­ç»ƒçš„æ¨¡å‹å‘¢ï¼Ÿæ€ä¹ˆç”¨å‘¢ï¼Ÿ

Caffeï¼š<https://github.com/BVLC/caffe/wiki/Model-Zoo>
TensorFlow: <https://github.com/tensorflow/models>
PyTorch: <https://github.com/pytorch/vision>

æ€»ç»“ï¼š

1. ä¼˜åŒ–æ–¹æ³•ï¼šåŠ¨é‡æ³•ï¼Œ RMSPropï¼Œ Adamæ–¹æ³•
2. æ­£åˆ™åŒ–æ–¹æ³•ï¼š dropout
3. è¿ç§»å­¦ä¹ ï¼š æŠŠä»–ç”¨åœ¨ä½ çš„é¡¹ç›®å½“ä¸­

ä¸‹ä¸€èŠ‚ï¼šæ·±åº¦å­¦ä¹ è½¯ä»¶ã€‚

## æ·±åº¦å­¦ä¹ è½¯ä»¶ï¼Œåº”è¯¥åå·¥ç¨‹ä¸€ç‚¹

CPUï¼šæ›´å°‘çš„æ ¸ï¼Œä½†æ˜¯æ¯ä¸ªæ ¸è¿è¡Œé€Ÿåº¦æ›´å¿«ï¼Œå¹¶ä¸”æ›´åŠ å¥½ç”¨ã€‚å¯¹äºåºåˆ—ä»»åŠ¡æ¥è¯´æ¯”è¾ƒé€‚ç”¨ã€‚

GPUï¼šæ›´å¤šæ ¸ï¼Œä½†æ˜¯æ¯ä¸ªæ ¸éƒ½æ…¢ä¸€ç‚¹ï¼Œç¬¨ä¸€ç‚¹ï¼Œæ¯”è¾ƒé€‚åˆå¹¶è¡Œä»»åŠ¡ã€‚

### GPUç¼–ç¨‹

- CUDAï¼ˆåªæœ‰Nvidiaæœ‰ï¼‰
  - Cè¯­è¨€é£æ ¼çš„ä»£ç ï¼Œç›´æ¥åœ¨GPUä¸Šé¢è¿è¡Œ
  - é«˜çº§APIï¼š cuBLASï¼Œ cuFFTï¼ŒcuDNNã€‚

- OpenCL
  - å’ŒCUDAç›¸ä¼¼ï¼Œä½†æ˜¯èƒ½å¤Ÿè¿è¡Œåœ¨ä»»ä½•å¹³å°ã€‚
  - ä¸€èˆ¬æ¥è¯´æ¯”è¾ƒæ…¢qwq

- Udacity
  - å¼•å…¥å¹¶è¡Œç¼–ç¨‹ï¼Œçœ‹[cs344è¿™é—¨è¯¾](https://www.udacity.com/course/cs344)
  - å¯¹äºæ·±åº¦å­¦ä¹ æ¥è¯´ï¼Œåªéœ€è¦ä½¿ç”¨å­˜åœ¨çš„åº“å°±å¯ä»¥äº†

### CPUä¸GPUæ€§èƒ½å·®è·ï¼š

è¿™æ˜¯ä½¿ç”¨cudnnå’Œcpuçš„æ¯”è¾ƒ

|vgg-16|66å€|2.8å€|
|vgg-19|67å€|3.0å€|
|ResNet-18|71å€|3.1å€|
|ResNet-50|64å€|3.4å€|
|ResNet-200|76å€|2.8å€|

å¯èƒ½ç”±äºä»ç¡¬ç›˜å½“ä¸­è¯»å–æ•°æ®äº§ç”Ÿæ€§èƒ½ç“¶é¢ˆã€‚

è§£å†³åŠæ³•ï¼š
1. æŠŠæ‰€æœ‰æ•°æ®è¯»åˆ°RAMä¸­
2. ä½¿ç”¨SSDè€Œä¸æ˜¯HDD
3. ä½¿ç”¨å¤šæ ¸CPUæ¥è·å–æ•°æ®

### æ¡†æ¶

*Caffe* -> *Caffe2*
Torch -> **PyTorch**
Theano ->**TensorFlow**
Paddle
CNTK
MXNet

ä¸€ä¸ªæ¡†æ¶åº”è¯¥åšåˆ°ï¼š

1. å®¹æ˜“æ„å»ºå¤§å‹è®¡ç®—å›¾
2. å®¹æ˜“åœ¨å¤§å‹è®¡ç®—å›¾å½“ä¸­è®¡ç®—æ¢¯åº¦
3. åœ¨GPUä¸Šé¢è¿è¡Œæ•ˆç‡é«˜ï¼ŒåŒ…æ‹¬cuDNNï¼ŒcuBLASã€‚

{% codeblock lang:python %}
import numpy as np
np.random.seed(0)

N, D = 3,4
x = np.random.randn(N, D)
y = np.random.randn(N, D)
z = np.random.randn(N, D)

a = x * y
b = a + z
c = np.sum(b)

grad_c = 1.0
grad_b = grad_c * np.ones((N ,D))
grad_a = grad_b.copy()
grad_z = grad_b.copy()
grad_x = grad_a * y
grad_y = grad_a * x

{% endcodeblock %}

Numpyçš„æ–¹æ³•æœ‰å¦‚ä¸‹ç¼ºç‚¹ï¼š
1. ä¸èƒ½å†GPUä¸Šè¿è¡Œ
2. éœ€è¦è‡ªå·±å»è®¡ç®—æ¢¯åº¦ã€‚

{% codeblock lang:python %}
import numpy as np
np.random.seed(0)
import tensorflow as tf

N, D = 3, 4
# æ„å»ºè®¡ç®—å›¾
# åœ¨è¿™é‡Œå¯ä»¥é€‰æ‹©ä½¿ç”¨gpuè¿˜æ˜¯cpuï¼Œå¥½åƒtfæ˜¯é»˜è®¤æ‰¾gpuï¼Œæ‰¾ä¸åˆ°çš„è¯ç”¨cpu
x = tf.placeHolder(tf.float32)
y = tf.placeHolder(tf.float32)
z = tf.placeHolder(tf.float32)

a = x * y
b = a + z
c = tf.reduce_sum(b)

# è®¡ç®—æ¢¯åº¦
grad_x, grad_y, grad_z = tf.gradients(c, [x, y, z])

with tf.Session() as sess:
  values = {
    x: np.random.randn(N,D),
    y: np.random.randn(N,D),
    z: np.random.randn(N,D),
  }
  out = sess.run([c, grad_x, grad_y, grad_z], feed_dict = values)
  c_val, grad_x_val, grad_y_val, grad_z_val = out
{% endcodeblock %}

PyTorch

{% codeblock lang:python %}
import torch
from torch.autograd import Variable

N, D = 3, 4

x = Variable(torch.randn(N, D), require_grad = True)
# x = Variable(torch.randn(N, D).cuda(), require_grad = True) è¿™æ˜¯ç”¨GPUçš„æƒ…å†µ
y = Variable(torch.randn(N, D), require_grad = True)
z = Variable(torch.randn(N, D), require_grad = True)

a = x * y
b = a + z
c = torch.sum(b)

c.backward()

print(x.grad.data)
print(y.grad.data)
print(z.grad.data)
{% endcodeblock %}

## Tensorflowè®­ç»ƒä¸€ä¸ª2å±‚ç¥ç»ç½‘ç»œ

è¿­ä»£ä¸€æ¬¡ç‰ˆæœ¬ï¼š

{% codeblock lang:python %}
import tensorflow as tf
import numpy as np
N,D,H = 64,1000,100
x = tf.placeholder(tf.float32, shape=(N,D))
y = tf.placeholder(tf.float32, shape=(N,D))
w1 = tf.placeholder(tf.float32, shape=(D,H))
w2 = tf.placeholder(tf.float32, shape=(H,D))

h = tf.maximum(tf.matmul(x, w1), 0)
y_pred = tf.matmul(h, w2)
diff = y_pred - y
loss = tf.reduce_mean(tf.reduce_sum(diff ** 2, axis = 1))
grad_w1, grad_w2 = tf.gradients(loss, [w1, w2])

with tf.Session() as sess:
  values = {
    x: np.random.randn(N, D), 
    w1: np.random.randn(D, H), 
    w2: np.random.randn(H, D), 
    y: np.random.randn(N, D), 
  }
  out = sess.run([loss, grad_w1, grad_w2], feed_dict=values)
  loss_val, grad_w1_val, grad_w2_val = out
{% endcodeblock %}

å¤šæ¬¡è¿­ä»£ç‰ˆæœ¬ï¼š

{% codeblock lang:python %}
with tf.Session() as sess:
  values = {
    x: np.random.randn(N, D), 
    w1: np.random.randn(D, H), 
    w2: np.random.randn(H, D), 
    y: np.random.randn(N, D), 
  }
  learning_rate = 1e-5
  for t in range(50):
    out = sess.run([loss, grad_w1, grad_w2], feed_dict=values)
    loss_val, grad_w1_val, grad_w2_val = out
    values[w1] -=learning_rate * grad_w1_val
    values[w2] -=learning_rate * grad_w2_val
{% endcodeblock %}

ä½†æ˜¯è¿™æ ·ä¼šåœ¨cpuå’Œgpuä¹‹é—´é¢‘ç¹å¤åˆ¶æ•°æ®ã€‚äºæ˜¯æœ‰æ–°çš„å½¢å¼ï¼š

{% codeblock lang:python %}
import tensorflow as tf
import numpy as np
N,D,H = 64,1000,100
x = tf.placeholder(tf.float32, shape=(N,D))
y = tf.placeholder(tf.float32, shape=(N,D))
w1 = tf.Variable(tf.random_normal((D,H)))
w2 = tf.Variable(tf.random_normal((H,D)))

h = tf.maximum(tf.matmul(x, w1), 0)
y_pred = tf.matmul(h, w2)
diff = y_pred - y
loss = tf.reduce_mean(tf.reduce_sum(diff ** 2, axis = 1))
grad_w1, grad_w2 = tf.gradients(loss, [w1, w2])

learning_rate = 1e-5
new_w1 = w1.assign(w1 - learning_rate * grad_w1)
new_w2 = w2.assign(w2 - learning_rate * grad_w2)

with tf.Session() as sess:
  sess.run(tf.global_variables_initiializer())
  values = {
    x: np.random.randn(N, D),
    y: np.random.randn(N, D),
  }
  for t in range(50):
    loss_val, = sess.run([loss], feed_dict=values)
{% endcodeblock %}

è¿™æ ·çš„è¯ï¼Œåªæœ‰è¾“å…¥å’Œè¾“å‡ºæ—¶placeholderï¼Œå…¶ä»–éƒ½æ˜¯ä¸­é—´å˜é‡ã€‚assignæ“ä½œæ¥æ›´æ–°è¿™ä¸ªw1å’Œw2ã€‚sess.run(tf.global_variables_initiializer())è¿™ä¸ªå°±æŠŠæ‰€æœ‰çš„æƒé‡åˆå§‹åŒ–äº†ã€‚

é—®é¢˜ï¼šlossä¸ä¸‹é™ï¼Œassignæ“ä½œå…¶å®æ²¡æœ‰è¢«æ‰§è¡Œã€‚æ€ä¹ˆåŠï¼Ÿ

è§£å†³åŠæ³•ï¼šæ·»åŠ ä¸€ä¸ªå¤šä½™çš„dummyèŠ‚ç‚¹ï¼Œè¿™æ ·å°±èƒ½å¤Ÿä½¿å¾—æ­»æ‰çš„w1å’Œw2åŠ¨èµ·æ¥ã€‚

{% codeblock lang:python %}
import tensorflow as tf
import numpy as np
N,D,H = 64,1000,100
x = tf.placeholder(tf.float32, shape=(N,D))
y = tf.placeholder(tf.float32, shape=(N,D))
w1 = tf.Variable(tf.random_normal((D,H)))
w2 = tf.Variable(tf.random_normal((H,D)))

h = tf.maximum(tf.matmul(x, w1), 0)
y_pred = tf.matmul(h, w2)
diff = y_pred - y
loss = tf.reduce_mean(tf.reduce_sum(diff ** 2, axis = 1))
grad_w1, grad_w2 = tf.gradients(loss, [w1, w2])

learning_rate = 1e-5
new_w1 = w1.assign(w1 - learning_rate * grad_w1)
new_w2 = w2.assign(w2 - learning_rate * grad_w2)
updates = tf.group(new_w1, new_w2)

with tf.Session() as sess:
  sess.run(tf.global_variables_initiializer())
  values = {
    x: np.random.randn(N, D),
    y: np.random.randn(N, D),
  }
  for t in range(50):
    loss_val,_ = sess.run([loss,updates], feed_dict=values)
{% endcodeblock %}

æ›´é›†æˆåŒ–ï¼Œå¯ä»¥ç›´æ¥ç”¨tfè‡ªå¸¦çš„ä¼˜åŒ–å™¨æ¥è®¡ç®—æ¢¯åº¦å’Œæƒé‡ã€‚

{% codeblock lang:python %}
import tensorflow as tf
import numpy as np
N,D,H = 64,1000,100
x = tf.placeholder(tf.float32, shape=(N,D))
y = tf.placeholder(tf.float32, shape=(N,D))
w1 = tf.Variable(tf.random_normal((D,H)))
w2 = tf.Variable(tf.random_normal((H,D)))

h = tf.maximum(tf.matmul(x, w1), 0)
y_pred = tf.matmul(h, w2)
diff = y_pred - y
loss = tf.reduce_mean(tf.reduce_sum(diff ** 2, axis = 1))

optimizer = tf.train.GradientDescentOptimizer(1e-5)
updates = optimizer.minimize(loss)

with tf.Session() as sess:
  sess.run(tf.global_variables_initiializer())
  values = {
    x: np.random.randn(N, D),
    y: np.random.randn(N, D),
  }
  for t in range(50):
    loss_val,_ = sess.run([loss,updates], feed_dict=values)
{% endcodeblock %}

è¿˜èƒ½è¿›ä¸€æ­¥æ”¹è¿›ï¼Œæ¯”å¦‚æˆ‘ä»¬ç”¨é€šå¸¸ä½¿ç”¨çš„losså…¶å®ä¹Ÿæœ‰æ–¹æ³•èƒ½ä»£æ›¿ï¼š


{% codeblock lang:python %}
import tensorflow as tf
import numpy as np
N,D,H = 64,1000,100
x = tf.placeholder(tf.float32, shape=(N,D))
y = tf.placeholder(tf.float32, shape=(N,D))
w1 = tf.Variable(tf.random_normal((D,H)))
w2 = tf.Variable(tf.random_normal((H,D)))

h = tf.maximum(tf.matmul(x, w1), 0)
y_pred = tf.matmul(h, w2)
diff = y_pred - y
loss = tf.losses.mean_squared_error(y_pred, y)

optimizer = tf.train.GradientDescentOptimizer(1e-5)
updates = optimizer.minimize(loss)

with tf.Session() as sess:
  sess.run(tf.global_variables_initiializer())
  values = {
    x: np.random.randn(N, D),
    y: np.random.randn(N, D),
  }
  for t in range(50):
    loss_val,_ = sess.run([loss,updates], feed_dict=values)
{% endcodeblock %}

ä¸€æ­¥æ­¥æ”¹çš„é¢ç›®å…¨éç³»åˆ—ï¼š


{% codeblock lang:python %}
import tensorflow as tf
import numpy as np
N,D,H = 64,1000,100
x = tf.placeholder(tf.float32, shape=(N,D))
y = tf.placeholder(tf.float32, shape=(N,D))

init = tf.contrib.layers.xavier_initializer()
h = tf.layers.dense(inputs=x,units=H, activation=tf.nn.relu,kernel_initializer = init)
y_pred = tf.layers.dense(inputs=h, units=D, kernel_initializer=init)

loss = tf.losses.mean_squared_error(y_pred, y)

optimizer = tf.train.GradientDescentOptimizer(1e0)
updates = optimizer.minimize(loss)

with tf.Session() as sess:
  sess.run(tf.global_variables_initiializer())
  values = {
    x: np.random.randn(N, D),
    y: np.random.randn(N, D),
  }
  for t in range(50):
    loss_val,_ = sess.run([loss,updates], feed_dict=values)
{% endcodeblock %}

Keras:æ›´é«˜çº§çš„åŒ…è£…çº¸ï¼ˆåŒæ ·æ”¯æŒTheanoåç«¯ï¼‰

{% codeblock lang:python %}
from keras.models import Sequential
from keras.layers.core import Dense, Activation
from keras.optimizers import SGD

N, D, H = 64, 1000, 100

model = Sequential()
model.add(Dense(input_dim = D, output_dim = H))
model.add(Activation('relu'))
model.add(Dense(input_dim = H, output_dim=D))

optimizer = SGD(lr=1e0)
model.compile(loss='mean_squared_error', optimizer=optimizer)

x = np.random.randn(N,D)
y = np.random.randn(N,D)
history = model.fit(x,y,nb_epoach=50, batch_size=N, verbose =0)
{% endcodeblock %}

å…¶ä»–çš„é«˜çº§apiï¼š

- Keras,
- TFLearn,
- TensorLayer,
- tf.layers,
- TF-Slim,
- tf.contrib.learn
- Pretty Tensor
- Sonnet

ä¸€äº›é¢„è®­ç»ƒæ¨¡å‹ï¼š

- [TF-Slim](https://github.com/tensorflow/models/tree/master/slim/nets)
- [Keras](https://github.com/fchollet/deep-learning-models)

ä½¿ç”¨tensorboardæ¥è®°å½•æ•°æ®
tensorflowçš„åˆ†å¸ƒå¼ç‰ˆæœ¬ï¼š<https://www.tensorflow.org/deploy/distributed>

## PyTorch ä¸‰å±‚çš„æŠ½è±¡

Tensorï¼šè¦è®¡ç®—çš„èŠ‚ç‚¹ï¼Œåœ¨gpuä¸Šé¢è¿è¡Œ
Variableï¼šå­˜å‚¨æ•°æ®å’Œæ¢¯åº¦
Moduleï¼šç¥ç»ç½‘ç»œå±‚ï¼Œå¾ˆå¤šéƒ½å‚¨å­˜çŠ¶æ€æˆ–è€…å¯å­¦ä¹ çš„æƒé‡ã€‚

tensorflowä¸­çš„ç­‰ä»·å…ƒç´ ï¼š

numpyæ•°ç»„
Tensor, Variable, Placeholder
tf.layers, TFSlim, TFLearn, Sonnets

{% codeblock lang:python %}
import torch

dtype = torch.FloatTensor
# dtype = torch.cuda.FloatTensor å¦‚æœä½¿ç”¨gpuçš„è¯

N, D_in, H, D_out = 64, 1000, 100, 10
x = torch.randn(N, D_in).type(dtype)
y = torch.randd(N, D_out).type(dtype)
w1 = torch.randn(D_in, H).type(dtype)
w2 = torch.randn(H, D_out).dtype(dtype)

learning_rate = 1e-6
for t in range(500):
  h = x.mm(w1)
  # æ–œå¡ï¼Œæœ€å°å€¼ä¸º0
  h_relu = h.clamp(min=0)
  y_pred = h_relu.mm(w2)
  loss = (y_pred - y).pow(2).sum()

  grad_y_pred = 2.0 * (y_pred - y)
  grad_w2 = h_relu.t().mm(grad_y_pred)
  grad_h_relu = grad_y_pred.mm(w2.t())
  grad_h = grad_h_relu.clone()
  grad_h[h<0]=0
  grad_w1 = x.t().mm(grad_h)

  w1 -= learning_rate * grad_w1
  w2 -= learning_rate * grad_w2

{% endcodeblock %}

è‡ªåŠ¨æ±‚å¯¼ï¼š

{% codeblock lang:python %}
import torch
import torch.autograd import Variable

N, D_in, H, D_out = 64, 1000, 100, 10
x = Variable(torch.randn(N, D_in),requires_grad=False)
y = Variable(torch.randn(N, D_out),requires_grad=False)
w1 = Variable(torch.randn(D_in, H),requires_grad=True)
w2 = Variable(torch.randn(H, D_out),requires_grad=True)

learning_rate  =1e-6
for t in range(500):
  y_pred = x.mm(w1).clamp(min=0).mm(w2)
  loss = (y_pred - y).pow(2).sum()

  # è®¡ç®—æ¢¯åº¦çš„æ—¶å€™è¦å…ˆæŠŠè¿™ä¸€é¡¹çš„æ¢¯åº¦è®¾ä¸º0ï¼Œç„¶åå†å»è®¡ç®—
  if w1.grad:
    w1.grad.data().zero_()
  if w2.grad:
    w2.grad.data().zero_()
  loss.backward()

  w1.data -= learning_rate * w1.grad.data
  w2.data -= learning_rate * w2.grad.data
{% endcodeblock %}

å¯¹äºè¾“å…¥å˜é‡æ¥è¯´ï¼Œä¸éœ€è¦æ±‚å¯¼ï¼Œxå’Œyä¸éœ€è¦æ±‚å¯¼ã€‚
æ‰€æœ‰çš„Tensorçš„æ“ä½œå¯¹æ¥è¯´ï¼Œå¯¹Variableä¹Ÿæ˜¯ç›¸åŒçš„æ“ä½œã€‚

è‡ªå®šä¹‰å‡½æ•°æ¥æ‰§è¡Œforwardå’Œbackwardï¼š
{% codeblock lang:python %}
class ReLU(torch.autograd.Function):
  def forward(self, x):
    self.save_for_backward(x)
    return x.clamp(min=0)
  
  def backward(self, grad_y):
    x, = self.saved_tensors
    grad_input = grad_y.clone()
    grad_input[x<0] = 0
    return grad_input
{% endcodeblock %}

è¿™é‡Œå¯ä»¥ä½¿ç”¨è‡ªå·±å†™çš„å‡½æ•°è°ƒç”¨äº†ã€‚

{% codeblock lang:python %}
import torch
import torch.autograd import Variable

N, D_in, H, D_out = 64, 1000, 100, 10
x = Variable(torch.randn(N, D_in),requires_grad=False)
y = Variable(torch.randn(N, D_out),requires_grad=False)
w1 = Variable(torch.randn(D_in, H),requires_grad=True)
w2 = Variable(torch.randn(H, D_out),requires_grad=True)

learning_rate  =1e-6
for t in range(500):
  relu = ReLU()
  y_pred = relu(x.mm(w1)).mm(w2)
  loss = (y_pred - y).pow(2).sum()

  # è®¡ç®—æ¢¯åº¦çš„æ—¶å€™è¦å…ˆæŠŠè¿™ä¸€é¡¹çš„æ¢¯åº¦è®¾ä¸º0ï¼Œç„¶åå†å»è®¡ç®—
  if w1.grad:
    w1.grad.data().zero_()
  if w2.grad:
    w2.grad.data().zero_()
  loss.backward()

  w1.data -= learning_rate * w1.grad.data
  w2.data -= learning_rate * w2.grad.data
{% endcodeblock %}

ç¨å¾®å¼„é«˜çº§äº†ä¸€ç‚¹ç‚¹

{% codeblock lang:python %}
import torch
import torch.autograd import Variable

N, D_in, H, D_out = 64, 1000, 100, 10
x = Variable(torch.randn(N, D_in),requires_grad=False)
y = Variable(torch.randn(N, D_out),requires_grad=False)

model = torch.nn.Sequential(
  torch.nn.Linear(D_in, H),
  torch.nn.ReLU(),
  torch.nn.Linear(H, D_out)
)
loss_fn = torch.nn.MSELoss(size_average = False)

learning_rate = 1e-4
for t in range(500):
  y_pred = model(x)
  loss = loss_fn(y_pred, y)

  model.zero_grad()
  loss.backward()

  for param in model.parameters():
    param.data -= learning_rate * param.grad.data
{% endcodeblock %}

ä½¿ç”¨ä¼˜åŒ–å™¨è€Œä¸æ˜¯ç›´æ¥ç®—

{% codeblock lang:python %}
import torch
import torch.autograd import Variable

N, D_in, H, D_out = 64, 1000, 100, 10
x = Variable(torch.randn(N, D_in),requires_grad=False)
y = Variable(torch.randn(N, D_out),requires_grad=False)

model = torch.nn.Sequential(
  torch.nn.Linear(D_in, H),
  torch.nn.ReLU(),
  torch.nn.Linear(H, D_out)
)
loss_fn = torch.nn.MSELoss(size_average = False)

learning_rate = 1e-4
optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)
for t in range(500):
  y_pred = model(x)
  loss = loss_fn(y_pred, y)

  optimizer.zero_grad()
  loss.backward()

  optimizer.step()
{% endcodeblock %}

ä¸€ä¸ªä¸¤å±‚ç½‘ç»œçš„ä¾‹å­

{% codeblock lang:python %}

import torch
import torch.autograd import Variable

class TwoLayerNet(torch.nn.Module):
  def __init__(self, D_in,H, D_out):
    super(TwoLayerNet, self).__init__()
    self.linear1 = torch.nn.Linear(D_in, H)
    self.linear2 = torch.nn.Linear(H, D_out)

  def forward(self, x):
    h_relu = self.linear1(x).clamp(min=0)
    y_pred = self.linear2(h_relu)
    return y_pred
  # ä¸ç”¨å†™backwardï¼Œtorchä¼šè‡ªåŠ¨è®¡ç®—qwq

N, D_in, H, D_out = 64, 1000, 100, 10
x = Variable(torch.randn(N, D_in),requires_grad=False)
y = Variable(torch.randn(N, D_out),requires_grad=False)

model = TwoLayerNet(D_in, H, D_out)

critierion = torch.nn.MSELoss(size_average=False)
optimizer = torch.optim.SGD(model.parameters(), lr=1e-4)
for t in range(500):
  y_pred = model(x)
  loss = criterion(y_pred, y)

  optimizer.zero_grad()
  loss.backward()
  optimizer.step()
{% endcodeblock %}

å¯ä»¥ä»å¤–éƒ¨å¯¼å…¥æ•°æ®ï¼šDataLoadersã€‚å½“éœ€è¦å¤–éƒ¨æ•°æ®çš„æ—¶å€™ï¼Œå°±å†™ä½ çš„ç±»å°±å¯ä»¥äº†ã€‚

{% codeblock lang:python %}

import torch
import torch.autograd import Variable
from torch.utils.data import TensorDataset, DataLoader

N, D_in, H, D_out = 64, 1000, 100, 10

loader = DataLoader(TensorDataset(x, y), batch_size = 8)

model = TwoLayerNet(D_in, H, D_out)

critierion = torch.nn.MSELoss(size_average=False)
optimizer = torch.optim.SGD(model.parameters(), lr=1e-4)
for epoch in range(10):
  for x_batch, y_batch in loader:
    x_var, y_var = Variable(x), Variable(y)
    y_pred = model(x_var)
    loss = criterion(y_pred, y_var)

    optimizer.zero_grad()
    loss.backward()
    optimizer.step()
{% endcodeblock %}

å¯ä»¥ç›´æ¥ä»[è¿™ä¸ªç½‘å€](https://github.com/pytorch/vision)é‡Œé¢æ‰¾è®­ç»ƒå¥½çš„æ¨¡å‹ã€‚

{% codeblock lang:python %}
import torch 
import torchvision

alexnet = torchvision.models.alexnet(pretrained = True)
vgg16 = torchvision.models.vgg16(pretrained = True)
resnet101 = torchvision.models.resnet101(pretrained = True)
{% endcodeblock %}

Visdomå’ŒTensorboardæœ‰ç‚¹åƒï¼Œå¯ä»¥è®°å½•æ•°æ®ç­‰ç­‰ç­‰ï¼Œä½†æ˜¯æš‚æ—¶ä¸èƒ½çœ‹åˆ°è®¡ç®—å›¾ï¼Ÿ
<https://github.com/facebookresearch/visdom>

### Torchå’ŒPyTorchçš„å¯¹æ¯”ï¼š

|Torch|PyTorch|
|Luaï¼Œä¸å’‹ä¼šğŸ˜­|Pythonï¼Œè¿˜ç®—ä¼šğŸ˜|
|ä¸èƒ½è‡ªåŠ¨æ±‚å¯¼ğŸ˜­|è‡ªåŠ¨æ±‚å¯¼ğŸ˜¸|
|æ›´ç¨³å®šğŸ™‚|æ›´æ–°ä¸€ç‚¹ï¼Œè¿˜åœ¨ä¸æ–­æ”¹å˜ğŸ¤¦â€|
|æœ‰å¾ˆå¤šç°æˆä»£ç ğŸ™‹â€|ç°åœ¨ä»£ç ä¸å¤šï¼Œä½†æ˜¯ç°åœ¨åº”è¯¥ä¹Ÿå¤šäº†èµ·æ¥ğŸ˜‚|
|å·®ä¸å¤šå¿«ğŸ™…â€|å·®ä¸å¤šå¿«|

ç»“è®ºå¯èƒ½PyTorchä¼šå¥½ä¸€ç‚¹ã€‚

### Tensorflowå’ŒPyTorchçš„å¯¹æ¯”

Tensorflowæ˜¯å…ˆæ„å»ºè®¡ç®—å›¾ï¼ˆä¸€æ¬¡ï¼‰ï¼Œç„¶åæ¯æ¬¡è¿­ä»£ã€‚
PyTorchæ˜¯æ¯æ¬¡ä½¿ç”¨æ–°çš„å›¾æ¥è¿­ä»£ã€‚

ä½¿ç”¨é™æ€å›¾çš„è¯ï¼Œèƒ½å¤Ÿä¸€å¼€å§‹å°±ä¼˜åŒ–æˆç­‰ä»·çš„æ“ä½œã€‚

é™æ€å›¾çš„è¯ä¸€æ—¦æ„å»ºå¥½ï¼Œå°±å¯ä»¥åºåˆ—åŒ–çš„è¿è¡Œï¼Œä¹Ÿå°±ä¸éœ€è¦æ„å»ºå›¾çš„é‚£ä¸€éƒ¨åˆ†ä»£ç äº†ã€‚

åŠ¨æ€å›¾çš„æ„å»ºå’Œæ‰§è¡Œæ˜¯äº¤ç»‡åœ¨ä¸€èµ·çš„ï¼Œéœ€è¦ä¿æŒä»£ç ã€‚

tensorflowçš„foldæ“ä½œèƒ½å¤Ÿä½¿å¾—æ„å»ºåŠ¨æ€å›¾æ›´åŠ å®¹æ˜“ï¼Œé€šè¿‡dynamic batching

åŠ¨æ€å›¾çš„åº”ç”¨ï¼š
Recurrent networksï¼ˆå¾ªç¯ç½‘ç»œï¼‰
Recursive networks é€’å½’ç¥ç»ç½‘ç»œ
Modular network æ¨¡å—ç½‘ç»œï¼Ÿ

æ›´æ·±çš„ç½‘ç»œçš„losså¾ˆéš¾é™ä¸‹å»ï¼Œä¸æ˜¯å› ä¸ºè¿‡æ‹Ÿåˆäº§ç”Ÿçš„ã€‚

å‡è¯´ï¼šè¿™ä¸ªé—®é¢˜æ˜¯ä¸€ä¸ªä¼˜åŒ–é—®é¢˜ï¼Œæ›´æ·±çš„æ¨¡å‹æ›´éš¾ä»¥è¢«ä¼˜åŒ–ï¼Œæ›´æ·±çš„æ¨¡å‹åº”è¯¥è‡³å°‘å’Œæµ…ä¸€ç‚¹çš„æ¨¡å‹ä¸€æ ·å¥½çš„æ•ˆæœæ‰å¯¹ã€‚

ä¸€ä¸ªæ›´å¥½çš„æ–¹æ¡ˆæ˜¯ä»æµ…å±‚çš„ç½‘ç»œå½“ä¸­å¤åˆ¶å­¦ä¹ çš„å±‚ï¼Œå¹¶ä¸”æ·»åŠ é¢å¤–çš„å±‚ã€‚

è§£å†³åŠæ³•ï¼šä½¿ç”¨ç½‘ç»œå±‚å»æ‹Ÿåˆä¸€ä¸ªæ®‹å·®ï¼Œè€Œä¸æ˜¯ç›´æ¥å»æ‹Ÿåˆç»“æœã€‚

åŸæ¥çš„ç½‘ç»œç»“æ„ï¼š

X -> conv -> relu -> conv -> H(x)

æ®‹å·®æ¨¡å—ï¼š

x -> conv -> relu -> conv -> + -> relu
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€^

è¿™ä¸ªå±‚æ˜¯ç”¨æ¥æ‹Ÿåˆæ®‹å·®çš„F(x) = H(x) - xï¼Œè€Œä¸æ˜¯ç›´æ¥ç®—H(x)çš„ã€‚

æ•´ä¸ªResNetç»“æ„ï¼š

- å †å residual block
- æ¯ä¸ªresidual blockæœ‰ä¸¤ä¸ª3x3çš„convå±‚ã€‚
- æ¯ä¸¤ä¸ªæ»¤æ³¢å™¨å’Œä¸‹é‡‡æ ·ç©ºé—´ä½¿ç”¨ä¸¤ä¸ªstride
- åœ¨ä¸€å¼€å§‹ä½¿ç”¨ä¸€ä¸ªå·ç§¯å±‚
- æ²¡æœ‰ä½¿ç”¨å…¨è¿æ¥å±‚ï¼Œåªæœ‰æœ€åè¾“å‡º1000ä¸ªç±»åˆ«çš„æ—¶å€™æ‰ç”¨åˆ°å…¨è¿æ¥å±‚ã€‚
- ä½¿ç”¨å…¨å±€å¹³å‡æ± åŒ–å±‚åœ¨æœ€åçš„å·ç§¯å±‚ä¹‹åã€‚

æ€»çš„æ·±åº¦æœ‰34ï¼Œ50ï¼Œ101ï¼Œ152å±‚ã€‚

å¯¹äºæ›´æ·±çš„ç½‘ç»œæ¥è¯´ï¼Œä½¿ç”¨ç“¶é¢ˆå±‚æ¥æ”¹å–„ç²¾åº¦ï¼ˆå’ŒGoogLeNetå·®ä¸å¤šï¼‰

28x28x256, input -> 1x1x64, conv -> 3x3x64 conv -> 1x1x256 conv

å®é™…çš„è®­ç»ƒè¿‡ç¨‹ä¸­ï¼š

- æ¯ä¸ªconvå±‚åé¢è·Ÿä¸€ä¸ªbatchnorm
- Xavier/2 çš„åˆå§‹åŒ–å‚æ•°
- SGD + åŠ¨é‡ç®—æ³•ï¼ˆ0.9ï¼‰æ¥æ›´æ–°
- å­¦ä¹ ç‡ä¸º0.1ï¼Œæ¯æ¬¡åˆ°è¾¾éªŒè¯é›†æ£€æŸ¥ç‚¹çš„é«˜åœ°çš„æ—¶å€™å‡å°‘å­¦ä¹ ç‡ä¸ºåŸæ¥çš„1/10
- mini-batchå¤§å°256
- æƒé‡è¡°å˜ä¸º1e-5
- æ²¡æœ‰ä½¿ç”¨dropoutå±‚ã€‚

å®éªŒæ•ˆæœï¼š
æ‰€æœ‰5é¡¹ç›®æ ‡éƒ½æ˜¯ğŸ†ï¼š
ImageNetå›¾åƒåˆ†ç±»ï¼Œå›¾åƒæ£€æµ‹ï¼Œå›¾åƒå®šä½
COCOæ£€æµ‹ï¼Œåˆ†å‰²

æ¯”äººçš„æ•ˆæœéƒ½å¥½qwq

Inception-v4: Resnet+Inception
VGGï¼šç”¨äº†æœ€å¤šçš„å†…å­˜ï¼Œ
GoogLeNetï¼šæœ€é«˜æ•ˆ
AlexNetï¼šç²¾åº¦ä¸é«˜ï¼Œå†…å­˜ç”¨é‡å¤§ï¼Œå‡†ç¡®ç‡ä½ã€‚
ResNetï¼šç¨³å¥çš„æ•ˆç‡ï¼Œä¾èµ–äºæ¨¡å‹ï¼Œå‡†ç¡®ç‡é«˜

å…¶ä»–ç½‘ç»œï¼š
Network in Networkï¼ˆNiNï¼‰

å¯¹ResNetçš„æ”¹è¿›ï¼š
Identity Mappings in Deep Residual Networks
åˆ›å»ºäº†æ›´ç›´æ¥çš„ï¼Œä»åº•éƒ¨å¾€ä¸Šå±‚çš„é€šè·¯

å®½æ®‹å·®ç½‘ç»œï¼šå¹¶æ’è¿™ä¸ªæ®‹å·®çš„å·ç§¯å±‚ã€‚
å¢åŠ å®½åº¦è€Œä¸æ˜¯æ·±åº¦å¯ä»¥æé«˜è®¡ç®—æ€§èƒ½ã€‚

ResNeXt
çµæ„Ÿæ¥è‡ªäºInceptionã€‚

Deep Network with Stochastic Depth:
åŠ¨æœºï¼šå‡å°‘æ¢¯åº¦æ¶ˆå¤±
åœ¨æ¯æ¬¡è®­ç»ƒçš„è¿‡ç¨‹å½“ä¸­ï¼Œä»¥åŒä¸€ä¸ªå‡½æ•°æ¥éšæœºä¸¢æ‰å­å±‚ã€‚
åœ¨æµ‹è¯•çš„æ—¶å€™ä½¿ç”¨æ•´ä¸ªç½‘ç»œ

FractalNetï¼š Ultra-Deep Neural Networks without Residuals.
æ®‹å·®çš„è¡¨ç¤ºæ˜¯æ²¡æœ‰æ„ä¹‰çš„ï¼Œä»æµ…å±‚åˆ°æ·±å±‚çš„ä¿¡æ¯ä¼ é€’æ‰æ˜¯æœ‰æ„ä¹‰çš„ã€‚
è¿™ä¸ªç»“æ„å¯ä»¥ä»æ·±å±‚ç½‘ç»œå’Œæµ…å±‚ç½‘ç»œéƒ½æœ‰ç›´è¿çš„é€šè·¯ã€‚
è®­ç»ƒçš„æ—¶å€™éšæœºdropoutä¸€äº›é€šè·¯
æµ‹è¯•çš„æ—¶å€™ä½¿ç”¨æ•´ä¸ªç½‘ç»œã€‚

DenseNetç´§å¯†è”ç³»çš„å·ç§¯ç¥ç»ç½‘ç»œã€‚
ä¼˜ç‚¹ï¼šé¿å…æ¢¯åº¦æ¶ˆå¤±ï¼Œå¼ºåŒ–ç‰¹å¾çš„ä¼ æ’­ï¼Œé¼“åŠ±ç‰¹å¾çš„é‡å¤ä½¿ç”¨ã€‚

æœ‰æ•ˆç‡çš„ç½‘ç»œï¼š
SqueezeNetï¼šAlexNetç­‰çº§çš„ç²¾åº¦ä½†æ˜¯å‚æ•°æ•°é‡æ˜¯AlexNetçš„1/50ã€‚

squeezeå±‚ï¼Œç”±1x1å’Œ3x3çš„å·ç§¯å±‚æ„æˆã€‚

å‚æ•°æ•°é‡510å€ç¼©å°ã€‚

### æ€»ç»“ï¼š

- VGG, GoogLeNet, ResNet ç›®å‰ä¹Ÿåœ¨ä½¿ç”¨ï¼Œåœ¨zooé‡Œé¢ã€‚
- ResNetæœ‰æœ€å¥½çš„æ•ˆæœ
- è¶‹åŠ¿æ˜¯éå¸¸æ·±çš„ç¥ç»ç½‘ç»œ
- ç ”ç©¶é‡ç‚¹æ˜¯å›´ç»•è®¾è®¡å±‚ï¼Œè·³è¿‡è¿æ¥å±‚å’Œæ”¹å–„æ¢¯åº¦ä¼ æ’­
- æ·±åº¦ï¼Œå®½æ®‹å·®è¿æ¥ä¹Ÿæ˜¯è¶‹åŠ¿ã€‚

ä¸‹ä¸€èŠ‚ï¼š

### Recurrent Neural Networks

Vanillaç¥ç»ç½‘ç»œï¼šä¸€ä¸ªæ¥ç€ä¸€ä¸ª

Recurrent NN: å¤„ç†åºåˆ—

ä¸€å¯¹å¤šï¼Œå¤šå¯¹ä¸€ï¼Œå¤šå¯¹å¤š

ä¸€å¯¹å¤šï¼šä¸€å¼ å›¾ç‰‡ç”¨è‹¥å¹²å•è¯è¡¨ç¤º

å¤šå¯¹ä¸€ï¼šä»ä¸€æ®µè¯å½“ä¸­åˆ¤æ–­å‡ºè¯´è¯äººçš„æƒ…ç»ª

å¤šå¯¹å¤šï¼šæœºå™¨ç¿»è¯‘ï¼šä¸€ä¸²å•è¯å˜æˆå¦ä¸€ä¸²ã€‚

å¦ä¸€ä¸ªå¤šå¯¹å¤šï¼šå¸§çº§åˆ«çš„è§†é¢‘åˆ†ç±»ã€‚

åºåˆ—åŒ–å¤„ç†éåºåˆ—åŒ–å›¾åƒï¼šæŠŠè¿™ä¸€ç³»åˆ—å›¾åƒå˜æˆä¸€ä¸²ï¼Œç”Ÿæˆä¸€ä¸ªå¿«ç…§ã€‚ä¸€æ¬¡ç”Ÿæˆå›¾åƒçš„ä¸€å—ã€‚

x -> RNN -> y
åœ¨æŸäº›æ—¶åˆ»æƒ³è¦é¢„æµ‹å‘é‡çš„ç»“æœã€‚

é€’æ¨å…¬å¼ï¼š

$$h_t = f_W(h_{t-1}, x_t)$$

æ¯ä¸€æ¬¡éƒ½ä½¿ç”¨åŒæ ·çš„å‡½æ•°ï¼ŒåŒæ ·çš„å‚æ•°é›†åˆã€‚

æ¯”å¦‚è¿™ä¸ªå…¬å¼ï¼š

$$h_{t}=f_{W}\left(h_{t-1}, x_{t}\right)$$

å°±èƒ½å¤Ÿå˜æˆè¿™æ ·ï¼š

$$h_{t}=\tanh \left(W_{h h} h_{t-1}+W_{x h} x_{t}\right)$$

$$y_{t}=W_{h y} h_{t}$$

å‡½æ•°ä¸å†ä»…ä»¥è¾“å…¥ä¸ºè‡ªå˜é‡ï¼Œè‡ªèº«çš„çŠ¶æ€ä¹Ÿå‚ä¸è®¡ç®—ï¼Œå¹¶ä¸”æ‰€æœ‰çš„æ—¶é—´æ­¥æ›´æ–°å…±äº«ä¸€ä¸ªæƒé‡ã€‚

å¤šåˆ°ä¸€ï¼šæŠŠè¾“å…¥å‘é‡ç¼–ç æˆå•ä¸ªå‘é‡ã€‚
ä¸€åˆ°å¤šï¼šä»è¾“å…¥å¾—åˆ°çš„å•ä¸ªå‘é‡åˆ¶é€ è¾“å‡ºåºåˆ—ã€‚

æ¯”å¦‚å­—ç¬¦å±‚é¢çš„è¯­è¨€æ¨¡å‹ï¼š

è¯å…¸ï¼š `h e l o`

è®­ç»ƒçš„åºåˆ—ä¸ºï¼š`hello`

æ¯ä¸ªè¾“å‡ºå¾—åˆ°çš„å­—ç¬¦ç»§ç»­ä½œä¸ºä¸‹ä¸€ä¸ªçš„è¾“å…¥é…±ç´«ã€‚

ä½†æ˜¯å‘åbackpropogationä¸å¥½å¼„ï¼Œå¤„ç†çš„ä¸œè¥¿å¤ªå¤šäº†ã€‚

é‚£å°±åˆ†æ‰¹æ¬¡çš„æ¥ï¼Œtruncatedã€‚

çœ‹é‚£ä¸ª[min-char-rnn.py](https://gist.github.com/karpathy/d4dee566867f8291f086)

ç”Ÿæˆåå››è¡Œè¯—`SONNETS`ï¼Œç”Ÿæˆæ•°å­¦å…¬å¼ï¼Œç”Ÿæˆcä»£ç ã€‚

æ‰¾åˆ°è§£é‡Šçš„å•å…ƒã€‚å¼•ç”¨æ£€æµ‹å•å…ƒã€‚é•¿åº¦è·Ÿè¸ªå•å…ƒã€‚ifçŠ¶æ€çš„å•å…ƒï¼Œå¼•ç”¨/æ³¨é‡Šå•å…ƒï¼Œä»£ç æ·±åº¦å•å…ƒã€‚

å›¾åƒæ ‡æ³¨ï¼šCNN+RNNã€‚

æŠŠå›¾åƒçš„æœ€åä¸€å±‚å…¨è¿æ¥å±‚å»æ‰ï¼Œæ¥åˆ°RNNä¸Šé¢å»ã€‚

åŸæ¥çš„éšå«å±‚ä¸ºï¼š

$$\mathrm{h}=\tanh \left(\mathrm{Wxh}^{*} \mathrm{x}+\mathrm{Whh}^{*} \mathrm{h}\right)$$

ç°åœ¨å˜æˆï¼š

$$\mathrm{h}=\tanh \left(\mathrm{Wxh}^{*} \mathrm{x}+\mathrm{Whh}^{*} \mathrm{h}+\mathrm{Wih}^{*} \mathrm{v}\right)$$

Image Captioning With Attention
ç”Ÿæˆæ¯ä¸ªå•è¯çš„æ—¶å€™ï¼ŒRNNåªä¼šæ³¨æ„åœ¨è‡ªå·±æ³¨æ„èŒƒå›´å†…çš„ä¸œè¥¿ã€‚

è¾“å…¥å›¾åƒ -> å·ç§¯ç‰¹å¾æå– -> RNN + Attentionå †å åœ¨è¿™ä¸ªå›¾åƒä¸Š -> ä¸€ä¸ªä¸€ä¸ªå•è¯ç”Ÿæˆã€‚

ç»™äº†æƒé‡çš„ç‰¹å¾ï¼Œç»™äº†æƒé‡çš„è¿æ¥ã€‚
è½¯æ³¨æ„ï¼Œç¡¬æ³¨æ„ã€‚

è§†è§‰é—®é¢˜å›ç­”ï¼š
å¡è½¦ä¸Šæœ‰å“ªä¸€ç§æ¿’å±åŠ¨ç‰©ï¼Ÿ
å¦‚æœå¸æœºâ†ªï¸çš„è¯ä¼šå»å“ªé‡Œï¼Ÿ
è¿™ä¸ªç…§ç‰‡æ˜¯ä»€ä¹ˆæ—¶å€™æ‹æ‘„çš„ï¼Ÿ
é›¨ä¼ä¸‹é¢æœ‰è°ï¼Ÿ

å¤šå±‚RNNï¼š

$$h_{t}^{l}=\tanh W^{l} \left( \begin{array}{l}{h_{t}^{l-1}} \\ {h_{t-1}^{l}}\end{array}\right)$$

LSTMï¼š

$$\left( \begin{array}{l}{i} \\ {f} \\ {o} \\ {g}\end{array}\right)=\left( \begin{array}{l}{\operatorname{sigm}} \\ {\operatorname{sigm}} \\ {\operatorname{sigm}} \\ {\operatorname{sigm}} \\ {\tanh }\end{array}\right) W^{l} \left( \begin{array}{c}{h_{t}^{l-1}} \\ {h_{t-1}^{l}}\end{array}\right)$$

$$c_{t}^{l}=f \odot c_{t-1}^{l}+i \odot g$$

$$h_{t}^{l}=o \odot \tanh \left(c_{t}^{l}\right)$$

Vanilla RNNæ¢¯åº¦æµï¼š

$$\begin{aligned} h_{t} &=\tanh \left(W_{h h} h_{t-1}+W_{x h} x_{t}\right) \\ &=\tanh \left(\left(W_{h h} \quad W_{h x}\right) \left( \begin{array}{c}{h_{t-1}} \\ {x_{t}}\end{array}\right)\right) \\ &=\tanh \left(W \left( \begin{array}{c}{h_{t-1}} \\ {x_{t}}\end{array}\right)\right) \end{aligned}$$

å°±æ˜¯æŠŠä¸¤ä¸ªWå †å åœ¨ä¸€èµ·ã€‚

ä»ä¸Šå±‚æµåˆ°ä¸‹å±‚è¿™æ ·å­ã€‚
æœ€å¤§ç‰¹å¾å€¼>1ï¼Œæ¢¯åº¦çˆ†æ‰ã€‚æ‰€ä»¥è¦è£å‰ªæ‰è¿™ä¸ªæ¢¯åº¦ï¼šå¦‚æœè¿™ä¸ªæ¢¯åº¦çš„normå¤ªå¤§çš„è¯å°±è¦æˆæ¯”ä¾‹ç¼©æ”¾ã€‚
æœ€å¤§ç‰¹å¾å€¼<1ï¼Œæ¢¯åº¦æ¶ˆå¤±ã€‚è¿™å°±è¦æ”¹å˜RNNçš„ç»“æ„äº†

Vanilla RNNï¼š
$$h_{t}=\tanh \left(W \left( \begin{array}{c}{h_{t-1}} \\ {x_{t}}\end{array}\right)\right)$$

é•¿çŸ­æ—¶é—´è®°å¿†ï¼ŒLSTMï¼š
$$\left( \begin{array}{l}{i} \\ {f} \\ {o} \\ {g}\end{array}\right)=\left( \begin{array}{l}{\operatorname{sigm}} \\ {\operatorname{sigm}} \\ {\operatorname{sigm}} \\ {\operatorname{sigm}} \\ {\tanh }\end{array}\right) W^{l} \left( \begin{array}{c}{h_{t}^{l-1}} \\ {h_{t-1}^{l}}\end{array}\right)$$

$$c_{t}^{l}=f \odot c_{t-1}^{l}+i \odot g$$

$$h_{t}^{l}=o \odot \tanh \left(c_{t}^{l}\right)$$

f: é—å¿˜é—¨ï¼Œå‘Šè¯‰è¦ä¸è¦æ“¦æ‰è¿™ä¸ªå•å…ƒ
i: è¾“å…¥é—¨ï¼Œå†³å®šè¦ä¸è¦å†™é“è¿™ä¸ªcellé‡Œé¢
g: é—¨é—¨ï¼Œè¦å¾€è¿™ä¸ªcellé‡Œé¢å†™å¤šå°‘æ•°æ®
o: è¾“å‡ºé—¨ï¼Œè¿™ä¸ªé—¨çš„è¾“å‡ºæ˜¯å¤šå°‘ã€‚

é—¨é—¨çš„æ¿€æ´»å‡½æ•°æ˜¯tanhï¼Œå…¶ä»–é—¨éƒ½æ˜¯sigmoidã€‚

æ— æ³•è§£é‡Šçš„æ¢¯åº¦æµã€‚
å’ŒResNetå¾ˆåƒ
ä¸­é—´è¿˜æœ‰ä¸€ä¸ªHighway Network

$$\begin{aligned} g &=T\left(x, W_{T}\right) \\ y &=g \odot H\left(x, W_{H}\right)+(1-g) \odot x \end{aligned}$$

å…¶ä»–RNNçš„å˜ä½“ï¼š

GRU
$$\begin{aligned} r_{t} &=\sigma\left(W_{x r} x_{t}+W_{h r} h_{t-1}+b_{r}\right) \\ z_{t} &=\sigma\left(W_{x z} x_{t}+W_{h z} h_{t-1}+b_{z}\right) \\ \tilde{h}_{t} &=\tanh \left(W_{x h} x_{t}+W_{h h}\left(r_{t} \odot h_{t-1}\right)+b_{h}\right) \\ h_{t} &=z_{t} \odot h_{t-1}+\left(1-z_{t}\right) \odot \tilde{h}_{t} \end{aligned}
$$

æ€»ç»“ï¼š

- RNNå¼•å…¥äº†ç½‘ç»œçš„å¾ˆå¤šçµæ´»æ€§
- Vanilla RNNå¾ˆç®€å•ä½†æ˜¯æ•ˆæœä¸å’‹åœ°
- ä¸€èˆ¬ç”¨LSTMå’ŒGRUï¼Œä»–ä»¬çš„ç›¸äº’ä½œç”¨å¯ä»¥æ”¹è¿›æ¢¯åº¦æµ
- å‘åä¼ æ’­æ¢¯åº¦RNNæœ‰å¯èƒ½ä¼šçˆ†ç‚¸æˆ–è€…æ¶ˆå¤±ã€‚
- çˆ†ç‚¸çš„è¯å¯ä»¥ä½¿ç”¨æ¢¯åº¦è£å‰ªçš„æ–¹æ³•è¿›è¡Œæ§åˆ¶
- æ¶ˆå¤±çš„è¯å¯ä»¥é€šè¿‡æ·»åŠ é¢å¤–çš„äº¤äº’æ¥å®ç°ï¼ˆæ¯”å¦‚ç”¨LSTMï¼‰
- æ›´å¥½çš„æ¶æ„å’Œæ›´ç®€å•çš„ç»“æ„éƒ½æ˜¯ç°åœ¨ç ”ç©¶çš„çƒ­ç‚¹é—®é¢˜
- æ›´å¥½çš„ç†è§£ç†è®ºå’Œå®éªŒå¾ˆå¿…è¦ã€‚

## æ£€æµ‹å’Œåˆ†å‰²

CVä»»åŠ¡ï¼š

- åƒç´ çº§åˆ«è¯­ä¹‰åˆ†å‰²
- åˆ†ç±»å’Œå®šä½
- ç‰©ä½“æ£€æµ‹
- å®ä¾‹åˆ†å‰²

## åƒç´ çº§åˆ«è¯­ä¹‰åˆ†å‰²

ä¸è€ƒè™‘å®ä¾‹ä¹‹é—´çš„å·®å¼‚ï¼Œåªè€ƒè™‘åƒç´ çš„å·®å¼‚ã€‚

- æ€è·¯ï¼šæ»‘åŠ¨çª—å£æ³•
  é—®é¢˜ï¼šæ€§èƒ½å¤ªå·®äº†ï¼Œåœ¨ç‰¹å¾ä¹‹é—´é‡å çš„éƒ¨åˆ†æ²¡æœ‰è¢«é‡å¤ä½¿ç”¨ã€‚

- å…¨å·ç§¯ï¼š
  é—®é¢˜ï¼šåœ¨åŸå›¾ä¸Šé¢åšå·ç§¯æˆæœ¬å¤ªé«˜äº†qwq
  æ‰€ä»¥è®¾è®¡ä¸€ä¸²å·ç§¯å±‚ï¼ŒåŒ…æ‹¬ä¸‹é‡‡æ ·å’Œä¸Šé‡‡æ ·å±‚ã€‚

è¾“å…¥3 * H * W
- é«˜åˆ†è¾¨ç‡ï¼šD1 * H/2 * W/2
- ä¸­åˆ†è¾¨ç‡ï¼šD2 * H/4 * W/4
- ä½åˆ†è¾¨ç‡ï¼šD3 * H/4 * H/4
- ä¸­åˆ†è¾¨ç‡ï¼šD2 * H/4 * W/4
- é«˜åˆ†è¾¨ç‡ï¼šD1 * H/2 * W/2
é¢„æµ‹ï¼š H * W

ä¸‹é‡‡æ ·ï¼š æœ‰strideçš„å·ç§¯ï¼Œæ± åŒ–
ä¸Šé‡‡æ ·ï¼š åæ± åŒ–ï¼Œæœ‰strideçš„è½¬ç½®å·ç§¯

ä¸‹é‡‡æ ·æ˜¯å–ä¸€ä¸ªå·ç§¯æ ¸çš„æœ€å¤§å€¼ï¼Œä¸Šé‡‡æ ·çš„è¯å°±æ˜¯æ’å€¼æ–¹æ³•äº†ï¼Œæœ‰æœ€è¿‘é‚»æ’å€¼æ³•ï¼Œä¹Ÿæœ‰åªæŠŠå·¦ä¸Šè§’çš„è®¾å®šä¸ºè¾“å…¥çš„å€¼ï¼Œå…¶ä»–éƒ½ä¸º0

æœ€å¤§æ± åŒ–å±‚çš„è¯ï¼Œæ¯ä¸ªå…ƒç´ éƒ½æ˜¯å·ç§¯æ ¸çš„æœ€å¤§å€¼ã€‚æœ€å¤§åæ± åŒ–çš„è¯éœ€è¦è®°å½•æœ€å¤§å€¼çš„ä½ç½®ã€‚ä¸‹é‡‡æ ·å’Œä¸Šé‡‡æ ·æ˜¯ä¸€ä¸€å¯¹åº”çš„ã€‚

ç„¶åå­¦ä¹ åå·ç§¯æŠŠqwqã€‚è¾“å…¥æ˜¯4x4ï¼Œè¾“å‡ºä¹Ÿæ˜¯4x4ã€‚
å¹³å¸¸çš„strideä¸º1ï¼Œpaddingä¸º1ï¼Œ3x3çš„å·ç§¯ã€‚

å¦‚æœä¸‹é‡‡æ ·çš„è¯ï¼Œè¾“å…¥ä¸º4x4ï¼Œè¾“å‡ºä¸º2x2ã€‚

è¾“å…¥ä¸º2x2çš„è¯ï¼Œè¿›è¡Œ3x3çš„è½¬ç½®å·ç§¯ï¼Œstrideä¸º2ï¼Œpaddingä¸º1ã€‚

è¾“å…¥ï¼ša,b
æ»¤æ³¢å™¨ï¼šx,y,z
è¾“å‡ºï¼šax, ay, az+bx, by, bz

åæ­£å°±æ˜¯è½¬ç½®å·ç§¯
æ­£å¸¸å·ç§¯ï¼Œæ¯”å¦‚è¾“å…¥æ˜¯2x6, å·ç§¯å±•å¼€æˆä¹˜æ³•æ˜¯6x1ï¼Œè¾“å‡ºæ˜¯2x1
è½¬ç½®å·ç§¯ï¼Œè¾“å…¥æ˜¯6x2, åå·ç§¯å±•å¼€æˆä¹˜æ³•æ˜¯2x1ï¼Œè¾“å‡ºæ˜¯6x1

## åˆ†ç±»å’Œå®šä½é—®é¢˜

æ€è·¯ï¼šæŠŠå®šä½é—®é¢˜å½“åšä¸€ä¸ªå›å½’é—®é¢˜æ¥è§£å†³ã€‚

æ¡†çš„åæ ‡ï¼šx,y,w,hã€‚

åˆ†ç±»æ ‡ç­¾ï¼šä½¿ç”¨softmax lossæ¥ä½œä¸ºæ ‡ç­¾çš„è¾“å‡ºã€‚
è¾“å‡ºæ¡†ï¼šä½¿ç”¨L2 lossæ¥ä½œä¸ºæŸå¤±å‡½æ•°ã€‚

æ‰€ä»¥æ˜¯ä¸€ä¸ªå¤šä»»åŠ¡çš„lossï¼Œä¸¤ä¸ªlossåŠ èµ·æ¥æ˜¯æ€»çš„lossã€‚

é€šå¸¸ä½¿ç”¨å·²ç»è®­ç»ƒå¥½çš„ImageNetæ¨¡å‹æ¥è¿ç§»å­¦ä¹ ã€‚

å¦å¤–ä¹Ÿæœ‰äººç±»å§¿æ€ä¼°è®¡ã€‚è¡¨ç¤º14ä¸ªå…³èŠ‚çš„ä½ç½®ã€‚ä¹Ÿæ˜¯åˆ†åˆ«è®¡ç®—è¿™14ä¸ªç‚¹çš„L2 lossï¼Œæœ€ååŠ èµ·æ¥ã€‚

## ç‰©ä½“æ£€æµ‹é—®é¢˜

PASCAL VOCã€‚å¹³å‡å¹³å‡ç²¾åº¦ã€‚
ç‰©ä½“æ£€æµ‹èƒ½å¤Ÿå½“åšä¸€ä¸ªå›å½’é—®é¢˜å—ï¼Ÿè¿™ä¸ªå˜é‡ä¹Ÿå¤ªå¤šäº†ã€‚

æ‰€ä»¥å°±ç”¨æ»‘åŠ¨çª—å£æ¥è§£å†³ã€‚
é—®é¢˜ï¼šéœ€è¦æŠŠå¾ˆå¤šä½ç½®å’Œå°ºåº¦éƒ½æ”¾åˆ°CNNé‡Œé¢ï¼Œè®¡ç®—æˆæœ¬å¤ªå¤§äº†ã€‚

Region Proposals
ä¸¤æ­¥æ³•ï¼Œç¬¬ä¸€æ­¥å…ˆèƒ½å¤Ÿæ‰¾åˆ°å¯èƒ½åŒ…å«ç‰©ä½“çš„æ¡†ã€‚
ç›¸å¯¹å¿«ä¸€ç‚¹ï¼ŒCPUéƒ½èƒ½å¤Ÿåœ¨å‡ ç§’å®Œæˆã€‚

### R-CNNï¼š
å…ˆä½¿ç”¨æå‡ºçš„æ–¹æ³•ç”ŸæˆRoiã€‚
ç„¶åæ‰­æ›²å›¾åƒåŒºåŸŸã€‚
æŠŠæ¯ä¸ªåŒºåŸŸéƒ½é€šè¿‡å·ç§¯ç½‘ç»œã€‚
æœ€åç”¨SVMæ¥åˆ†ç±»ã€‚
è¿˜æœ‰Bboxæ­£åˆ™ã€‚

ç‰¹åˆ«è®¾å®šå¾—åˆ°çš„è®­ç»ƒç›®æ ‡ï¼š
- ä½¿ç”¨softmaxåˆ†ç±»å™¨fine-tuneçš„ç½‘ç»œï¼Œå¯¹æ•°æŸå¤±ã€‚
- è®­ç»ƒåçš„çº¿æ€§SVMåˆ†ç±»å™¨ï¼Œhinge loss
- è®­ç»ƒåçš„bounding-box å›å½’ï¼ˆæœ€å°äºŒä¹˜ï¼‰

è®­ç»ƒé€Ÿåº¦å¾ˆæ…¢ï¼Œç”¨äº†å¾ˆå¤šç£ç›˜ç©ºé—´

å¤„ç†å¾ˆæ…¢ï¼Œæ¯ä¸ªå›¾ç‰‡47ç§’ã€‚
è¿™ä¸ªé—®é¢˜åœ¨SPPnetä¸­å¾—åˆ°ä¿®å¤

# Fast R-CNN
æ•´å¹…å›¾åƒé€šè¿‡å·ç§¯ç½‘ç»œ
ç”ŸæˆRoIï¼Œ
roi poolingå±‚
ç„¶åæ˜¯å…¨è¿æ¥å±‚
softmaxåˆ†ç±»å™¨+çº¿æ€§åˆ†ç±»å™¨å›å½’bbox
æœ€åä¹Ÿæ˜¯å¤šä»»åŠ¡æŸå¤±ã€‚log loss + smooth l1 lossã€‚

# Faster R-CNN

RoI Poolingã€‚
é«˜åˆ†è¾¨ç‡å›¾åƒè¾“å…¥3x640x480ï¼Œç„¶ååŒºåŸŸæå‡ºã€‚
é€šè¿‡CNN
é«˜åˆ†è¾¨ç‡ç‰¹å¾ï¼š512x20x15
æŠ•å½±çš„ç‰¹å¾ï¼š512x18x8ï¼Œéšç€ä¸åŒçš„proposalå˜åŒ–ã€‚
åˆ†é…åˆ°7x7çš„ç½‘æ ¼å½“ä¸­ã€‚æœ€å¤§æ± åŒ–æ¯ä¸ªcell
å¯¹æ¯ä¸€ä¸ªproposalæ¥è¯´ï¼Œæœ€åroi convçš„ç‰¹å¾æ˜¯512x7x7
å…¨è¿æ¥å±‚ï¼š512x7x7

è®­ç»ƒæ—¶é—´ï¼Œæµ‹è¯•æ—¶é—´éƒ½å¾ˆçŸ­ã€‚
åˆ†åˆ«æµ‹è¯•äº†å•ä»»åŠ¡å’Œå¤šä»»åŠ¡ã€‚
å¯ä»¥çœ‹åˆ°fast rcnnå½“ä¸­çš„region proposalå ç”¨äº†å¤§éƒ¨åˆ†çš„æ—¶é—´ã€‚

## Faster R-CNN
è®©CNNæ¥åšproposalï¼š
æ’å…¥Region Proposal Networkå±‚ï¼Œæ¥é¢„æµ‹ç‰¹å¾çš„proposalã€‚

è¿™é‡Œæœ‰4ä¸ªloss

1. RPNèƒ½ä¸èƒ½åˆ†å‰²å‡ºç›®æ ‡ï¼Ÿèƒ½æˆ–è€…ä¸èƒ½ã€‚
2. RPNå›å½’çš„æ¡†çš„åæ ‡
3. æœ€åçš„åˆ†ç±»åˆ†æ•°ï¼ˆç‰©ä½“æ£€æµ‹ï¼‰
4. æœ€åçš„æ¡†çš„åæ ‡

Faster R-CNNçš„æµ‹è¯•æ—¶é—´æ›´çŸ­0.2ç§’ã€‚

## æ²¡æœ‰proposalçš„æ£€æµ‹

Yolo/SSD

SSD:
å•ä¸ªshotï¼Œå¤šæ¡†æ£€æµ‹ã€‚
è¾“å…¥å›¾åƒ3xHxW
è¾“å‡ºå›¾åƒåˆ†æˆ7x7 grid

imageï¼ˆå¡‘é€ ï¼‰ä¸€ç³»åˆ—base boxesï¼Œåœ¨æ¯ä¸ªç½‘æ ¼çš„ä¸­å¿ƒï¼Œè¿™é‡Œçš„B=3

åœ¨è¿™äº›æ¡†å½“ä¸­ï¼Œå›å½’5ä¸ªæ•°å­—ï¼šdx,dy,dh,dwå’Œconfidenceã€‚
é¢„æµ‹åˆ†æ•°å¯¹è¿™Cä¸ªåˆ†ç±»
èƒŒæ™¯ä¹Ÿä½œä¸ºä¸€ä¸ªåˆ†ç±»ã€‚

æœ€åçš„è¾“å‡ºä¸º7x7x(5xB + C)

ç‰©ä½“æ£€æµ‹ï¼š

- åŸºç¡€ç½‘ç»œï¼š
  - VGG16
  - ResNet-101
  - Inception V2,V3
  - Inception
  - ResNet
  - MobileNet 

- ç›®æ ‡æ£€æµ‹çš„ç»“æ„
  - Faster R-CNN
  - R-FCN
  - SSD

å›¾åƒå¤§å°ï¼š Region Proposals

æŠ˜ä¸­: Faster R-CNN æ…¢ä¸€ç‚¹ä½†æ˜¯æ›´ç²¾ç¡®
SSDå¿«ä¸€ç‚¹ä½†ä¸é‚£ä¹ˆç²¾ç¡®ã€‚

å¦å¤–ï¼Œç¨ å¯†æ ‡è®°ï¼šç‰©ä½“æ£€æµ‹+æ ‡è®°(ä¸çœ‹äº†çœ‹ä¸æ‡‚qwq)

## å®ä¾‹åˆ†å‰²

ä¸¤åªç‹—ä¹‹é—´æ€ä¹ˆåˆ†å‰²å¼€ï¼Ÿ

### Mask R-CNN

è¾“å…¥å›¾åƒ -> CNN -> Roiå¯¹é½ -> Conv -> Conv -> ä¸ºæ¯ä¸€ä¸ªåˆ†ç±»é¢„æµ‹ä¸€ä¸ªmaskã€‚

ç»“æœè¶…çº§å¥½qwqï¼Œç»™å¤§ä½¬è·ªäº†ã€‚

Mask R-CNNä¹Ÿèƒ½å¤Ÿåšå§¿æ€ã€‚

è¾“å…¥å›¾åƒ -> CNN -> Roiå¯¹é½ -> Conv -> Conv -> ä¸ºæ¯ä¸€ä¸ªåˆ†ç±»é¢„æµ‹ä¸€ä¸ªmaskã€‚

åœ¨Roiå¯¹é½åé¢çš„é‚£ä¸ªå·ç§¯å±‚é‡Œé¢ï¼Œè¿›è¡Œç±»åˆ«åˆ†ç±»ï¼Œæœ‰Cä¸ªå‚æ•°ã€‚æ¯ä¸ªæ¡†éƒ½æœ‰4*Cä¸ªå‚æ•°ã€‚ç„¶åå…³èŠ‚åæ ‡ã€‚

æ€»ç»“ä¸€ä¸‹ï¼Œè¯­ä¹‰åˆ†å‰²çš„è¯æ²¡æœ‰ç‰©ä½“ï¼Œåªæœ‰åƒç´ ã€‚
åˆ†ç±»å’Œå®šä½çš„è¯æ˜¯å•ä¸ªç‰©ä½“
ç‰©ä½“æ£€æµ‹å’Œå®ä¾‹åˆ†å‰²çš„è¯æ˜¯é’ˆå¯¹å¤šä¸ªç‰©ä½“ã€‚

## ä¸‹ä¸€èŠ‚ï¼šçœ‹åˆ°CNNçš„ç‰¹å¾ï¼Œæ·±æ¢¦+é£æ ¼è½¬æ¢ã€‚

æƒ³è¦çŸ¥é“å·ç§¯å±‚å½“ä¸­çš„ç‰¹å¾é•¿ä»€ä¹ˆæ ·å­ã€‚

ç¬¬ä¸€å±‚ï¼šå¯è§†åŒ–æ»¤æ³¢å™¨ã€‚
ç¬¬ä¸€å±‚æƒé‡ï¼š16x3x7x7
ç¬¬äºŒå±‚ï¼š20x16x7x7
ç¬¬ä¸‰å±‚ï¼š20x20x7x7

ç¬¬ä¸€å±‚å› ä¸ºæœ‰ä¸‰ä¸ªé€šé“æ‰€ä»¥æ˜¯å½©è‰²çš„ã€‚
åœ¨æ›´é«˜çš„å±‚å½“ä¸­èƒ½å¤Ÿçœ‹åˆ°ï¼Œä½†æ˜¯çœ‹ä¸å‡ºå•¥ä¸œè¥¿qwqã€‚

æœ€åä¸€å±‚æ˜¯4096çš„å‘é‡ï¼ŒæŠŠè¿™ä¸ªç½‘ç»œè¿è¡Œå¾ˆå¤šå›¾ç‰‡ï¼Œæ”¶é›†è¿™äº›ç‰¹å¾å‘é‡ã€‚

åœ¨ç‰¹å¾ç©ºé—´æµ‹è¯•å›¾åƒã€‚

é€šè¿‡é™ç»´æŠ€æœ¯èƒ½å¤ŸæŠŠ4096ç»´çš„å‘é‡é™æˆ2ç»´ã€‚
ç®€å•çš„æ–¹æ³•ï¼šPCA
å¤æ‚ä¸€ç‚¹çš„ï¼št-SNEã€‚

conv5çš„ç‰¹å¾æ˜¯ä¸€ä¸ª128x13x13çš„å‘é‡ï¼Œæˆ‘ä»¬æŠŠä»–çœ‹æˆ128ä¸ª13x13ç»„æˆçš„ç°åº¦å›¾åƒã€‚
ç„¶åæœ€ç™½çš„åœ°æ–¹å°±æ˜¯æƒé‡æœ€å¼ºçš„åœ°æ–¹ã€‚

æœ€å¤§æ¿€æ´»patchã€‚æ¯”å¦‚conv5æ˜¯128x13x13çš„ï¼Œé€‰æ‹©17ä¸ªé€šé“ã€‚é€šè¿‡è¿™ä¸ªç½‘ç»œè¿è¡Œè®¸å¤šå›¾ç‰‡ï¼ŒæŠŠè¿™äº›ç›¸åº”æœ€å¤§çš„å›¾ç‰‡è¾“å‡ºï¼Œå°±èƒ½å¤Ÿçœ‹å‡ºè¿™äº›æƒé‡å¯¹åº”äº†ä»€ä¹ˆç‰¹å¾äº†ã€‚

é®æŒ¡å®éªŒï¼šé®æŒ¡å›¾åƒçš„ä¸€éƒ¨åˆ†ï¼Œç„¶ååœ¨é®æŒ¡çš„åœ°æ–¹çœ‹çƒ­åŠ›å›¾ã€‚

å‡¸æ˜¾å›¾ï¼šå¦‚ä½•å‘Šè¯‰å“ªäº›åƒç´ å¯¹åˆ†ç±»æœ‰ç”¨ï¼Ÿ

å¯¹æ¯ä¸ªå›¾åƒåƒç´ ï¼Œè®¡ç®—æ²¡æœ‰å½’ä¸€åŒ–çš„åˆ†ç±»åˆ†æ•°ï¼Œå»ç»å¯¹å€¼ï¼Œå¹¶ä¸”åœ¨RGBé€šé“ä¸Šå åŠ ã€‚

æ— ç›‘ç£çš„åˆ†å‰²ï¼šä½¿ç”¨è¿™ä¸ªå‡¸æ˜¾å›¾ã€‚

é€šè¿‡å¯¼å‘çš„å‘åä¼ æ’­æ¥æ‰¾åˆ°ä¸­é—´ç‰¹å¾ï¼š

åŸæ¥çš„ReLUè®¡ç®—backpropçš„æ—¶å€™æ˜¯æ ¹æ®å‰å‘ä¼ æ’­çš„å€¼æ˜¯æ­£çš„è¿˜æ˜¯è´Ÿçš„æ¥å†³å®šå‘åä¼ æ’­ä¸ä¼ æ’­çš„ã€‚
ç°åœ¨æ˜¯ç›´æ¥æ ¹æ®æ¢¯åº¦çš„å€¼æ¥å†³å®šåˆ°åº•å‘åä¼ ä¸ä¼ ã€‚

è¿™æ ·åªè®°å½•å¤§äº0çš„æƒé‡çš„è¯ï¼Œè¾“å‡ºçš„å›¾ä¼šå¥½çœ‹ä¸€ç‚¹ã€‚

å¯è§†åŒ–CNNçš„ç‰¹å¾ï¼š
æ¢¯åº¦ä¸‹é™æ³•ï¼šå¯¹å›¾åƒæ±‚è¿™ä¸€ä¸ªç¥ç»å…ƒçš„å“åº”ã€‚
æ¢¯åº¦ä¸Šå‡æ³•ï¼šåˆ›é€ ä¸€ä¸ªå‡çš„å›¾åƒï¼Œèƒ½å¤Ÿæœ€å¤§çš„æ¿€æ´»ä½ çš„ç¥ç»å…ƒã€‚

$$I^{*}=\arg \max _{\mathrm{I}}[\mathrm{f}(\mathrm{I})]+[\mathrm{R}(\mathrm{I})]$$

ç¥ç»å…ƒçš„å€¼ï¼Œå’Œè‡ªç„¶å›¾åƒæ­£åˆ™é¡¹ã€‚

ç”Ÿæˆå‡å›¾çš„ç®—æ³•ï¼š
- é¦–å…ˆæ˜¯ç”Ÿæˆä¸€å¼ å›¾ï¼Œå…¨éƒ¨æ˜¯0
- é‡å¤ä»¥ä¸‹æ“ä½œ
  1. å‰å‘ä¼ é€’è¿™å¼ å›¾ï¼Œç®—å½“å‰çš„æ¢¯åº¦
  2. åå‘æ¥å¾—åˆ°å¯¹åº”åƒç´ ç‚¹çš„ç¥ç»å…ƒçš„æ¢¯åº¦
  3. æ›´æ–°è¿™å¼ å›¾

ç®€å•çš„æ­£åˆ™é¡¹ï¼šå¯¹æ–°çš„å›¾åƒè¿›è¡ŒL2æƒ©ç½š
æ›´å¥½çš„æ­£åˆ™é¡¹ï¼šå¯¹å›¾åƒè¿›è¡ŒL2æƒ©ç½šï¼Œå¹¶ä¸”å‘¨æœŸæ€§çš„åšä»¥ä¸‹äº‹æƒ…ï¼š
- å¯¹å›¾åƒé«˜æ–¯æ¨¡ç³Š
- æŠŠå›¾åƒè£å‰ªåˆ°æœ€å°å€¼ä¸º0
- æŠŠå›¾åƒè£å‰ªåˆ°æœ€å°æ¢¯åº¦ä¸º0

åŒæ ·çš„æ–¹æ³•å¯ä»¥çœ‹åˆ°å…¶ä»–å±‚çš„å›¾åƒã€‚

æ·»åŠ å¤šé¢å¯è§†åŒ–èƒ½å¤Ÿå¾—åˆ°æ›´å¥½çš„ç»“æœã€‚
æ·»åŠ æ›´å¤šç»†è‡´çš„æ­£åˆ™è§„åˆ™ï¼Œä¸­å¿ƒåç½®ã€‚

å¯¹äºå…¨è¿æ¥å±‚çš„è¯ä¸åœ¨åƒç´ ç©ºé—´ï¼Œè€Œåœ¨æ½œåœ¨ç©ºé—´å°±å¯ä»¥äº†qwq

ä»ä¸€ä¸ªéšæœºçš„å›¾åƒå¼€å§‹ã€‚
é€‰æ‹©ä»»æ„çš„ç±»åˆ«
æ”¹å˜å›¾åƒå¹¶ä¸”ä½¿å¾—è¿™ä¸ªç›¸åº”æœ€å¤§
é‡å¤ç›´åˆ°è¿™ä¸ªç½‘ç»œå®Œå…¨è¢«æ¬ºéª—ã€‚

ä¸€èˆ¬çš„ä¾‹å­ï¼Œæœ¬æ¥æ˜¯ä¸€ä¸ªğŸ˜ï¼Œä½†æ˜¯è¢«ç¥ç»ç½‘ç»œè®¤ä¸ºæ˜¯ä¸€ä¸ªğŸ¨ã€‚å·®åˆ«è¿˜æ˜¯ä¸å¤§çš„ï¼Œä½†æ˜¯æ”¾å¤§10å€ä¹‹åå·®åˆ«å°±å¾ˆå¤§äº†ã€‚

## DeepDreamï¼š æ”¾å¤§ç°æœ‰çš„ç‰¹å¾

ä¸æ˜¯åˆ›å»ºä¸€ä¸ªå‡å›¾ï¼Œè€Œæ˜¯è¯•ç€æ”¾å¤§æŸäº›å±‚çš„æ¯ä¸ªç¥ç»å…ƒçš„å“åº”ã€‚

ç®—æ³•ï¼š
- é€‰æ‹©ä¸€å¼ å›¾ï¼Œå¾ªç¯æ‰§è¡Œä»¥ä¸‹æ“ä½œ
  - å‰å‘ä¼ é€’ï¼šè®¡ç®—é€‰å®šçš„å±‚çš„æ¿€æ´»å‡½æ•°å€¼
  - è®¾å®šè¿™ä¸ªæ¢¯åº¦çš„å€¼ç­‰äºä»–çš„æ¿€æ´»å€¼
  - å‘åä¼ æ’­ï¼šè®¡ç®—å›¾åƒçš„æ¢¯åº¦
  - æ›´æ–°å›¾åƒ

ç­‰ä»·äºï¼š
$$\mathrm{I}^{*}=\arg \max _{\mathrm{I}} \sum_{\mathrm{i}} \mathrm{f}_{\mathrm{i}}(\mathrm{I})^{2}$$

[ä»£ç çœ‹è¿™é‡Œ](https://github.com/google/deepdream)ï¼šå¾ˆç®€å•ä½†æ˜¯æœ‰ä¸€å †trickï¼š

- æ¯”å¦‚ä½¿ç”¨np.roll(np.roll(...))æ¥ä½¿å¾—å›¾åƒ`æŠ–åŠ¨`ã€‚

- æ¢¯åº¦æ˜¯è¿›è¡ŒL1æ­£åˆ™åŒ–çš„ã€‚
- å¾—åˆ°åå·®ä¹‹åä¼šå‡å»å¹³å‡å€¼ï¼Œå¯¹æ¯ä¸ªåƒç´ è¿›è¡Œè£å‰ª

## ç‰¹å¾ç¿»è½¬

ç»™ä¸€ä¸ªCNNç‰¹å¾å‘é‡ï¼Œæ‰¾åˆ°ä¸€ä¸ªæ–°çš„å›¾åƒï¼Œä½¿å¾—ï¼š
- åŒ¹é…è¿™ä¸ªç‰¹å¾å‘é‡
- çœ‹èµ·æ¥è‡ªç„¶ï¼ˆåŠ å…¥å›¾åƒçš„å…ˆéªŒæ­£åˆ™ï¼‰

$$\mathbf{x}^{*}=\underset{\mathbf{x} \in \mathbb{R}^{H \times W \times C}{\operatorname{argmin}}}{\operatorname{argmin}} \ell\left(\Phi(\mathbf{x}), \widehat{\Phi_{0}}\right)+\lambda \mathcal{R}(\mathbf{x})$$

$$\Phi(x)$$æ˜¯æ–°å›¾åƒä¸Šçš„ç‰¹å¾ã€‚
$$\Phi_0$$æ˜¯ç»™å®šçš„å›¾åƒç‰¹å¾ã€‚

$$\ell\left(\Phi(\mathbf{x}), \Phi_{0}\right)=\left \vert \Phi(\mathbf{x})-\Phi_{0}\right \vert ^{2}$$

$$\mathcal{R}_{V^{\beta}}(\mathbf{x})=\sum_{i, j}\left(\left(x_{i, j+1}-x_{i j}\right)^{2}+\left(x_{i+1, j}-x_{i j}\right)^{2}\right)^{\frac{\beta}{2}}$$

è¿™ä¸ªæ˜¯æ­£åˆ™é¡¹ï¼Œbetaä½œä¸ºæŒ‡æ•°æ˜¯é¼“åŠ±ç©ºé—´æ›´åŠ å¹³æ»‘ã€‚å¯¹xå’Œyä¸¤ä¸ªæ–¹å‘ã€‚

### çº¹ç†é€ å‡ï¼š

ç»™ä¸€å°å—çº¹ç†ï¼Œèƒ½ç”Ÿæˆæ›´å¤§çš„çº¹ç†å—ï¼Ÿ

æŒ‰ç…§æ‰«ææ¬¡åºç”Ÿæˆåƒç´ ï¼Œä»è¾“å…¥å½“ä¸­å¤åˆ¶æœ€è¿‘çš„é‚»å±…å·²ç»ç”Ÿæˆçš„åƒç´ ã€‚è¿™ä¸ªå°±æœ‰ç‚¹ä¸è‡ªç„¶äº†qwq

ç¥ç»ç½‘ç»œç”Ÿæˆå‡çº¹ç†ï¼š
gramçŸ©é˜µã€‚æ¯å±‚çš„CNNéƒ½æ˜¯CxHxWçš„å‘é‡ã€‚HxWä¸ªç½‘æ ¼ï¼ŒCç»´çš„å‘é‡

å¤–ç§¯ä¸¤ä¸ªCç»´çš„å‘é‡ï¼ŒCxCçŸ©é˜µï¼Œæµ‹é‡å…±ç”Ÿå…³ç³»ï¼Œå¾—åˆ°æ‰€æœ‰çš„é«˜åº¦å’Œå®½åº¦å¯¹ï¼Œå¾—åˆ°çš„è¿™ä¸ªå°±æ˜¯å…‹çŸ©é˜µã€‚

æŠŠç‰¹å¾å‘é‡ä»CxHxWå˜æˆCxHWï¼Œè®¡ç®—G=FF^T

æ­¥éª¤ï¼š
1. é¢„è®­ç»ƒä¸€ä¸ªCNN ImageNet
2. æŠŠè¿™ä¸ªçº¹ç†å‰å‘é€šè¿‡CNNï¼Œè®°å½•æ¯å±‚çš„æ¿€æ´»å€¼ï¼Œå±‚içš„ç‰¹å¾æ˜ å°„çš„å½¢çŠ¶ä¸ºC_i * H_i * W_i
3. æ¯å±‚è®¡ç®—å…‹çŸ©é˜µï¼Œå¾—åˆ°è¾“å‡ºç‰¹å¾çš„ä¹˜ç§¯

  $$G_{i j}^{l}=\sum_{k} F_{i k}^{l} F_{j k}^{l}\left(\text { shape } \mathrm{C}_{\mathrm{i}} \times \mathrm{C}_{\mathrm{i}}\right)$$

4. ç™½å™ªå£°åˆå§‹åŒ–ç”Ÿæˆå›¾åƒ
5. æŠŠç”Ÿæˆçš„è¿™å¼ å›¾æ”¾åˆ°å…‹çŸ©é˜µå½“ä¸­ï¼Œè®¡ç®—æ¯å±‚çš„å…‹çŸ©é˜µ
6. è®¡ç®—lossï¼šå…‹çŸ©é˜µçš„åŠ æƒçš„L2è·ç¦»

  $$E_{l}=\frac{1}{4 N_{l}^{2} M_{l}^{2}} \sum_{i, j}\left(G_{i j}^{l}-\hat{G}_{i j}^{l}\right)^{2}$$

  $$\mathcal{L}(\vec{x}, \hat{\vec{x}})=\sum_{l=0}^{L} w_{l} E_{l}$$

7. Backpropå¾—åˆ°å›¾åƒæ¢¯åº¦ã€‚
8. æ¢¯åº¦ä½œç”¨åœ¨å›¾åƒä¸Š
9. goto 5

é‡æ„é«˜å±‚çš„çº¹ç†ï¼Œæ¢å¤äº†æ›´å¤§çš„è¾“å…¥çº¹ç†ã€‚

çº¹ç†å°±æ˜¯è‰ºæœ¯å“ï¼
é£æ ¼è¿ç§»ï¼ç‰¹å¾+å…‹é‡æ„ã€‚

å†…å®¹å›¾ç‰‡ï¼šç»™å‡ºå†…å®¹ç›®æ ‡
é£æ ¼å›¾ç‰‡ï¼šç»™å‡ºé£æ ¼ç›®æ ‡

è¾“å‡ºå›¾åƒåˆå§‹æ˜¯å™ªå£°ã€‚

æ›´å¤§çš„æƒé‡ç»™å†…å®¹çš„è¯ï¼Œä¼šæ›´åƒå†…å®¹â€¦â€¦å¥½å§æ˜¯åºŸè¯ã€‚
é£æ ¼å›¾ç‰‡çš„å¤§å°ä¼šå½±å“ç‰¹å¾çš„ç§ç±»ã€‚
å¤šç§ç‰¹å¾å¯ä»¥ç”Ÿæˆæ··åˆé£æ ¼çš„å›¾åƒã€‚

é—®é¢˜ï¼šé£æ ¼è½¬æ¢éœ€è¦åœ¨VGGä¸Šè¿›è¡Œå¤§é‡çš„å‰å‘ã€åé¡¹ä¼ æ’­ï¼Œéå¸¸æ…¢ã€‚

è§£å†³åŠæ³•ï¼šè®­ç»ƒå¦ä¸€ä¸ªç¥ç»ç½‘ç»œï¼Œå¸®åŠ©æˆ‘ä»¬è¿›è¡Œé£æ ¼è¿ç§»ã€‚

å¿«é€Ÿé£æ ¼è¿ç§»ï¼š
1. å¯¹æ¯ç§é£æ ¼è®­ç»ƒä¸€ä¸ªå‰é¦ˆç½‘ç»œ
2. å’Œä»¥å‰ä¸€æ ·ï¼Œä½¿ç”¨é¢„è®­ç»ƒçš„CNNè®¡ç®—ç›¸åŒçš„loss
3. è®­ç»ƒè¿‡åï¼Œä½¿ç”¨å•ä¸ªå‰å‘ä¼ é€’è¿›è¡Œå›¾ç‰‡çš„é£æ ¼åŒ–ã€‚

[åœ°å€åœ¨è¿™é‡Œ](https://github.com/jcjohnson/fast-neural-style)

ä¸€ä¸ªç½‘ç»œï¼Œå¤šç§é£æ ¼ã€‚

### æ€»ç»“ï¼šæœ‰å¾ˆå¤šæ–¹å¼èƒ½å¤Ÿè¡¨ç¤ºCNNçš„æƒé‡

- æ¿€æ´»å‡½æ•°ï¼šæœ€è¿‘é‚»ï¼Œé™ç»´ï¼Œæœ€å¤§patchï¼Œé®æŒ¡çœ‹ç›¸åº”çƒ­åŠ›å›¾
- æ¢¯åº¦ï¼šæ˜¾è‘—æ€§æ˜ å°„ï¼Œç±»åˆ«å¯è§†åŒ–ï¼Œæ„šå¼„ç¥ç»ç½‘ç»œçš„å›¾ï¼Œç‰¹å¾ç¿»è½¬ã€‚
- æ·±æ¢¦ï¼Œé£æ ¼è¿ç§»ã€‚
